---
title: Spark åŸºæœ¬åŸç†
author: ç‹å“²å³°
date: '2022-08-17'
slug: spark-principle
categories:
  - spark
tags:
  - tool
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [Spark ä¼˜åŠ¿ç‰¹ç‚¹](#spark-ä¼˜åŠ¿ç‰¹ç‚¹)
  - [é«˜æ•ˆæ€§](#é«˜æ•ˆæ€§)
  - [æ˜“ç”¨æ€§](#æ˜“ç”¨æ€§)
  - [é€šç”¨æ€§](#é€šç”¨æ€§)
  - [å…¼å®¹æ€§](#å…¼å®¹æ€§)
- [Spark åŸºæœ¬æ¦‚å¿µ](#spark-åŸºæœ¬æ¦‚å¿µ)
  - [åŸºæœ¬æ¦‚å¿µ](#åŸºæœ¬æ¦‚å¿µ)
  - [Application æ€»ç»“](#application-æ€»ç»“)
- [Spark æ¶æ„è®¾è®¡](#spark-æ¶æ„è®¾è®¡)
  - [Spark](#spark)
  - [PySpark](#pyspark)
- [Spark è¿è¡Œæµç¨‹](#spark-è¿è¡Œæµç¨‹)
  - [è¿è¡Œæµç¨‹](#è¿è¡Œæµç¨‹)
  - [è¿è¡Œæ¶æ„ç‰¹ç‚¹](#è¿è¡Œæ¶æ„ç‰¹ç‚¹)
- [Spark éƒ¨ç½²æ–¹å¼](#spark-éƒ¨ç½²æ–¹å¼)
  - [éƒ¨ç½²æ–¹å¼](#éƒ¨ç½²æ–¹å¼)
  - [Hadoop å’Œ Spark çš„ç»Ÿä¸€éƒ¨ç½²](#hadoop-å’Œ-spark-çš„ç»Ÿä¸€éƒ¨ç½²)
- [RDD æ•°æ®ç»“æ„](#rdd-æ•°æ®ç»“æ„)
  - [RDD ç®€ä»‹](#rdd-ç®€ä»‹)
  - [RDD åˆ›å»º](#rdd-åˆ›å»º)
  - [RDD æ“ä½œ](#rdd-æ“ä½œ)
  - [RDD ç‰¹æ€§](#rdd-ç‰¹æ€§)
  - [RDD ä¾èµ–](#rdd-ä¾èµ–)
    - [RDD çª„ä¾èµ–](#rdd-çª„ä¾èµ–)
    - [RDD å®½ä¾èµ–](#rdd-å®½ä¾èµ–)
    - [DAG åˆ‡åˆ†ä¸º Stage](#dag-åˆ‡åˆ†ä¸º-stage)
- [Apache Spark](#apache-spark)
  - [Spark çš„è®¾è®¡å“²å­¦å’Œå†å²](#spark-çš„è®¾è®¡å“²å­¦å’Œå†å²)
  - [Spark å¼€å‘ç¯å¢ƒ](#spark-å¼€å‘ç¯å¢ƒ)
  - [Spark's Interactive Consoles](#sparks-interactive-consoles)
  - [äº‘å¹³å°ã€æ•°æ®](#äº‘å¹³å°æ•°æ®)
- [Spark](#spark-1)
  - [Spark's Architecture](#sparks-architecture)
  - [Spark's Language API](#sparks-language-api)
  - [Spark's API](#sparks-api)
  - [Spark ä½¿ç”¨](#spark-ä½¿ç”¨)
    - [SparkSession](#sparksession)
    - [DataFrames](#dataframes)
    - [Partitions](#partitions)
    - [Transformation](#transformation)
      - [Lazy Evaluation](#lazy-evaluation)
    - [Action](#action)
    - [Spark UI](#spark-ui)
    - [ä¸€ä¸ª ğŸŒ°](#ä¸€ä¸ª-)
- [Spark å·¥å…·](#spark-å·¥å…·)
  - [Spark åº”ç”¨ç¨‹åº](#spark-åº”ç”¨ç¨‹åº)
  - [Dataset: ç±»å‹å®‰å…¨çš„ç»“æœåŒ– API](#dataset-ç±»å‹å®‰å…¨çš„ç»“æœåŒ–-api)
  - [Spark Structured Streaming](#spark-structured-streaming)
    - [åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›† DataFrame ä»¥åŠ Schema](#åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›†-dataframe-ä»¥åŠ-schema)
    - [å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ](#å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ)
    - [è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®](#è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®)
    - [å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç ](#å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç )
  - [Spark æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ](#spark-æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ)
  - [Spark ä½é˜¶ API](#spark-ä½é˜¶-api)
  - [SparkR](#sparkr)
  - [Spark ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…](#spark-ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…)
- [WordCount ç¤ºä¾‹](#wordcount-ç¤ºä¾‹)
- [å‚è€ƒ](#å‚è€ƒ)
</p></details><p></p>

# Spark ä¼˜åŠ¿ç‰¹ç‚¹

> ä½œä¸ºå¤§æ•°æ®è®¡ç®—æ¡†æ¶ MapReduce çš„ç»§ä»»è€…ï¼ŒSpark å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ç‰¹æ€§

![img](images/spark1.png)

* é«˜æ•ˆæ€§ï¼šè¿è¡Œé€Ÿåº¦å¿«
    - ä½¿ç”¨ DAG æ‰§è¡Œå¼•æ“ä»¥æ”¯æŒå¾ªç¯æ•°æ®æµä¸å†…å­˜è®¡ç®—
* æ˜“ç”¨æ€§
    - æ”¯æŒä½¿ç”¨ Scalaã€Javaã€Python å’Œ R è¯­è¨€è¿›è¡Œç¼–ç¨‹ï¼Œå¯ä»¥é€šè¿‡ Spark Shell è¿›è¡Œäº¤äº’å¼ç¼–ç¨‹ 
* é€šç”¨æ€§
    - Spark æä¾›äº†å®Œæ•´è€Œå¼ºå¤§çš„æŠ€æœ¯æ ˆï¼ŒåŒ…æ‹¬ SQL æŸ¥è¯¢ã€æµå¼è®¡ç®—ã€æœºå™¨å­¦ä¹ å’Œå›¾ç®—æ³•ç»„ä»¶
* å…¼å®¹æ€§ï¼šè¿è¡Œæ¨¡å¼å¤šæ ·
    - å¯è¿è¡Œäºç‹¬ç«‹çš„é›†ç¾¤æ¨¡å¼ä¸­ï¼Œå¯è¿è¡Œäº Hadoop ä¸­ï¼Œ
      ä¹Ÿå¯è¿è¡Œäº Amazon EC2 ç­‰äº‘ç¯å¢ƒä¸­ï¼Œ
      å¹¶ä¸”å¯ä»¥è®¿é—® HDFSã€Cassandraã€HBaseã€Hive ç­‰å¤šç§æ•°æ®æº 

## é«˜æ•ˆæ€§

ä¸åŒäº MapReduce å°†ä¸­é—´è®¡ç®—ç»“æœæ”¾å…¥ç£ç›˜ä¸­ï¼ŒSpark é‡‡ç”¨å†…å­˜å­˜å‚¨ä¸­é—´è®¡ç®—ç»“æœï¼Œ
å‡å°‘äº†è¿­ä»£è¿ç®—ç£ç›˜ IOï¼Œå¹¶é€šè¿‡å¹¶è¡Œè®¡ç®— DAG å›¾çš„ä¼˜åŒ–ï¼Œå‡å°‘äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„ä¾èµ–ï¼Œ
é™ä½äº†å»¶è¿Ÿç­‰å¾…æ—¶é—´ã€‚å†…å­˜è®¡ç®—ä¸‹ï¼ŒSpark æ¯” MapReduce å— 100 å€

## æ˜“ç”¨æ€§

ä¸åŒäº MapReduce ä»…æ”¯æŒ Map å’Œ Reduce ä¸¤ç§ç¼–ç¨‹ç®—å­ï¼Œ
Spark æä¾›äº†è¶…è¿‡ 80 ä¸­ä¸åŒçš„ Transformation å’Œ Action ç®—å­ï¼Œ
å¦‚ map, reduce, filter, groupByKey, sortByKey, foreach ç­‰ï¼Œ
å¹¶ä¸”é‡‡ç”¨å‡½æ•°å¼ç¼–ç¨‹é£æ ¼ï¼Œå®ç°ç›¸åŒçš„åŠŸèƒ½éœ€è¦çš„ä»£ç é‡æå¤§ç¼©å°

MapReduce å’Œ Spark å¯¹æ¯”ï¼š

| Item              | MapReduce                               | Spark                                         |
|-------------------|-----------------------------------------|-----------------------------------------------|
| æ•°æ®å­˜å‚¨ç»“æ„        | ç£ç›˜ HDFS æ–‡ä»¶ç³»ç»Ÿçš„åˆ†å‰²                   | ä½¿ç”¨å†…å­˜æ„å»ºå¼¹æ€§åˆ†å¸ƒæ•°æ®é›†(RDD)å¯¹æ•°æ®è¿›è¡Œè¿ç®—å’Œ cache |
| ç¼–ç¨‹èŒƒå¼           | Map + Reduce                             | DAG: Transformation + Action                  |
| è®¡ç®—ä¸­é—´ç»“æœå¤„ç†æ–¹å¼ | è®¡ç®—ä¸­é—´ç»“æœå†™å…¥ç£ç›˜ï¼ŒIOåŠåºåˆ—åŒ–ã€ååºåˆ—åŒ–ä»£ä»·å¤§ | ä¸­é—´è®¡ç®—ç»“æœåœ¨å†…å­˜ä¸­ç»´æŠ¤ï¼Œå­˜å–é€Ÿåº¦æ¯”ç£ç›˜é«˜å‡ ä¸ªæ•°é‡çº§   |
| Task ç»´æŠ¤æ–¹å¼      | Task ä»¥è¿›ç¨‹çš„æ–¹å¼ç»´æŠ¤                       | Task ä»¥çº¿ç¨‹çš„æ–¹å¼ç»´æŠ¤                            |


## é€šç”¨æ€§

![img](images/spark2.png)

Spark æä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚Spark å¯ä»¥ç”¨äºæ‰¹å¤„ç†ã€äº¤äº’å¼æŸ¥è¯¢(Spark SQL)ã€
å®æ—¶æµå¼è®¡ç®—(Spark Streaming)ã€æœºå™¨å­¦ä¹ (Spark MLlib)å’Œå›¾è®¡ç®—(GraphX)ã€‚
è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨ï¼Œè¿™å¯¹äºä¼ä¸šåº”ç”¨æ¥è¯´ï¼Œ
å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¹³å°æ¥è¿›è¡Œä¸åŒçš„å·¥ç¨‹å®ç°ï¼Œå‡å°‘äº†äººåŠ›å¼€å‘å’Œå¹³å°éƒ¨ç½²æˆæœ¬

## å…¼å®¹æ€§

![img](images/spark3.png)

Spark èƒ½å¤Ÿè·Ÿå¾ˆå¤šå¼€æºå·¥ç¨‹å…¼å®¹ä½¿ç”¨ï¼Œå¦‚ Spark å¯ä»¥ä½¿ç”¨ Hadoop çš„ YARN å’Œ Apache Mesos ä½œä¸ºå®ƒçš„èµ„æºç®¡ç†å’Œè°ƒåº¦å™¨ï¼Œ
å¹¶ä¸” Spark å¯ä»¥è¯»å–å¤šç§æ•°æ®æºï¼Œå¦‚ HDFSã€HBaseã€MySQL ç­‰

# Spark åŸºæœ¬æ¦‚å¿µ

## åŸºæœ¬æ¦‚å¿µ

* RDD
    - å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed Dataset)çš„ç®€ç§°ï¼Œ
      æ˜¯åˆ†å¸ƒå¼å†…å­˜çš„ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å—é™çš„å…±äº«å†…å­˜æ¨¡å‹
* DAG
    - Directed Acyclic Graph(æœ‰å‘æ— ç¯å›¾)çš„ç®€ç§°ï¼Œåæ˜  RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»
* Master Node
    - æ¯ä¸ª Master Node ä¸Šå­˜åœ¨ä¸€ä¸ª Driver Program
    - Driver Program
        - æ§åˆ¶ç¨‹åºï¼Œè´Ÿè´£ä¸º Application æ„å»º DAG å›¾
* Cluster Manager
    - é›†ç¾¤èµ„æºç®¡ç†ä¸­å¿ƒï¼Œè´Ÿè´£åˆ†é…è®¡ç®—èµ„æº
* Worker Node
    - å·¥ä½œèŠ‚ç‚¹ï¼Œè´Ÿè´£å®Œæˆå…·ä½“è®¡ç®—
    - æ¯ä¸ª Worker Node ä¸Šå­˜åœ¨ä¸€ä¸ª Executor è¿›ç¨‹
* Executor
    - è¿è¡Œåœ¨ Worker Node ä¸Šçš„ä¸€ä¸ªè¿›ç¨‹
    - è´Ÿè´£è¿è¡Œ Taskï¼Œä¸€ä¸ª Executor è¿›ç¨‹ä¸­åŒ…å«å¤šä¸ª Task çº¿ç¨‹
    - å¹¶ä¸ºåº”ç”¨ç¨‹åºå­˜å‚¨æ•°æ®
* Application
    - ç”¨æˆ·ç¼–å†™çš„ Spark åº”ç”¨ç¨‹åº
    - ä¸€ä¸ª Application åŒ…å«å¤šä¸ª Job
* Job
    - ä½œä¸š
    - ä¸€ä¸ª Job åŒ…å«å¤šä¸ª RDD åŠä½œç”¨äºç›¸åº” RDD ä¸Šçš„å„ç§æ“ä½œ
* Stage
    - é˜¶æ®µï¼ŒJob çš„åŸºæœ¬è°ƒåº¦å•ä½
    - ä¸€ä¸ª Job ä¼šåˆ†ä¸ºå¤šç»„ä»»åŠ¡ï¼Œæ¯ç»„ä»»åŠ¡è¢«ç§°ä¸º Stage
* Task
    - ä»»åŠ¡è¿è¡Œåœ¨ Executor ä¸Šçš„å·¥ä½œå•å…ƒï¼Œæ˜¯ Executor ä¸­çš„ä¸€ä¸ªçº¿ç¨‹

## Application æ€»ç»“

Application ç”±å¤šä¸ª Job ç»„æˆï¼ŒJob ç”±å¤šä¸ª Stage ç»„æˆï¼Œ
Stage ç”±å¤šä¸ª Task ç»„æˆã€‚Stage æ˜¯ Task è°ƒåº¦çš„åŸºæœ¬å•ä½

```
Application [Driver]
    - Job 1
        - Stage 1
            - Task 1 [Executor]
            - Task 2
            - ...
            - Task p
        - Stage 2
        - ...
        - Stage n
    - Job 2
    - ...
    - Job m
```

# Spark æ¶æ„è®¾è®¡

Spark é›†ç¾¤ç”± Driverã€Cluster Manager(Standalone, Yarn æˆ– Mesos)ï¼Œä»¥åŠ Worker Node ç»„æˆ

## Spark

å¯¹äºæ¯ä¸ª Spark åº”ç”¨ç¨‹åºï¼ŒWorker Node ä¸Šå­˜åœ¨ä¸€ä¸ª Executor è¿›ç¨‹ï¼ŒExecutor è¿›ç¨‹ä¸­åŒ…æ‹¬å¤šä¸ª Task çº¿ç¨‹

![img](images/sparkæ¶æ„è®¾è®¡.png)

## PySpark

å¯¹äº PySparkï¼Œä¸ºäº†ä¸ç ´å Spark å·²æœ‰çš„è¿è¡Œæ¶æ„ï¼ŒSpark åœ¨å¤–å›´åŒ…è£…äº†ä¸€å±‚ Python API

* åœ¨ Driver ç«¯ï¼Œå€ŸåŠ© Py4j å®ç° Python å’Œ Java äº¤äº’ï¼Œè¿›è€Œå®ç°é€šè¿‡ Python ç¼–å†™ Spark åº”ç”¨ç¨‹åº
* åœ¨ Executor ç«¯ï¼Œåˆ™ä¸éœ€è¦å€ŸåŠ© Py4jï¼Œå› ä¸º Executor ç«¯è¿è¡Œçš„ Task é€»è¾‘æ˜¯ç”± Driver å‘è¿‡æ¥çš„ï¼Œé‚£æ˜¯åºåˆ—åŒ–åçš„å­—èŠ‚ç 

![img](images/pysparkæ¶æ„è®¾è®¡.png)

# Spark è¿è¡Œæµç¨‹

## è¿è¡Œæµç¨‹

1. é¦–å…ˆï¼ŒDriver ä¸º Application æ„å»º DAGï¼Œå¹¶åˆ†è§£ä¸º Stage
2. ç„¶åï¼ŒDriver å‘ Cluster Manager ç”³è¯·èµ„æº
3. Cluster Manager å‘æŸäº› Worker Node å‘é€å¾å¬ä¿¡å·
4. è¢«å¾å¬çš„ Worker Node å¯åŠ¨ Executor è¿›ç¨‹å“åº”å¾å¬ï¼Œå¹¶å‘ Driver ç”³è¯·ä»»åŠ¡
5. Driver åˆ†é… Task ç»™ Worker Node
6. Executor è¿›ç¨‹ä»¥ Stage ä¸ºå•ä½æ‰§è¡Œ Taskï¼ŒæœŸé—´ Driver è¿›è¡Œç›‘æ§
7. Driver æ”¶åˆ° Executor ä»»åŠ¡å®Œæˆçš„ä¿¡å·åå‘ Cluster Manager å‘é€æ³¨é”€ä¿¡å·
8. Cluster Manager å‘ Worker Node å‘é€é‡Šæ”¾èµ„æºä¿¡å·
9. Worker Node å¯¹åº”çš„ Executor è¿›ç¨‹åœæ­¢è¿è¡Œ

> * Question 1: Task å¦‚ä½•ç”Ÿæˆï¼Ÿ

![img](images/sparkä»»åŠ¡æµç¨‹.png)

## è¿è¡Œæ¶æ„ç‰¹ç‚¹

* æ¯ä¸ª Application éƒ½æœ‰è‡ªå·±ä¸“å±çš„ Executor è¿›ç¨‹ï¼Œ
  å¹¶ä¸”è¯¥è¿›ç¨‹åœ¨ Application è¿è¡ŒæœŸé—´ä¸€ç›´é©»ç•™ï¼Œ
  Executor è¿›ç¨‹ä»¥å¤šçº¿ç¨‹çš„æ–¹å¼è¿è¡Œ Task
* Spark è¿è¡Œè¿‡ç¨‹ä¸èµ„æºç®¡ç†å™¨æ— å…³ï¼Œåªè¦èƒ½å¤Ÿè·å– Executor è¿›ç¨‹å¹¶ä¿æŒé€šä¿¡å³å¯
* Task é‡‡ç”¨äº†æ•°æ®æœ¬åœ°æ€§å’Œæ¨æµ‹æ‰§è¡Œç­‰ä¼˜åŒ–æœºåˆ¶

# Spark éƒ¨ç½²æ–¹å¼

## éƒ¨ç½²æ–¹å¼

* Local
    - æœ¬åœ°è¿è¡Œæ¨¡å¼ï¼Œéåˆ†å¸ƒå¼
* Standalone
    - ä½¿ç”¨ Spark è‡ªå¸¦é›†ç¾¤ç®¡ç†å™¨ï¼Œéƒ¨ç½²ååªèƒ½è¿è¡Œ Spark ä»»åŠ¡
* Yarn
    - Hadoop é›†ç¾¤ç®¡ç†å™¨ï¼Œéƒ¨ç½²åå¯ä»¥åŒæ—¶è¿è¡Œ MapReduceã€Sparkã€Stormã€HBase ç­‰å„ç§ä»»åŠ¡
* Mesos
    - ä¸ Yarn æœ€å¤§çš„ä¸åŒæ˜¯ Mesos çš„èµ„æºåˆ†é…æ˜¯äºŒæ¬¡çš„ï¼ŒMesos è´Ÿè´£åˆ†é…ä¸€æ¬¡ï¼Œè®¡ç®—æ¡†æ¶å¯ä»¥é€‰æ‹©æ¥å—æˆ–è€…æ‹’ç»

## Hadoop å’Œ Spark çš„ç»Ÿä¸€éƒ¨ç½²

![img](images/hadoopä¸sparkç»Ÿä¸€éƒ¨ç½².png)

# RDD æ•°æ®ç»“æ„

## RDD ç®€ä»‹

RDD å…¨ç§° Resilient Distributed Datasetï¼Œå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œ
æ˜¯è®°å½•çš„åªè¯»åˆ†åŒºé›†åˆï¼Œæ˜¯ Spark çš„åŸºæœ¬æ•°æ®ç»“æ„
RDD ä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆ

## RDD åˆ›å»º

ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹å¼åˆ›å»º RDD:

* è¯»å–æ–‡ä»¶ä¸­çš„æ•°æ®ç”Ÿæˆ RDD
* é€šè¿‡å°†å†…å­˜ä¸­çš„å¯¹è±¡å¹¶è¡ŒåŒ–å¾—åˆ° RDD

## RDD æ“ä½œ

åˆ›å»º RDD åï¼Œå¯ä»¥ä½¿ç”¨å„ç§æ“ä½œå¯¹ RDD è¿›è¡Œç¼–ç¨‹ã€‚
RDD æ“ä½œæœ‰ä¸¤ç§ç±»å‹:

* Transformation æ“ä½œ
    - è½¬æ¢æ“ä½œæ˜¯ä»å·²ç»å­˜åœ¨çš„ RDD åˆ›å»ºä¸€ä¸ªæ–°çš„ RDD
* Action æ“ä½œ
    - è¡ŒåŠ¨æ“ä½œæ˜¯åœ¨ RDD ä¸Šè¿›è¡Œè®¡ç®—åè¿”å›ç»“æœåˆ° Driver

è½¬æ¢æ“ä½œå…·æœ‰ Lazy ç‰¹æ€§ï¼Œå³ Spark ä¸ä¼šç«‹å³è¿›è¡Œå®é™…çš„è®¡ç®—ï¼Œ
åªä¼šè®°å½•æ‰§è¡Œçš„è½¨è¿¹ï¼Œåªæœ‰è§¦å‘ Action æ“ä½œçš„æ—¶å€™ï¼Œæ‰ä¼šæ ¹æ® DAG è¿›è¡Œæ‰§è¡Œ

## RDD ç‰¹æ€§

* Spark ç”¨ Scala å®ç°äº† RDD çš„ APIï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨ API å®ç° RDD çš„å„ç§æ“ä½œ
* RDD æä¾›äº†ä¸€ç»„ä¸°å¯Œçš„æ“ä½œä»¥æ”¯æŒå¸¸è§çš„æ•°æ®è¿ç®—ï¼Œåˆ†ä¸º Action(åŠ¨ä½œ)å’Œ Transformation(è½¬æ¢)ä¸¤ç§ç±»å‹
* è¡¨é¢ä¸Š RDD åŠŸèƒ½å¾ˆå—é™ã€ä¸å¤Ÿå¼ºå¤§ï¼Œå®é™…ä¸Š RDD å·²ç»è¢«æ—¶é—´è¯æ˜å¯ä»¥é«˜æ•ˆåœ°è¡¨è¾¾è®¸å¤šæ¡†æ¶çš„ç¼–ç¨‹æ¨¡å‹ï¼Œ
  æ¯”å¦‚ MapReduceã€SQLã€Pregel
* RDD æä¾›çš„è½¬æ¢æ¥å£éƒ½éå¸¸ç®€å•ï¼Œéƒ½æ˜¯ç±»ä¼¼ mapã€filterã€groupbyã€join ç­‰ç²’åº¦çš„æ•°æ®è½¬æ¢æ“ä½œï¼Œ
  è€Œä¸æ˜¯é’ˆå¯¹æŸä¸ªæ•°æ®é¡¹çš„ç»†ç²’åº¦(ä¸é€‚åˆç½‘é¡µçˆ¬è™«)

## RDD ä¾èµ–

RDD æ“ä½œç¡®å®šäº† RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»æœ‰ä¸¤ç§ï¼Œå³çª„ä¾èµ–ã€å®½ä¾èµ–

### RDD çª„ä¾èµ–

çª„ä¾èµ–æ—¶ï¼Œçˆ¶ RDD çš„åˆ†åŒºå’Œå­ RDD çš„åˆ†åŒºçš„å…³ç³»æ˜¯ä¸€å¯¹ä¸€æˆ–è€…å¤šå¯¹ä¸€çš„å…³ç³»

![img](images/RDDçª„ä¾èµ–.png)

### RDD å®½ä¾èµ–

å®½ä¾èµ–æ—¶ï¼Œçˆ¶ RDD çš„åˆ†åŒºå’Œå­ RDD çš„åˆ†åŒºçš„å…³ç³»æ˜¯ä¸€å¯¹å¤šæˆ–è€…å¤šå¯¹å¤šçš„å…³ç³»

![img](images/RDDå®½ä¾èµ–.png)

### DAG åˆ‡åˆ†ä¸º Stage

RDD ä¾èµ–å…³ç³»ç¡®å®šäº† DAG åˆ‡åˆ†æˆ Stage çš„æ–¹å¼ï¼Œåˆ‡å‰²è§„åˆ™ä¸ºï¼šä»åå¾€å‰ï¼Œ

* é‡åˆ°å®½ä¾èµ–å°±åˆ‡å‰² Stage

RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»å½¢æˆäº†ä¸€ä¸ª DAG æœ‰å‘æ— ç¯å›¾ï¼ŒDAG ä¼šæäº¤ç»™ DAG Schedulerï¼Œ
DAG Scheduler ä¼šæŠŠ DAG åˆ’åˆ†æˆç›¸äº’ä¾èµ–çš„å¤šä¸ª Stageï¼Œåˆ’åˆ† Stage çš„ä¾æ®å°±æ˜¯ RDD ä¹‹é—´çš„å®½çª„ä¾èµ–ã€‚
é‡åˆ°å®½ä¾èµ–å°±åˆ’åˆ† Stageï¼Œæ¯ä¸ª Stage åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª Task ä»»åŠ¡ï¼Œ
ç„¶åå°†è¿™äº› Task ä»¥ TaskSet çš„å½¢å¼æäº¤ç»™ Task Scheduler è¿è¡Œ

![img](images/stageåˆ‡å‰²åŸç†.png)

# Apache Spark

## Spark çš„è®¾è®¡å“²å­¦å’Œå†å²

Apache Spark is **a unified computing engine** and **a set of libraries 
for parallel data processing(big data) on computer cluster**, and Spark 
**support multiple widely used programming language** (Python, Java,
Scala, and R), and Spark **runs anywhere** from a laptop to a cluster of
thousand of servers. This makes it an easy system to start with and
scale-up to big data processing or incredibly large scale.

- **A Unified Computing Engine**
    - [Unified]
        - Spark's key driving goal is to offer a unified platform for
        writing big data applications. Spark is designed to support a
        wide range of data analytics tasks, range from simple data
        loading and SQL queries to machine learning and streaming
        computation, over the same computing engine and with a
        consistent set of APIs.
    - [Computing Engine]
        - Spark handles loading data from storage system and performing
        computation on it, not permanent storage as the end itself, you
        can use Spark with a wide variety of persistent storage
        systems.
        - cloud storage system
            - Azure Stroage
            - Amazon S3
        - distributed file systems
            - Apache Hadoop
        - key-value stroes
            - Apache Cassandra
        - message buses
            - Apache Kafka
- **A set of libraries for parallel data processing on computer cluster**
    - Standard Libraries
        - SQL and sturctured data
        - SparkSQL
        - machine learning
        - MLlib
        - stream processing
        - Spark Streaming
        - Structured Streaming
        - graph analytics
        - GraphX
    - [External Libraries](https://spark-packages.org/) published as third-party packages by open source communities

## Spark å¼€å‘ç¯å¢ƒ


- Language API
    - Python
    - Java
    - Scala
    - R
    - SQL
- Dev Env
    - local
         - [Java(JVM)](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
         - [Scala](https://www.scala-lang.org/download/)
         - [Python interpreter(version 2.7 or later)](https://repo.continuum.io/archive/)
         - [R](https://www.r-project.org/)
         - [Spark](https://spark.apache.org/downloads.html)
    - web-based version in [Databricks Community Edition](https://community.cloud.databricks.com/)

## Spark's Interactive Consoles

- Python

```bash
./bin/pyspark
```

- Scala

```bash
./bin/spark-shell
```

- SQL

```bash
./bin/spark-sql
```

## äº‘å¹³å°ã€æ•°æ®

- [Project's Github](https://github.com/databricks/Spark-The-Definitive-Guide)
- [Databricks](https://community.cloud.databricks.com/)

# Spark

## Spark's Architecture

- **Cluster**
  - **Cluser**:
     - Single machine do not have enough power and resources to perform
        computations on huge amounts of information, or the user probably
        dose not have the time to wait for the computation to finish;
     - A cluster, or group, of computers, pools the resources of many
        machines together, giving us the ability to use all the cumulative
        resources as if they were a single computer.
     - A group of machines alone is not powerful, you need a framework to
        coordinate work across them. Spark dose just that, managing and
        coordinating the execution of task on data across a cluster of
        computers.
  - **Cluster manager**:
     - Spark's standalone cluster manager
     - YARN
     - Mesos
- **Spark Application**
    - **Cluster Manager**
         - A **Driver** process
            - the heart of a Spark Appliction and maintains all relevant
               information during the lifetime of the application;
            - runs ``main()`` functions;
            - sits on a node in the cluster;
            - responsible for:
                - maintaining information about the Spark Application
                - responding to user's program or input
                - analyzing, distributing and scheduling work across the **executors**
         - A Set of **Executor** process
            - responsible for actually carrying out the work that the **driver** assigns them
            - repsonsible for :
                - executing code assigned to it by the driver
                - reporting the state of the computation on that executor back to the dirver node
    - **Spark Application**
        - Spark employs a **cluster manager** that keeps track of the **resources** available;
        - The **dirver** process is responsible for executing the **dirver program's commands** 
            across the **executors** to complete a given task;
        - The **executors** will be running Spark code;

## Spark's Language API


- Scala
    - Spark's "default" language.
- Java
- Python
    - ``pyspark``
- SQL
    - Spark support a subset of the ANSI SQL 2003 standard.
- R
    - Spark core
        - ``SparkR``
    - R community-driven package
        - ``sparklyr``

## Spark's API

Spark has two fundamental sets of APIS:

- Low-level "unstructured" APIs
    - RDD
    - Streaming
- Higher-level structured APIs
    - Dataset
    - DataFrame
        - ``org.apache.spark.sql.functions``
        - Partitions
        - DataFrame(Dataset) Methods
        - DataFrameStatFunctions
        - DataFrameNaFunctions
        - Column Methods
        - alias
        - contains
    - Spark SQL
    - Structured Streaming

## Spark ä½¿ç”¨

- å¯åŠ¨ Spark's local mode
    - äº¤äº’æ¨¡å¼
        - ``./bin/spark-shell``
        - ``./bin/pyspark``
    - æäº¤é¢„ç¼–è¯‘çš„ Spark Application
        - ``./bin/spark-submit``
- åˆ›å»º ``SparkSession``
    - äº¤äº’æ¨¡å¼, å·²åˆ›å»º
        - ``spark``
    - ç‹¬ç«‹çš„ APP
        - Scala:

        ```scala
        import org.apache.spark.SparkSession

        val spark = SparkSession
            .builder()
            .master()
            .appName()
            .config()
            .getOrCreate()
        ```

        - Python:

        ```python
        from pyspark import SparkSession

        spark = SparkSession \
            .builder() \
            .master() \
            .appName() \
            .config() \
            .getOrCreate()
        ```
### SparkSession

- SparkSession ç®€ä»‹
   - **Spark Application** controled by a **Driver** process called the **SparkSession**ï¼›
   - **SparkSession** instance is the way Spark executes user-defined manipulations across the cluster, 
      and there is a one-to-one correspondence between a **SparkSession** and a **Spark Application**;
- SparkSession ç¤ºä¾‹
   - Scala äº¤äº’æ¨¡å¼ï¼š

    ```bash
    # in shell
    $ spark-shell
    ```
    
    ```scala
    // in Scala
    scala> val myRange = spark.range(1000).toDF("number")
    ```

   - Scala APP æ¨¡å¼ï¼š

    ```scala
    // in Scala
    import org.apache.spark.SparkSession
    val spark = SparkSession 
        .builder()
        .master()
        .appName()
        .config()
        .getOrCreate()
    ```

   - Python äº¤äº’æ¨¡å¼ï¼š

    ```bash
    # in shell
    $ pyspark
    ```

    ```python
    # in Pyton
    >>> myRange = spark.range(1000).toDF("number")
    ```

   - Python APP æ¨¡å¼ï¼š

    ```python
    # in Python
    from pyspark import SparkSession
    spark = SparkSession \
        .builder() \
        .master() \
        .appName() \
        .config() \
        .getOrCreate()
    ```

### DataFrames

- A DataFrame is the most common Structured API;
- A DataFrame represents a table of data with rows and columns;
- The list of DataFrame defines the columns, the types within those columns is called the schema;
- Spark DataFrame can span thousands of computers:
- the data is too large to fit on one machine
- the data would simply take too long to perform that computation on one machine

### Partitions


### Transformation

#### Lazy Evaluation


### Action

è½¬æ¢æ“ä½œèƒ½å¤Ÿå»ºç«‹é€»è¾‘è½¬æ¢è®¡åˆ’, ä¸ºäº†è§¦å‘è®¡ç®—, éœ€è¦è¿è¡Œä¸€ä¸ªåŠ¨ä½œæ“ä½œ(action)ã€‚ä¸€ä¸ªåŠ¨ä½œæŒ‡ç¤º Spark åœ¨ä¸€ç³»åˆ—è½¬æ¢æ“ä½œåè®¡ç®—ä¸€ä¸ªç»“æœã€‚

### Spark UI

- **Spark job** represents **a set of transformations** triggered by **an individual action**, and can monitor the Spark job from the Spark UI;
- User can monitor the progress of a Spark job through the **Spark web UI**:
- Spark UI is available on port ``4040`` of the **dirver node**;
- Local Mode: ``http://localhost:4040``
- Spark UI displays information on the state of:
   - Spark jobs
   - Spark environment
   - cluster state
   - tunning
   - debugging

### ä¸€ä¸ª ğŸŒ°

(1)æŸ¥çœ‹æ•°æ®é›†

```bash
$ head /data/flight-data/csv/2015-summary.csv
```

(2)è¯»å–æ•°æ®é›†

```scala
// in Scala
val flightData2015 = spark
    .read
    .option("inferSchema", "true")
    .option("header", "true")
    .csv("/data/flight-data/csv/2015-summary.csv")
```

```python
# in Python
flightData2015 = spark \
    .read \
    .option("inferSchema", "true") \
    .option("header", "true") \
    .csv("/data/flight-data/csv/2015-summary.csv")
```

(3)åœ¨æ•°æ®ä¸Šæ‰§è¡Œè½¬æ¢æ“ä½œå¹¶æŸ¥çœ‹ Spark æ‰§è¡Œè®¡åˆ’

```scala
// in Scala
// è½¬æ¢æ“ä½œ .sort()
flightData2015.sort("count").explain()
flightData2015.sort("count")
```

(4)åœ¨æ•°æ®ä¸ŠæŒ‡å®šåŠ¨ä½œæ“ä½œæ‰§è¡ŒæŠ€æœ¯

```scala
// in Scala
// é…ç½® Spark shuffle
spark.conf.set("spark.sql.shuffle.partitions", "5")
// åŠ¨ä½œæ“ä½œ .take(n)
flightData2015.sort("count").take(2)
```

(5)DataFrame å’Œ SQL

```scala
// in Scala
flightData2015.createOrReplaceTempView("flight_data_2015")
```

```scala      
// in Scala
val sqlWay = spark.sql("""
    SELECT DEST_COUNTRY_NAME, count(1)
    FROM flight_data_2015
    GROUP BY DEST_COUNTRY_NAME
    """)

val dataFrameWay = flightData2015
    .groupBy("DEST_COUNTRY_NAME")
    .count()

sqlWay.explain()
dataFrameWay.explain()
```

```python
# in Python
sqlWay = spark.sql("""
    SELECT DEST_COUNTRY_NAME, count(1)
    FROM flight_data_2015
    GROUP BY DEST_COUNTRY_NAME
    """)

dataFrameWay = flightData2015 \
    .groupBy("DEST_COUNTRY_NAME") \
    .count()

sqlWay.explain()
dataFrameWay.explain()
```

```scala
// in Scala
spark.sql("""
    SELECT max(count) 
    FROM flight_data_2015
    """)
    .take(1)

import org.apache.spark.sql.functions.max
flightData2015
    .select(max("count"))
    .take(1)
```

```python
# in Python
spark.sql("""
    SELECT max(count)
    FROM flight_data_2015
    """) \
    .take(1)

from pyspark.sql.functions import max
flightData2015.select(max("count")).take(1)
```

# Spark å·¥å…·

## Spark åº”ç”¨ç¨‹åº

Spark å¯ä»¥é€šè¿‡å†…ç½®çš„å‘½ä»¤è¡Œå·¥å…· ``spark-submit`` è½»æ¾åœ°å°†æµ‹è¯•çº§åˆ«çš„äº¤äº’ç¨‹åºè½¬åŒ–ä¸ºç”Ÿäº§çº§åˆ«çš„åº”ç”¨ç¨‹åº.

é€šè¿‡ä¿®æ”¹ ``spark-submit`` çš„ ``master`` å‚æ•°, å¯ä»¥å°†å°†åº”ç”¨ç¨‹åºä»£ç å‘é€åˆ°ä¸€ä¸ªé›†ç¾¤å¹¶åœ¨é‚£é‡Œæ‰§è¡Œ, 
åº”ç”¨ç¨‹åºå°†ä¸€ç›´è¿è¡Œ, ç›´åˆ°æ­£ç¡®é€€å‡ºæˆ–é‡åˆ°é”™è¯¯ã€‚åº”ç”¨ç¨‹åºéœ€è¦åœ¨é›†ç¾¤ç®¡ç†å™¨çš„æ”¯æŒä¸‹è¿›è¡Œ, å¸¸è§çš„é›†ç¾¤ç®¡ç†å™¨æœ‰ 
Standalone, Mesos å’Œ YARN ç­‰.

- ç¤ºä¾‹ 1

```bash
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \      # è¿è¡Œçš„ç±» 
--master local \                                 # åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œç¨‹åº
./examples/jars/spark-examples_2.11-2.2.0.jar 10 # è¿è¡Œçš„ JAR åŒ…
```

- ç¤ºä¾‹ 2

```bash
./bin/spark-submit \
-- master local \
./examples/src/main/python/pi.py 10
```

## Dataset: ç±»å‹å®‰å…¨çš„ç»“æœåŒ– API

- ç¤ºä¾‹

```scala
case class Flight(DEST_COUNTRY_NAME: String, 
                 ORIGIN_COUNTRY_NAME: String,
                 count: BigInt)
val flightDF = spark
  .read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")

val flights = flightDF.as[Flight]

flights
  .fliter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

flights
  .take(5)
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))
```

## Spark Structured Streaming

Spark Structured Streaming(Spark ç»“æ„åŒ–æµå¤„ç†) æ˜¯ç”¨äºæ•°æ®æµå¤„ç†çš„é«˜é˜¶ API, 
åœ¨ Spark 2.2 ç‰ˆæœ¬ä¹‹åå¯ç”¨ã€‚å¯ä»¥åƒä½¿ç”¨ Spark ç»“æ„åŒ– API åœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹ä¸€æ ·, 
æ‰§è¡Œç»“æ„åŒ–æµå¤„ç†, å¹¶ä»¥æµå¼æ–¹å¼è¿è¡Œå®ƒä»¬, ä½¿ç”¨ç»“æ„åŒ–æµå¤„ç†å¯ä»¥å‡å°‘å»¶è¿Ÿå¹¶å…è®¸å¢é‡å¤„ç†.
æœ€é‡è¦çš„æ˜¯, å®ƒå¯ä»¥å¿«é€Ÿåœ°ä»æµå¼ç³»ç»Ÿä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯, è€Œä¸”å‡ ä¹ä¸éœ€è¦æ›´æ”¹ä»£ç ã€‚
å¯ä»¥æŒ‰ç…§ä¼ ç»Ÿæ‰¹å¤„ç†ä½œä¸šçš„æ¨¡å¼è¿›è¡Œè®¾è®¡, ç„¶åå°†å…¶è½¬æ¢ä¸ºæµå¼ä½œä¸š, å³å¢é‡å¤„ç†æ•°æ®, 
è¿™æ ·å°±ä½¿å¾—æµå¤„ç†å˜å¾—å¼‚å¸¸ç®€å•.

- æ•°æ®é›†ï¼šhttps://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data

### åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›† DataFrame ä»¥åŠ Schema

```scala
// in Scala
val staticDataFrame = spark
    .read
    .format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
cal staticSchema = staticDataFrame.schema
```

```python
# in Python
staticDataFrame = spark \
    .read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema
```

### å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ

```scala
// in Scala
import org.apache.spark.sql.functions.{window, column, desc, col}
staticDataFrame
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    )
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    )
    .sum("total_cost")
    .show(5)
```

```python
# in Python
from pyspark.sql.functions import window, column, desc, col
staticDataFrame \
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    ) \
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    ) \
    .sum("total_cost") \
    .show(5)
```


### è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®

```scala
// in Scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

```python
# in Python
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

### å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç 

(1)è¯»å–æµå¼æ•°æ®ï¼š

```scala
// in Scala
val streamingDataFrame = spark
    .readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)       // æŒ‡å®šä¸€æ¬¡åº”è¯¥è¯»å…¥çš„æ–‡ä»¶æ•°é‡, åœ¨å®é™…åœºæ™¯ä¸­è¢«çœç•¥
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

```python

# in Python
streamingDataFrame = spark \
    .readStream \
    .schema(staticSchema) \
    .option("maxFilesPerTrigger", 1) \
    .format("csv") \
    .option("header", "true") \
    .load("/data/retail-data/by-day/*.csv")
```

(2)æŸ¥çœ‹ DataFrame æ˜¯å¦ä»£è¡¨æµæ•°æ®ï¼š

```scala
// in Scala
streamingDataFrame.isStreaming // è¿”å› true
```

```python
# in Python
streamingDataFrame.isStreaming # è¿”å› true
```

(3)å¯¹æµå¼æ•°æ®æ‰§è¡Œåˆ†ç»„èšåˆæ“ä½œ(è½¬æ¢æ“ä½œ)

```scala
# in Scala
val purchaseByCustomerPerHour = streamingDataFrame
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    )
    .groupBy(
    $"CustomerId", 
    window($"InvoiceDate", "1 day")
    )
    .sum("total_cost")
```

```python
# in Python
purchaseByCustomerPerHour = streamingDataFrame \
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    ) \
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    ) \
    .sum("total_cost") \
    .show(5)
```

(4)è°ƒç”¨å¯¹æµæ•°æ®çš„åŠ¨ä½œæ“ä½œ, å°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜ä¸­çš„ä¸€ä¸ªè¡¨ä¸­, åœ¨æ¯æ¬¡è¢«è§¦å‘åæ›´æ–°è¿™ä¸ªå†…å­˜ç¼“å­˜

```scala
// in Scala
purchaseByCustomerPerHour.writeStream
    .format("memory")               // memory ä»£è¡¨å°†è¡¨å­˜å…¥å†…å­˜
    .queryName("customer_purchases") // å­˜å…¥å†…å­˜çš„è¡¨çš„åç§°
    .outputMode("complete")         // complete è¡¨ç¤ºä¿å­˜è¡¨ä¸­æ‰€æœ‰è®°å½•
    .start()
```

```python
# in Python
purchaseByCustomerPerHour.writeStream \
    .format("memory") \
    .queryName("customer_purchases") \
    .outputMode("complete") \
    .start()
```

(5)è¿è¡ŒæŸ¥è¯¢è°ƒè¯•ç»“æœ

```scala
// in Scala
spark.sql("""
    SELECT * 
    FROM customer_purchases
    ORDER BY `sum(total_cost)` DESC
    """)
    .show(5)
```

```python
# in Python
spark.sql("""
    SELECT * 
    FROM customer_purchases
    ORDER BY `sum(total_cost)` DESC
    """) \
    .show(5)
```

(6)å°†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°

```scala
// in Scala
purchaseByCustomerPerHour.writeStream
    .format("console")
    .queryName("customer_purchases_2")
    .outputMode("complete")
    .start()
```

```python
# in Python
purchaseByCustomerPerHour.writeStream \
    .format("console") \
    .queryName("customer_purchases_2") \
    .outputMode("complete") \
    .start()
```

## Spark æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ



## Spark ä½é˜¶ API


Spark ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æ˜¯å»ºç«‹åœ¨ RDD ä¹‹ä¸Šçš„. Spark çš„é«˜é˜¶ API åŠæ‰€æ”¯æŒçš„é«˜çº§æ“ä½œéƒ½ä¼šè¢«ç¼–è¯‘åˆ°è¾ƒä½çº§çš„ RDD ä¸Šæ‰§è¡Œ, 
ä»¥æ–¹ä¾¿å’Œå®ç°å…¶è¾ƒé«˜æ•ˆçš„åˆ†å¸ƒå¼æ‰§è¡Œ. ä½¿ç”¨ RDD å¯ä»¥å¹¶è¡ŒåŒ–å·²ç»å­˜å‚¨åœ¨é©±åŠ¨å™¨æœºå™¨å†…å­˜ä¸­çš„åŸå§‹æ•°æ®.

å¤§å¤šæ•°æƒ…å†µä¸‹ç”¨æˆ·åªéœ€è¦ä½¿ç”¨ Spark çš„é«˜é˜¶ API æˆ–é«˜çº§æ“ä½œå°±å¯ä»¥å®ç°æ‰€éœ€çš„ä¸šåŠ¡é€»è¾‘, æœ‰æ—¶å€™å¯èƒ½éœ€è¦ä½¿ç”¨ RDD, 
ç‰¹åˆ«æ˜¯åœ¨è¯»å–æˆ–æ“ä½œåŸå§‹æ•°æ®(æœªå¤„ç†æˆ–éç»“æ„åŒ–çš„æ•°æ®)æ—¶.

- ç¤ºä¾‹ 1

```scala
// in Scala
spark.sparkContext.parallelize(Seq(1, 2, 3)).toDF() // å°† RDD è½¬åŒ–ä¸º DataFrame
```

- ç¤ºä¾‹ 2

```python
# in Python
from pyspark.sql import Row

spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()
```

## SparkR

SparkR æ˜¯ä¸€ä¸ªåœ¨ Spark ä¸Šè¿è¡Œçš„ R è¯­è¨€å·¥å…·, å®ƒå…·æœ‰ä¸ Spark å…¶ä»–æ”¯æŒè¯­è¨€ç›¸åŒçš„è®¾è®¡å‡†åˆ™. 
SparkR ä¸ Spark çš„ Python API éå¸¸ç›¸ä¼¼, åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹, SparkR æ”¯æŒ Python æ”¯æŒçš„æ‰€æœ‰åŠŸèƒ½.

- ç¤ºä¾‹ 1

```r
# in R
library(SparkR)
sparkDf <- read.df("/data/flight-data/csv/2015-summary.csv", source = "csv", header = "true", inferSchema = "true")
take(sparkDF, 5)
collect(orderBy(sparkDF, "count"), 20)
```

- ç¤ºä¾‹ 2

```r
# in R
library(magrittr)

sparkDF %>% 
    orderBy(desc(sparkDF$count)) %>%
    groupBy("ORIGIN_COUNTRY_NAME") %>%
    count() %>%
    limit(10) %>%
    collect()
```

## Spark ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…

å¯ä»¥åœ¨ [Spark Packages ç´¢å¼•](https://spark-packages.org) æ‰¾åˆ°æ‰€æœ‰çš„å¼€æºç¤¾åŒºç»´æŠ¤çš„å·¥å…·åŒ…, 
ç”¨æˆ·ä¹Ÿå¯ä»¥å°†è‡ªå·±å¼€å‘çš„å·¥å…·åŒ…å‘å¸ƒåˆ°æ­¤ä»£ç åº“ä¸­, ä¹Ÿå¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°å„ç§å…¶ä»–é¡¹ç›®å’Œå·¥å…·åŒ….


# WordCount ç¤ºä¾‹

```python
import findspark
import pyspark
from pyspark import SparkContext, SparkConf


# æŒ‡å®š spark_home ä¸º Spark è§£å‹è·¯å¾„
spark_home = "/usr/local/spark"
# æŒ‡å®š Python è·¯å¾„
python_path = "/Users/zfwang/.pyenv/versions/3.7.10/envs/pyspark/bin/python"
findspark.init(spark_home, python_path)

conf = SparkConf().setAppName("WordCount").setMaster("local[4]")
sc = SparkContext(conf = conf)

rdd_line = sc.textFile("./data/hello.text")
rdd_word = rdd_line.flatMap(lambda x: x.split(" "))
rdd_one = rdd_word.map(lambda t: (t, 1))
rdd_count = rdd_one.reduceByKey(lambda x, y: x + y)
rdd_count.collect()
```

```
[('world', 1),
 ('love', 3),
 ('jupyter', 1),
 ('pandas', 1),
 ('hello', 2),
 ('spark', 4),
 ('sql', 1)]
```


# å‚è€ƒ

* [Sparkçš„åŸºæœ¬åŸç†](https://mp.weixin.qq.com/s/dontNjAGFyskhHz7tbdWeg)

