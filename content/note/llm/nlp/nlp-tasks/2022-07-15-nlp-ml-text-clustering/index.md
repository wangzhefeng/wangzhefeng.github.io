---
title: NLP-文本聚类
author: 王哲峰
date: '2022-04-05'
slug: nlp-text-clustering
categories:
  - nlp
tags:
  - tool
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>目录</summary><p>

- [TODO](#TODO)
</p></details><p></p>

# 无监督学习

无监督学习希望能够发现数据本身的规律和模式, 与监督学习相比, 无监督学习不需要对数据进行标记。这样可以解决大量人力、物力, 
也可以让数据的获取变得非常容易。

某种程度上说, 机器学习的终极目标就是无监督学习。从功能上看, 无监督学习可以帮助我们发现数据的“簇”, 
同时也可以帮助我们找寻“离群点(outlier)”。

此外, 对于特征维度特别高的数据样本, 我们同样可以通过无监督学习对数据进行降维, 
保留数据的主要特征, 这样对高维空间的数据也可以进行处理。

常见的非监督学习任务:

- 聚类
    - 需要定义相似性
- 子空间估计
    - 通常研究如何将原始数据向量在更低维度下表示
    - 理想情况下, 子空间的表示要具有代表性从而才能与原始数据接近, 最常用的方法叫做主成分分析
- 表征学习
    - 希望在欧几里得空间中找到原始对象的表示方式, 从而能在欧几里得空间里表示出原始对象的符号性质
    - 例如:希望找到城市的向量表示, 从而使得可以进行这样的向量运算:首都 + 美国 = 华盛顿
- 生成对抗网络
    - 描述数据的生成过程, 并检查真实数据与生成数据是否统计上相似

# 文本聚类

聚类视图将数据集中的样本划分为若干个通常是不相交的子集, 每个子集称为一个“簇(cluser)”。
通过这样的划分, 每个簇可能对应于一些潜在的类别。这些概念对聚类算法而言事先是未知的, 
聚类过程仅能自动形成簇结构, 簇所对应的含义需要由使用者来把握和命名。
聚类常用于寻找数据内在的分布结构, 也可作为分类等其他学习任务的前驱过程。

- 在 NLP 领域, 一个很重要的应用方向是文本聚类, 文本聚类有很多中算法, 例如:K-means、DBScan、BIRCH、CURE 等。
- 文本聚类存在大量的使用场景, 比如数据挖掘、信息检索、主题检测、文本概括。
- 聚类算法中初始的聚类点对后续的最终划分有非常大的影响, 选择合适的初始点, 可以加快算法的收敛速度和增强类之间的区分度。
   选择初始聚类点的方法有如下几种:
    - 随机选择法
        - 随机的选择 k 个对象作为初始聚类点
    - 最小最大法
        - 先选择所有对象中的相距最遥远的两个对象作为聚类点。然后选择第三个点, 
          使得它与确定的聚类点的最小距离是所有点中最大的, 然后按照相同的原则选取
    - 最小距离法
        - 选择一个正数 r, 把所有对象的中心作为第一个聚类点, 然后依次输入对象, 
          当前输入对象与确认的聚点的距离都大于 r 时, 则该对象作为新的聚类点
    - 最近归类法
        - 划分方法就是决定当前对象应该分到哪个簇中。
        - 划分方法中最为流行的是最近归类法, 即将当前对象归类与最近的聚类点

