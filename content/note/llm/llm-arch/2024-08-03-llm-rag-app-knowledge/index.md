---
title: LLM RAG åº”ç”¨--æ­å»ºçŸ¥è¯†åº“ã€æ£€ç´¢é—®ç­”é“¾ã€éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹
author: ç‹å“²å³°
date: '2024-08-03'
slug: llm-rag-app-knowledge
categories:
  - llm
tags:
  - article
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [æ­å»ºå‘é‡çŸ¥è¯†åº“](#æ­å»ºå‘é‡çŸ¥è¯†åº“)
     - [è¯å‘é‡](#è¯å‘é‡)
          - [è¯å‘é‡ç®€ä»‹](#è¯å‘é‡ç®€ä»‹)
          - [é€šç”¨æ–‡æœ¬å‘é‡](#é€šç”¨æ–‡æœ¬å‘é‡)
          - [è¯å‘é‡çš„ä¼˜åŠ¿](#è¯å‘é‡çš„ä¼˜åŠ¿)
          - [æ„å»ºè¯å‘é‡çš„æ–¹æ³•](#æ„å»ºè¯å‘é‡çš„æ–¹æ³•)
     - [å‘é‡æ•°æ®åº“](#å‘é‡æ•°æ®åº“)
     - [è°ƒç”¨ Embedding](#è°ƒç”¨-embedding)
     - [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
          - [æ•°æ®é€‰å–](#æ•°æ®é€‰å–)
          - [æ•°æ®æ¸…æ´—](#æ•°æ®æ¸…æ´—)
          - [æ–‡æ¡£åˆ†å‰²](#æ–‡æ¡£åˆ†å‰²)
     - [æ„å»ºå‘é‡æ•°æ®åº“](#æ„å»ºå‘é‡æ•°æ®åº“)
     - [å‘é‡æ£€ç´¢](#å‘é‡æ£€ç´¢)
- [åŸºäº LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾](#åŸºäº-langchain-æ„å»ºæ£€ç´¢é—®ç­”é“¾)
     - [åŠ è½½æ•°æ®åº“å‘é‡](#åŠ è½½æ•°æ®åº“å‘é‡)
     - [åˆ›å»ºä¸€ä¸ª LLM](#åˆ›å»ºä¸€ä¸ª-llm)
     - [æ„å»ºæ£€ç´¢é—®ç­”é“¾](#æ„å»ºæ£€ç´¢é—®ç­”é“¾)
     - [æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•](#æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•)
          - [åŸºäºå¬å›ç»“æœå’Œ query ç»“åˆèµ·æ¥æ„å»º prompt æ•ˆæœ](#åŸºäºå¬å›ç»“æœå’Œ-query-ç»“åˆèµ·æ¥æ„å»º-prompt-æ•ˆæœ)
          - [å¤§æ¨¡å‹è‡ªå·±å›ç­”çš„æ•ˆæœ](#å¤§æ¨¡å‹è‡ªå·±å›ç­”çš„æ•ˆæœ)
     - [æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½](#æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½)
          - [è®°å¿†](#è®°å¿†)
          - [å¯¹è¯æ£€ç´¢é“¾](#å¯¹è¯æ£€ç´¢é“¾)
- [åŸºäº Streamlit éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹](#åŸºäº-streamlit-éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹)
     - [æ„å»ºåº”ç”¨ç¨‹åº](#æ„å»ºåº”ç”¨ç¨‹åº)
     - [æ·»åŠ æ£€ç´¢å›ç­”](#æ·»åŠ æ£€ç´¢å›ç­”)
     - [éƒ¨ç½²åº”ç”¨ç¨‹åº](#éƒ¨ç½²åº”ç”¨ç¨‹åº)
- [å‚è€ƒ](#å‚è€ƒ)
</p></details><p></p>

# æ­å»ºå‘é‡çŸ¥è¯†åº“

## è¯å‘é‡

### è¯å‘é‡ç®€ä»‹

åœ¨æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ï¼Œè¯å‘é‡(Word Embeddings)æ˜¯ä¸€ç§å°†éç»“æ„åŒ–æ•°æ®ï¼Œ
å¦‚å•è¯ã€å¥å­æˆ–è€…æ•´ä¸ªæ–‡æ¡£ï¼Œè½¬åŒ–ä¸ºå®æ•°å‘é‡çš„æŠ€æœ¯ã€‚è¿™äº›å®æ•°å‘é‡å¯ä»¥è¢«è®¡ç®—æœºæ›´å¥½åœ°ç†è§£å’Œå¤„ç†ã€‚

è¯å‘é‡(Embedding)èƒŒåçš„ä¸»è¦æƒ³æ³•æ˜¯ï¼Œç›¸ä¼¼æˆ–ç›¸å…³çš„å¯¹è±¡åœ¨åµŒå…¥ç©ºé—´ä¸­çš„è·ç¦»åº”è¯¥å¾ˆè¿‘ã€‚
ä¸¾ä¸ªä¾‹å­ï¼Œå¯ä»¥ä½¿ç”¨è¯å‘é‡æ¥è¡¨ç¤ºæ–‡æœ¬æ•°æ®ã€‚åœ¨è¯å‘é‡ä¸­ï¼Œæ¯ä¸ªå•è¯è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ï¼Œ
è¿™ä¸ªå‘é‡æ•è·äº†è¿™ä¸ªå•è¯çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œ"king" å’Œ "queen" è¿™ä¸¤ä¸ªå•è¯åœ¨å‘é‡ç©ºé—´ä¸­çš„ä½ç½®å°†ä¼šéå¸¸æ¥è¿‘ï¼Œ
å› ä¸ºå®ƒä»¬çš„å«ä¹‰ç›¸ä¼¼ã€‚è€Œ "apple" å’Œ "orange" ä¹Ÿä¼šå¾ˆæ¥è¿‘ï¼Œ
å› ä¸ºå®ƒä»¬éƒ½æ˜¯æ°´æœã€‚è€Œ "king" å’Œ "apple" è¿™ä¸¤ä¸ªå•è¯åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»å°±ä¼šæ¯”è¾ƒè¿œï¼Œå› ä¸ºå®ƒä»¬çš„å«ä¹‰ä¸åŒã€‚

### é€šç”¨æ–‡æœ¬å‘é‡

è¯å‘é‡å®é™…ä¸Šæ˜¯å°†å•è¯è½¬åŒ–ä¸ºå›ºå®šçš„é™æ€çš„å‘é‡ï¼Œè™½ç„¶å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ•æ‰å¹¶è¡¨è¾¾æ–‡æœ¬ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œ
ä½†å¿½ç•¥äº†å•è¯åœ¨ä¸åŒè¯­å¢ƒä¸­çš„æ„æ€ä¼šå—åˆ°å½±å“è¿™ä¸€ç°å®ã€‚

å› æ­¤åœ¨ RAG åº”ç”¨ä¸­ä½¿ç”¨çš„å‘é‡æŠ€æœ¯ä¸€èˆ¬ä¸º**é€šç”¨æ–‡æœ¬å‘é‡(Universal Text Embedding)**ï¼Œ
è¯¥æŠ€æœ¯å¯ä»¥å¯¹ä¸€å®šèŒƒå›´å†…ä»»æ„é•¿åº¦çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ï¼Œä¸è¯å‘é‡ä¸åŒçš„æ˜¯å‘é‡åŒ–çš„å•ä½ä¸å†æ˜¯å•è¯è€Œæ˜¯**è¾“å…¥çš„æ–‡æœ¬**ï¼Œ
è¾“å‡ºçš„å‘é‡ä¼šæ•æ‰æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚

### è¯å‘é‡çš„ä¼˜åŠ¿

åœ¨ RAG é‡Œé¢è¯å‘é‡çš„ä¼˜åŠ¿ä¸»è¦æœ‰ä¸¤ç‚¹ï¼š

* è¯å‘é‡æ¯”æ–‡å­—æ›´é€‚åˆæ£€ç´¢
    - å½“åœ¨æ•°æ®åº“æ£€ç´¢æ—¶ï¼Œå¦‚æœæ•°æ®åº“å­˜å‚¨çš„æ˜¯æ–‡å­—ï¼Œ
      ä¸»è¦é€šè¿‡æ£€ç´¢å…³é”®è¯ï¼ˆè¯æ³•æœç´¢ï¼‰ç­‰æ–¹æ³•æ‰¾åˆ°ç›¸å¯¹åŒ¹é…çš„æ•°æ®ï¼Œ
      åŒ¹é…çš„ç¨‹åº¦æ˜¯å–å†³äºå…³é”®è¯çš„æ•°é‡æˆ–è€…æ˜¯å¦å®Œå…¨åŒ¹é…æŸ¥è¯¢å¥çš„ï¼›
    - è¯å‘é‡ä¸­åŒ…å«äº†åŸæ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—é—®é¢˜ä¸æ•°æ®åº“ä¸­æ•°æ®çš„ç‚¹ç§¯ã€
      ä½™å¼¦è·ç¦»ã€æ¬§å‡ é‡Œå¾—è·ç¦»ç­‰æŒ‡æ ‡ï¼Œç›´æ¥è·å–é—®é¢˜ä¸æ•°æ®åœ¨è¯­ä¹‰å±‚é¢ä¸Šçš„ç›¸ä¼¼åº¦ã€‚
* è¯å‘é‡æ¯”å…¶å®ƒåª’ä»‹çš„ç»¼åˆä¿¡æ¯èƒ½åŠ›æ›´å¼º
    - å½“ä¼ ç»Ÿæ•°æ®åº“å­˜å‚¨æ–‡å­—ã€å£°éŸ³ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§åª’ä»‹æ—¶ï¼Œ
      å¾ˆéš¾å»å°†ä¸Šè¿°å¤šç§åª’ä»‹æ„å»ºèµ·å…³è”ä¸è·¨æ¨¡æ€çš„æŸ¥è¯¢æ–¹æ³•ï¼›
    - è¯å‘é‡å¯ä»¥é€šè¿‡å¤šç§å‘é‡æ¨¡å‹å°†å¤šç§æ•°æ®æ˜ å°„æˆç»Ÿä¸€çš„å‘é‡å½¢å¼ã€‚

### æ„å»ºè¯å‘é‡çš„æ–¹æ³•

åœ¨æ­å»º RAG ç³»ç»Ÿæ—¶ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ Embedding æ¨¡å‹æ¥æ„å»ºè¯å‘é‡ï¼Œå¯ä»¥é€‰æ‹©ï¼š

* ä½¿ç”¨å„ä¸ªå…¬å¸çš„ Embedding APIï¼›
* åœ¨æœ¬åœ°ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ•°æ®æ„å»ºä¸ºè¯å‘é‡ã€‚

## å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“ä»‹ç»åœ¨[è¿™é‡Œ](https://wangzhefeng.com/note/2024/09/23/llm-vector-database/)ã€‚

## è°ƒç”¨ Embedding

Embedding API è°ƒç”¨ä»‹ç»åœ¨[è¿™é‡Œ]()ã€‚

## æ•°æ®å¤„ç†

ä¸ºæ„å»ºæœ¬åœ°çŸ¥è¯†åº“ï¼Œéœ€è¦å¯¹ä»¥å¤šç§ç±»å‹å­˜å‚¨çš„æœ¬åœ°æ–‡æ¡£è¿›è¡Œå¤„ç†ï¼Œ
è¯»å–**æœ¬åœ°æ–‡æ¡£**å¹¶é€šè¿‡ **Embedding æ–¹æ³•**å°†**æœ¬åœ°æ–‡æ¡£çš„å†…å®¹**è½¬åŒ–ä¸º**è¯å‘é‡**æ¥æ„å»º**å‘é‡æ•°æ®åº“**ã€‚

### æ•°æ®é€‰å–

> PDF æ–‡æ¡£

ä½¿ç”¨ LangChain çš„ `PyMuPDFLoader` æ¥è¯»å–çŸ¥è¯†åº“çš„ PDF æ–‡ä»¶ã€‚
`PyMuPDFLoader` æ˜¯ PDF è§£æå™¨ä¸­é€Ÿåº¦æœ€å¿«çš„ä¸€ç§ï¼Œ
ç»“æœåŒ…å« PDF åŠé¡µé¢çš„è¯¦ç»†å…ƒæ•°æ®ï¼Œå¹¶ä¸”æ¯é¡µè¿”å›ä¸€ä¸ªæ–‡æ¡£ã€‚

```python
from langchain.document_loaders.pdf import PyMuPDFLoader

# åˆ›å»ºä¸€ä¸ª PyMuPDFLoader Class å®ä¾‹ï¼Œè¾“å…¥ä¸ºå¾…åŠ è½½çš„ PDF æ–‡æ¡£è·¯å¾„
loader = PyMuPDFLoader("data_base/knowledge_db/pumpkin_book/pumpkin_book.pdf")
# è°ƒç”¨ PyMuPDFLoader Class çš„å‡½æ•° load å¯¹ PDF æ–‡ä»¶è¿›è¡ŒåŠ è½½
pdf_pages = loader.load()
```

æ–‡æ¡£åŠ è½½åå‚¨å­˜åœ¨ `pdf_pages` å˜é‡ä¸­ï¼Œ`pdf_pages` çš„å˜é‡ç±»å‹ä¸º `List`ã€‚

æ‰“å° `pdf_pages` çš„é•¿åº¦å¯ä»¥çœ‹åˆ° PDF ä¸€å…±åŒ…å«å¤šå°‘é¡µï¼š

```python
print(f"è½½å…¥åçš„å˜é‡ç±»å‹ä¸ºï¼š{type(pdf_pages)}, è¯¥ PDF ä¸€å…±åŒ…å« {len(pdf_pages)} é¡µã€‚")
```

`pdf_pages` ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œå˜é‡ç±»å‹ä¸º `langchain_core.documents.base.Document`ï¼Œ
æ–‡æ¡£å˜é‡ç±»å‹åŒ…å«ä¸¤ä¸ªå±æ€§ï¼š

* `meta_data` ä¸ºæ–‡æ¡£ç›¸å…³çš„æè¿°æ€§æ•°æ®
* `page_content` åŒ…å«è¯¥æ–‡æ¡£çš„å†…å®¹

```python
pdf_page = pdf_pages[1]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(pdf_page)}", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{pdf_page.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹ï¼š\n{pdf_page.page_content}",
    sep = "\n------\n"
)
```

> Markdown æ–‡æ¡£

å¯ä»¥æŒ‰ç…§è¯»å– PDF æ–‡æ¡£å‡ ä¹ä¸€è‡´çš„æ–¹å¼è¯»å– Markdown æ–‡æ¡£ã€‚

```python
from langchain.document_loaders.markdown import UnstructureMarkdownLoader

loader = UnstructureMarkdownLoader("data_base/knowledge_db/prompt_engineering/1.ç®€ä»‹ Instroduction.md")
md_pages = loader.load()
```

è¯»å–çš„å¯¹è±¡å’Œ PDF æ–‡æ¡£è¯»å–å‡ºæ¥æ˜¯å®Œå…¨ä¸€è‡´çš„ï¼š

```python
print(f"è½½å…¥åçš„å˜é‡ç±»å‹ä¸ºï¼š{type(md_pages)}, è¯¥ Markdown ä¸€å…±åŒ…å« {len(md_pages)} é¡µã€‚")
```

```python
md_page = md_pages[0]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(md_page)}", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{md_page.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹ï¼š\n{md_page.page_content[0:][:200]}", 
    sep = "\n------\n"
)
```

### æ•°æ®æ¸…æ´—

æœŸæœ›çŸ¥è¯†åº“çš„æ•°æ®å°½é‡æ˜¯æœ‰åºçš„ã€ä¼˜è´¨çš„ã€ç²¾ç®€çš„ï¼Œå› æ­¤è¦åˆ é™¤ä½è´¨é‡çš„ã€ç”šè‡³å½±å“ç†è§£æ–‡æœ¬æ•°æ®ã€‚
å¯ä»¥çœ‹åˆ°ä¸Šä¸‹æ–‡ä¸­è¯»å–çš„ PDF æ–‡ä»¶ä¸ä»…å°†ä¸€å¥è¯æŒ‰ç…§åŸæ–‡çš„åˆ†è¡Œæ·»åŠ äº†æ¢è¡Œç¬¦ `\n`ï¼Œ
ä¹Ÿåœ¨åŸæœ¬ä¸¤ä¸ªç¬¦å·ä¸­æ’å…¥äº† `\n`ï¼Œå¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹¶åˆ é™¤æ‰ `\n`ã€‚

```python
import re
pattern = re.compile(r"[^\u4e00-\u9fff](\n)[^\u4e00-\u9fff]", re.DOTALL)
pdf_page.page_content = re.sub(
    pattern, 
    lambda match: match.group(0).replace("\n", ""), 
    pdf_page.page_content
)
```

è¿›ä¸€æ­¥åˆ†ææ•°æ®ï¼Œå‘ç°æ•°æ®ä¸­è¿˜æœ‰ä¸å°‘çš„ `â€¢` å’Œç©ºæ ¼ï¼Œç®€å•å®ç”¨çš„ `replace` æ–¹æ³•å³å¯ã€‚

```python
pdf_page.page_content = pdf_page.page_content.replace("â€¢", "")
pdf_page.page_content = pdf_page.page_content.replace(" ", "")
print(pdf_page.page_content)
```

ä¸Šä¸‹æ–‡ä¸­è¯»å–çš„ Markdown æ–‡ä»¶æ¯ä¸€æ®µä¸­é—´éš”äº†ä¸€ä¸ªæ¢è¡Œç¬¦ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ `replace` æ–¹æ³•å»é™¤ã€‚

```python
md_page.page_content = md_page.page_content.replace("\n\n", "\n")
print(md_page.page_content)
```

### æ–‡æ¡£åˆ†å‰²

> æ–‡æ¡£åˆ†å‰²ç®€ä»‹

ç”±äºå•ä¸ªæ–‡æ¡£çš„é•¿åº¦å¾€å¾€ä¼šè¶…è¿‡æ¨¡å‹æ”¯æŒçš„ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ£€ç´¢å¾—åˆ°çš„çŸ¥è¯†å¤ªé•¿è¶…å‡ºæ¨¡å‹çš„å¤„ç†èƒ½åŠ›ã€‚
å› æ­¤ï¼Œåœ¨æ„å»ºå‘é‡çŸ¥è¯†åº“çš„è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€éœ€è¦å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ï¼Œ
**å°†å•ä¸ªæ–‡æ¡£æŒ‰é•¿åº¦æˆ–è€…æŒ‰å›ºå®šçš„è§„åˆ™åˆ†å‰²æˆè‹¥å¹²ä¸ª chunkï¼Œç„¶åå°†æ¯ä¸ª chunk è½¬åŒ–ä¸ºè¯å‘é‡ï¼Œ
å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­**ã€‚

åœ¨æ£€ç´¢æ—¶ï¼Œä¼šä»¥ chunk ä½œä¸ºæ£€ç´¢çš„å…ƒå•ä½ï¼Œ
ä¹Ÿå°±æ˜¯æ¯ä¸€æ¬¡æ£€ç´¢åˆ° `k` ä¸ª chunk ä½œä¸ºæ¨¡å‹å¯ä»¥å‚è€ƒæ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†ï¼Œ
è¿™ä¸ª `k` æ˜¯å¯ä»¥è‡ªç”±è®¾å®šçš„ã€‚

> æ–‡æ¡£åˆ†å‰² API

Langchain ä¸­æ–‡æœ¬åˆ†å‰²å™¨éƒ½æ ¹æ® `chunk_size`(å—å¤§å°)å’Œ `chunk_overlap`(å—ä¸å—ä¹‹é—´çš„é‡å å¤§å°)è¿›è¡Œåˆ†å‰²ã€‚
`CharacterTextSpitter` API ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
langchain.text_splitter.CharacterTextSpitter(
    separator: str = "\n\n",
    chunk_size = 4000,
    chunk_overlap = 200,
    length_function = <buildin function len>,
)
```

æ–¹æ³•ï¼š

* `create_documents()`: Create documents from a list of texts.
* `split_documents()`: Split documents.

å‚æ•°ï¼š

* `chunk_size` æŒ‡æ¯ä¸ªå—åŒ…å«çš„å­—ç¬¦æˆ– Token(å¦‚å•è¯ã€å¥å­ç­‰)çš„æ•°é‡
* `chunk_overlap` æŒ‡ä¸¤ä¸ªå—ä¹‹é—´å…±äº«çš„å­—ç¬¦æ•°é‡ï¼Œç”¨äºä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ï¼Œé¿å…åˆ†å‰²ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯

Langchain æä¾›å¤šç§æ–‡æ¡£åˆ†å‰²æ–¹å¼ï¼ŒåŒºåˆ«åœ¨æ€ä¹ˆç¡®å®šå—ä¸å—ä¹‹é—´çš„è¾¹ç•Œã€
å—ç”±å“ªäº›å­—ç¬¦/token ç»„æˆã€ä»¥åŠå¦‚ä½•æµ‹é‡å—å¤§å°ï¼š

* `RecursiveCharacterTextSplitter()`: æŒ‰å­—ç¬¦ä¸²åˆ†å‰²æ–‡æœ¬ï¼Œé€’å½’åœ°å°è¯•æŒ‰ä¸åŒçš„åˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²æ–‡æœ¬
* `CharacterTextSplitter()`: æŒ‰å­—ç¬¦æ¥åˆ†å‰²æ–‡æœ¬
* `MarkdownHeaderTextSplitter()`: åŸºäºæŒ‡å®šçš„æ ‡é¢˜æ¥åˆ†å‰² Markdown æ–‡ä»¶
* `TokenTextSplitter()`: æŒ‰ token æ¥åˆ†å‰²æ–‡æœ¬
* `SentenceTransformersTokenTextSplitter()`: æŒ‰ token æ¥åˆ†å‰²æ–‡æœ¬
* `Language()`: ç”¨äº CPPã€Pythonã€Rubyã€Markdown ç­‰
* `NLTKTextSplitter()`: ä½¿ç”¨ `NLTK`ï¼ˆè‡ªç„¶è¯­è¨€å·¥å…·åŒ…ï¼‰æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
* `SpacyTextSplitter()`: ä½¿ç”¨ `Spacy` æŒ‰å¥å­çš„åˆ‡å‰²æ–‡æœ¬

> æ–‡æ¡£åˆ†å‰²ç¤ºä¾‹

```python
''' 
* RecursiveCharacterTextSplitter é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²ã€‚
  å°†æŒ‰ä¸åŒçš„å­—ç¬¦é€’å½’åœ°åˆ†å‰²(æŒ‰ç…§è¿™ä¸ªä¼˜å…ˆçº§ ["\n\n", "\n", " ", ""])ï¼Œ
  è¿™æ ·å°±èƒ½å°½é‡æŠŠæ‰€æœ‰å’Œè¯­ä¹‰ç›¸å…³çš„å†…å®¹å°½å¯èƒ½é•¿æ—¶é—´åœ°ä¿ç•™åœ¨åŒä¸€ä½ç½®
* RecursiveCharacterTextSplitter éœ€è¦å…³æ³¨çš„æ˜¯4ä¸ªå‚æ•°ï¼š
    - separators: åˆ†éš”ç¬¦å­—ç¬¦ä¸²æ•°ç»„
    - chunk_size: æ¯ä¸ªæ–‡æ¡£çš„å­—ç¬¦æ•°é‡é™åˆ¶
    - chunk_overlap: ä¸¤ä»½æ–‡æ¡£é‡å åŒºåŸŸçš„é•¿åº¦
    - length_function: é•¿åº¦è®¡ç®—å‡½æ•°
'''

# å¯¼å…¥æ–‡æœ¬åˆ†å‰²å™¨
from langchain.text_splitter import RecursiveCharacterTextSplitter

# çŸ¥è¯†åº“ä¸­å•æ®µæ–‡æœ¬é•¿åº¦
CHUNK_SIZE = 500
# çŸ¥è¯†åº“ä¸­ç›¸é‚»æ–‡æœ¬é‡åˆé•¿åº¦
OVERLAP_SIZE = 50

# ä½¿ç”¨é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = CHUNK_SIZE,
    chunk_overlap = OVERLAP_SIZE,
)
text_splitter.split_text(pdf_page.page_content[0:1000])

split_docs = text_splitter.split_documents(pdf_pages)
print(f"åˆ‡åˆ†åçš„æ–‡ä»¶æ•°é‡ï¼š{len(split_docs)}")
print(f"åˆ‡åˆ†åçš„å­—ç¬¦æ•°ï¼ˆå¯ä»¥ç”¨æ¥å¤§è‡´è¯„ä¼° token æ•°ï¼‰ï¼š{sum([len(doc.page_content) for doc in split_docs])}")
```

å¦‚ä½•å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ï¼Œå…¶å®æ˜¯æ•°æ®å¤„ç†ä¸­æœ€æ ¸å¿ƒçš„ä¸€æ­¥ï¼Œå…¶å¾€å¾€å†³å®šäº†æ£€ç´¢ç³»ç»Ÿçš„ä¸‹é™ã€‚
ä½†æ˜¯ï¼Œå¦‚ä½•é€‰æ‹©åˆ†å‰²æ–¹å¼ï¼Œå¾€å¾€å…·æœ‰å¾ˆå¼ºçš„ä¸šåŠ¡ç›¸å…³æ€§â€”â€”é’ˆå¯¹ä¸åŒçš„ä¸šåŠ¡ã€ä¸åŒçš„æºæ•°æ®ï¼Œ
å¾€å¾€éœ€è¦è®¾å®šä¸ªæ€§åŒ–çš„æ–‡æ¡£åˆ†å‰²æ–¹å¼ã€‚å› æ­¤ï¼Œè¿™é‡Œä»…ç®€å•æ ¹æ® `chunk_size` å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ã€‚

## æ„å»ºå‘é‡æ•°æ®åº“

Langchain é›†æˆäº†è¶…è¿‡ 30 ä¸ªä¸åŒçš„å‘é‡å­˜å‚¨åº“ã€‚é€‰æ‹© Chroma æ˜¯å› ä¸ºå®ƒè½»é‡çº§ä¸”æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œ
è¿™ä½¿å¾—å®ƒéå¸¸å®¹æ˜“å¯åŠ¨å’Œå¼€å§‹ä½¿ç”¨ã€‚LangChain å¯ä»¥ç›´æ¥ä½¿ç”¨ OpenAI å’Œç™¾åº¦åƒå¸†çš„ Embeddingï¼Œ
åŒæ—¶ï¼Œä¹Ÿå¯ä»¥é’ˆå¯¹å…¶ä¸æ”¯æŒçš„ Embedding API è¿›è¡Œè‡ªå®šä¹‰ã€‚

```python
import os
from dotenv import load_dotenv, find_dotenv

from langchain.document_loaders.pdf import PyMuPDFLoader
from langchain.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ä½¿ç”¨ OpenAI Embedding
# from langchain.embeddings.openai import OpenAIEmbeddings
# ä½¿ç”¨ç™¾åº¦åƒå¸† Embedding
# from langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint
# ä½¿ç”¨æˆ‘ä»¬è‡ªå·±å°è£…çš„æ™ºè°± Embeddingï¼Œéœ€è¦å°†å°è£…ä»£ç ä¸‹è½½åˆ°æœ¬åœ°ä½¿ç”¨
from zhipuai_embedding import ZhipuAIEmbeddings

# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡
_ = load_dotenv(find_dotenv())

# å¦‚æœéœ€è¦é€šè¿‡ä»£ç†ç«¯å£è®¿é—®ï¼Œä½ éœ€è¦å¦‚ä¸‹é…ç½®
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ["HTTP_PROXY"] = 'http://127.0.0.1:7890'


# è·å– folder_path ä¸‹æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼Œå‚¨å­˜åœ¨ file_paths é‡Œ
file_paths = []
folder_path = '../../data_base/knowledge_db'
for root, dirs, files in os.walk(folder_path):
    for file in files:
        file_path = os.path.join(root, file)
        file_paths.append(file_path)
print(file_paths[:3])

# éå†æ–‡ä»¶è·¯å¾„å¹¶æŠŠå®ä¾‹åŒ–çš„loaderå­˜æ”¾åœ¨loadersé‡Œ
loaders = []
for file_path in file_paths:
    file_type = file_path.split('.')[-1]
    if file_type == 'pdf':
        loaders.append(PyMuPDFLoader(file_path))
    elif file_type == 'md':
        loaders.append(UnstructuredMarkdownLoader(file_path))

# ä¸‹è½½æ–‡ä»¶å¹¶å­˜å‚¨åˆ°text
texts = []
for loader in loaders: 
    texts.extend(loader.load())

# æŸ¥çœ‹æ•°æ®
text = texts[1]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(text)}.", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{text.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹:\n{text.page_content[0:]}", 
    sep="\n------\n"
)

# åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500, 
    chunk_overlap = 50
)
split_docs = text_splitter.split_documents(texts)


# å®šä¹‰ Embeddings
# embedding = OpenAIEmbeddings() 
# embedding = QianfanEmbeddingsEndpoint()
embedding = ZhipuAIEmbeddings()

# å®šä¹‰æŒä¹…åŒ–è·¯å¾„
persist_directory = '../../data_base/vector_db/chroma'
```

åˆ é™¤æ—§çš„æ•°æ®åº“æ–‡ä»¶ï¼ˆå¦‚æœæ–‡ä»¶å¤¹ä¸­æœ‰æ–‡ä»¶çš„è¯ï¼‰ï¼Œwindows ç”µè„‘è¯·æ‰‹åŠ¨åˆ é™¤

```bash
$ !rm -rf '../../data_base/vector_db/chroma'
```

```python
from langchain.vectorstores.chroma import Chroma

vectordb = Chroma.from_documents(
    documents = split_docs,
    embedding = embedding,
    persist_directory = persist_directory  # å…è®¸å°† persist_directory ç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
)
```

åœ¨æ­¤ä¹‹åï¼Œè¦ç¡®ä¿é€šè¿‡è¿è¡Œ `vectordb.persist` æ¥æŒä¹…åŒ–å‘é‡æ•°æ®åº“ï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨ã€‚

```python
vectordb.persist()
print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")
```

## å‘é‡æ£€ç´¢

> ç›¸ä¼¼åº¦æ£€ç´¢

Chroma çš„ç›¸ä¼¼åº¦æœç´¢ä½¿ç”¨çš„æ˜¯ä½™å¼¦è·ç¦»ï¼Œå³ï¼š

`$$\begin{align}
similarity
&=cos(A, B) \\
&=\frac{A \cdot B}{||A|| ||B||} \\
&=\frac{\sum_{i=1}^{n}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{n}a_{i}^{2}}\sqrt{\sum_{i=1}^{n}b_{i}^{2}}}
\end{align}$$`

å…¶ä¸­ `$a_{i}, b_{i}$` åˆ†åˆ«æ˜¯å‘é‡ `$A, B$` çš„åˆ†é‡ã€‚

å½“éœ€è¦æ•°æ®åº“è¿”å›ä¸¥è°¨çš„æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºçš„ç»“æœæ—¶å¯ä»¥ä½¿ç”¨ `similarity_search` å‡½æ•°ã€‚

```python
question = "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹"
sim_docs = vectordb.similarity_search(question, k = 3)
print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(sim_docs)}")
for i, sim_doc in enumerate(sim_docs):
    print(f"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªå†…å®¹ï¼š\n{sim_doc.page_content[:200]}", end = "\n-------------\n")
```

> MMR æ£€ç´¢

å¦‚æœåªè€ƒè™‘æ£€ç´¢å‡ºå†…å®¹çš„ç›¸å…³æ€§ä¼šå¯¼è‡´å†…å®¹è¿‡äºå•ä¸€ï¼Œå¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚
æœ€å¤§è¾¹é™…ç›¸å…³æ€§(MMR, Maximum Marginal Relevance)å¯ä»¥å¸®åŠ©åœ¨ä¿æŒç›¸å…³æ€§çš„åŒæ—¶ï¼Œ
å¢åŠ å†…å®¹çš„ä¸°å¯Œåº¦ã€‚

æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨å·²ç»é€‰æ‹©äº†ä¸€ä¸ªç›¸å…³æ€§é«˜å¾—æ–‡æ¡£ä¹‹åï¼Œå†é€‰æ‹©ä¸€ä¸ªä¸å·²é€‰æ–‡æ¡£ç›¸å…³æ€§è¾ƒä½ä½†æ˜¯ä¿¡æ¯ä¸°å¯Œçš„æ–‡æ¡£ã€‚
è¿™æ ·å¯ä»¥åœ¨ä¿æŒç›¸å…³æ€§çš„åŒæ—¶ï¼Œå¢åŠ å†…å®¹çš„å¤šæ ·æ€§ï¼Œé¿å…è¿‡äºå•ä¸€çš„ç»“æœã€‚

```python
mmr_docs = vector_db.max_marginal_relevance_search(question, k = 3)
print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(mmr_docs)}")
for i, sim_doc in enumerate(mmr_docs):
    print(f"MMR æ£€ç´¢åˆ°çš„ç¬¬ {i} ä¸ªå†…å®¹ï¼š\n{sim_doc.page_content[:200]}", end = "\n-----------\n")
```

# åŸºäº LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾

åœ¨ä¸Šé¢ä»‹ç»äº†å¦‚ä½•æ ¹æ®è‡ªå·±çš„æœ¬åœ°çŸ¥è¯†æ–‡æ¡£ï¼Œæ­å»ºäº†ä¸€ä¸ªå‘é‡çŸ¥è¯†åº“ã€‚
è¿™é‡Œå°†ä½¿ç”¨æ­å»ºå¥½çš„å‘é‡æ•°æ®åº“ï¼Œå¯¹æŸ¥è¯¢é—®é¢˜è¿›è¡Œå¬å›ï¼Œ
å¹¶å°†å¬å›ç»“æœå’ŒæŸ¥è¯¢ç»“åˆèµ·æ¥æ„å»º promptï¼Œè¾“å…¥åˆ°å¤§æ¨¡å‹ä¸­è¿›è¡Œé—®ç­”ã€‚

## åŠ è½½æ•°æ®åº“å‘é‡

é¦–å…ˆï¼ŒåŠ è½½å·²ç»æ„å»ºçš„å‘é‡æ•°æ®åº“ã€‚æ³¨æ„ï¼Œæ­¤æ—¶éœ€è¦ä½¿ç”¨å’Œæ„å»ºæ—¶ç›¸åŒçš„ Embeddingã€‚

```python
import os
from dotenv import find_dotenv, load_dotenv
from langchain.vectorstores.chroma import Chroma
from zhipuai_embedding import ZhipuAIEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡ä¸­çš„ API_KEY
_ = load_dotenv(find_dotenv())
zhipuai_api_key = os.environ["ZHIPUAI_API_KEY"]


# å®šä¹‰ Embeddings
embedding = ZhipuAIEmbeddings()
# å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
persisit_directory = "../data_base/vector_db/chroma"
# åŠ è½½æ•°æ®åº“
vectordb = Chroma(
     persist_directory = persist_directory,
     embedding_function = embedding,
)
print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")
```

> æµ‹è¯•åŠ è½½çš„å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨ä¸€ä¸ªé—®é¢˜ query è¿›è¡Œå‘é‡æ£€ç´¢ï¼Œåœ¨å‘é‡æ•°æ®åº“ä¸­æ ¹æ®ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢ï¼Œ
> è¿”å›å‰ `$k$` ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£ï¼š
> 
> ```python
> question = "ä»€ä¹ˆæ˜¯ prompt engineering?"
> docs = vectordb.similarity_search(question, k = 3)
> print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(docs)}")
> 
> for i, doc in enumerate(docs):
>      print(f"æ£€ç´¢åˆ°çš„ç¬¬ {i} ä¸ªå†…å®¹ï¼š\n{doc.page_content}", end = "\n-------")
> ```

## åˆ›å»ºä¸€ä¸ª LLM

```python
import os

from langchain_openai import ChatOpenAI

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0)
llm.invoke("è¯·ä½ è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ï¼")
```

## æ„å»ºæ£€ç´¢é—®ç­”é“¾

```python
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

tempalte = """
ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
{context}
é—®é¢˜: {question}
"""
QA_CHAIN_PROMPT = PromptTemplates(
     input_variables = ["context", "question"],
     template = template,
)
qa_chain = RetrievalQA.from_chain_type(
     # æŒ‡å®šä½¿ç”¨çš„ LLM
     llm,
     retriever = vectordb.as_retriever(),
     # è¿”å›æºæ–‡æ¡£ï¼Œé€šè¿‡æŒ‡å®šè¯¥å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ RetrievalQAWithSourceChain() æ–¹æ³•ï¼Œ
     # è¿”å›æºæ–‡æ¡£çš„å¼•ç”¨(åæ ‡æˆ–è€…å«ä¸»é”®ã€ç´¢å¼•)
     return_source_documents = True,
     # è‡ªå®šä¹‰ prompt
     chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT},
)
```

## æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•

```python
question_1 = "ä»€ä¹ˆæ˜¯å—ç“œä¹¦ï¼Ÿ"
question_2 = "Prompt Engineering for Developer æ˜¯è°å†™çš„ï¼Ÿ"
```

### åŸºäºå¬å›ç»“æœå’Œ query ç»“åˆèµ·æ¥æ„å»º prompt æ•ˆæœ

```python
result = qa_chain({"query": question_1})
print(f"å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_1 çš„ç»“æœï¼š\n {result["result"]}")

result = qa_chain({"query": question_2})
print(f"å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_2 çš„ç»“æœï¼š\n {result["result"]}")
```

### å¤§æ¨¡å‹è‡ªå·±å›ç­”çš„æ•ˆæœ

```python
prompt_template = f"è¯·å›ç­”ä¸€ä¸‹é—®é¢˜ï¼š{question_1}"
llm.predict(prompt_template)

prompt_template = f"è¯·å›ç­”ä¸€ä¸‹é—®é¢˜ï¼š{question_2}"
llm.predict(prompt_template)
```

é€šè¿‡ä»¥ä¸Šçš„ä¸¤ä¸ªé—®é¢˜ï¼Œå‘ç° LLM å¯¹äºä¸€äº›è¿‘å‡ å¹´çš„çŸ¥è¯†ä»¥åŠéå¸¸è¯†æ€§çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå›ç­”çš„å¹¶ä¸æ˜¯å¾ˆå¥½ã€‚
è€ŒåŠ ä¸Šæœ¬åœ°çŸ¥è¯†ï¼Œå°±å¯ä»¥å¸®åŠ© LLM åšå‡ºæ›´å¥½çš„å›ç­”ã€‚å¦å¤–ï¼Œä¹Ÿæœ‰åŠ©äºç¼“è§£å¤§æ¨¡å‹çš„â€œå¹»è§‰â€é—®é¢˜ã€‚

## æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½

ç°åœ¨å·²ç»å®ç°äº†é€šè¿‡ä¸Šä¼ æœ¬åœ°çŸ¥è¯†æ–‡æ¡£ï¼Œç„¶åå°†å®ƒä»¬ä¿å­˜åˆ°å‘é‡çŸ¥è¯†åº“ï¼Œ
é€šè¿‡å°†æŸ¥è¯¢é—®é¢˜ä¸å‘é‡çŸ¥è¯†åº“çš„å¬å›ç»“æœè¿›è¡Œç»“åˆè¾“å…¥åˆ° LLM ä¸­ï¼Œ
å°±å¾—åˆ°äº†ä¸€ä¸ªç›¸æ¯”äºç›´æ¥è®© LLM å›ç­”è¦å¥½çš„å¤šçš„ç»“æœã€‚

åœ¨ä¸è¯­è¨€æ¨¡å‹äº¤äº’æ—¶ï¼Œå¯èƒ½å·²ç»æ³¨æ„åˆ°ä¸€ä¸ªå…³é”®çš„é—®é¢˜ï¼š**å®ƒä»¬å¹¶ä¸è®°å¾—ä½ ä¹‹å‰çš„äº¤æµå†…å®¹**ã€‚
è¿™åœ¨æ„å»ºä¸€äº›åº”ç”¨ç¨‹åºï¼ˆå¦‚èŠå¤©æœºå™¨äººï¼‰çš„æ—¶å€™ï¼Œå¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œä½¿å¾—å¯¹è¯ä¼¼ä¹ç¼ºä¹çœŸæ­£çš„è¿ç»­æ€§ã€‚

### è®°å¿†

> Memory

è¿™é‡Œå°†ä»‹ç» LangChain ä¸­çš„å­˜å‚¨æ¨¡å—ï¼Œå³å¦‚ä½•å°†å…ˆå‰çš„å¯¹è¯åµŒå…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿å…¶å…·æœ‰è¿ç»­å¯¹è¯çš„èƒ½åŠ›ã€‚
å°†ä½¿ç”¨ `ConversationBufferMemory` è¿™ä¸ª APIï¼Œå®ƒä¿å­˜èŠå¤©æ¶ˆæ¯å†å²è®°å½•çš„åˆ—è¡¨ï¼Œ
è¿™äº›å†å²è®°å½•å°†åœ¨å›ç­”é—®é¢˜æ—¶ä¸é—®é¢˜ä¸€èµ·ä¼ é€’ç»™èŠå¤©æœºå™¨äººï¼Œä»è€Œå°†å®ƒä»¬æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ã€‚

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
     memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
     return_messages = True,  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
)
```

Memory çš„é…ç½®è¿˜åŒ…æ‹¬ä¿ç•™æŒ‡å®šå¯¹è¯è½®æ•°ã€ä¿å­˜æŒ‡å®š token æ•°é‡ã€ä¿å­˜å†å²å¯¹è¯çš„æ€»ç»“æ‘˜è¦ç­‰å†…å®¹ã€‚

### å¯¹è¯æ£€ç´¢é“¾

> Conversational Retrieval Chain

å¯¹è¯ç´¢å¼•é“¾åœ¨æ£€ç´¢ QA é“¾çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†å¤„ç†å¯¹è¯å†å²çš„èƒ½åŠ›ã€‚å®ƒçš„å·¥ä½œæµç¨‹æ˜¯ï¼š

1. å°†ä¹‹å‰çš„å¯¹è¯ä¸æ–°é—®é¢˜åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„æŸ¥è¯¢è¯­å¥ï¼›
2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢è¯¥æŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£ï¼›
3. è·å–ç»“æœåï¼Œå­˜å‚¨æ‰€æœ‰ç­”æ¡ˆåˆ°å¯¹è¯è®°å¿†åŒºï¼›
4. ç”¨æˆ·å¯ä»¥åœ¨ UI ä¸­æŸ¥çœ‹å®Œæ•´çš„å¯¹è¯æµç¨‹ã€‚

è¿™ç§é“¾å¼æ–¹å¼å°† **æ–°é—®é¢˜** æ”¾åœ¨ä¹‹å‰å¯¹è¯çš„è¯­å¢ƒä¸­è¿›è¡Œæ£€ç´¢ï¼Œå¯ä»¥å¤„ç†ä¾èµ–å†å²ä¿¡æ¯çš„æŸ¥è¯¢ã€‚
å¹¶ä¿ç•™æ‰€æœ‰ä¿¡æ¯åœ¨å¯¹è¯è®°å¿†ä¸­ï¼Œæ–¹ä¾¿è¿½è¸ªã€‚

æ¥ä¸‹æ¥è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„å‘é‡æ•°æ®åº“å’Œ LLMï¼Œæµ‹è¯•è¿™ä¸ªå¯¹è¯æ£€ç´¢é“¾çš„æ•ˆæœã€‚

```python
from langchain.chains import ConversationalRetrievalChain

qa = ConversationalRetrievalChain.from_llm(
     llm,
     retriever = vectordb.as_retriever(),
     memory = memory,
)

question = "æˆ‘å¯ä»¥å­¦ä¹ åˆ°å…³äºæç¤ºå·¥ç¨‹çš„çŸ¥è¯†å—ï¼Ÿ"
result = qa({"question": question})
print(result["answer"])

question = "ä¸ºä»€ä¹ˆè¿™é—¨è¯¾éœ€è¦æ•™è¿™æ–¹é¢çš„çŸ¥è¯†ï¼Ÿ"
result = qa({"question": question})
print(result["answer"])
```

å¯ä»¥çœ‹åˆ°ï¼ŒLLM å®ƒå‡†ç¡®åœ°åˆ¤æ–­äº†è¿™æ–¹é¢çš„çŸ¥è¯†ï¼ŒæŒ‡ä»£å†…å®¹æ˜¯å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬æˆåŠŸåœ°ä¼ é€’ç»™äº†å®ƒå†å²ä¿¡æ¯ã€‚
è¿™ç§æŒç»­å­¦ä¹ å’Œå…³è”å‰åé—®é¢˜çš„èƒ½åŠ›ï¼Œå¯å¤§å¤§å¢å¼ºäº†é—®ç­”ç³»ç»Ÿçš„è¿ç»­æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚

# åŸºäº Streamlit éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹

å½“å¯¹çŸ¥è¯†åº“å’Œ LLM å·²ç»æœ‰äº†åŸºæœ¬çš„ç†è§£ï¼Œç°åœ¨æ˜¯å°†å®ƒä»¬å·§å¦™åœ°èåˆå¹¶æ‰“é€ æˆä¸€ä¸ªå¯Œæœ‰è§†è§‰æ•ˆæœçš„ç•Œé¢çš„æ—¶å€™äº†ã€‚
è¿™æ ·çš„ç•Œé¢ä¸ä»…å¯¹æ“ä½œæ›´åŠ ä¾¿æ·ï¼Œè¿˜èƒ½ä¾¿äºä¸ä»–äººåˆ†äº«ã€‚

> Streamlit æ˜¯ä¸€ç§å¿«é€Ÿä¾¿æ·çš„æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨ Python ä¸­é€šè¿‡å‹å¥½çš„ Web ç•Œé¢æ¼”ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
> åœ¨æ„å»ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹åï¼Œå¦‚æœæƒ³æ„å»ºä¸€ä¸ª demo ç»™å…¶ä»–äººçœ‹ï¼Œä¹Ÿè®¸æ˜¯ä¸ºäº†è·å¾—åé¦ˆå¹¶æ¨åŠ¨ç³»ç»Ÿçš„æ”¹è¿›ï¼Œ
> æˆ–è€…åªæ˜¯å› ä¸ºè§‰å¾—è¿™ä¸ªç³»ç»Ÿå¾ˆé…·ï¼Œæ‰€ä»¥æƒ³æ¼”ç¤ºä¸€ä¸‹ï¼šStreamlit å¯ä»¥é€šè¿‡ Python æ¥å£ç¨‹åºå¿«é€Ÿå®ç°è¿™ä¸€ç›®æ ‡ï¼Œ
> è€Œæ— éœ€ç¼–å†™ä»»ä½•å‰ç«¯ã€ç½‘é¡µæˆ– JavaScript ä»£ç ã€‚

## æ„å»ºåº”ç”¨ç¨‹åº

```python
# streamlit_app.py
import streamlit as st
from langchain_openai import ChatOpenAI


def generate_response(input_text, openai_api_key):
     """
     å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç”¨æˆ·å¯†é’¥å¯¹ OpenAI API è¿›è¡Œèº«ä»½éªŒè¯ã€å‘é€æç¤ºå¹¶è·å– AI ç”Ÿæˆçš„å“åº”ã€‚
     è¯¥å‡½æ•°æ¥å—ç”¨æˆ·çš„æç¤ºä½œä¸ºå‚æ•°ï¼Œå¹¶ä½¿ç”¨ st.info æ¥åœ¨è“è‰²æ¡†ä¸­æ˜¾ç¤º AI ç”Ÿæˆçš„å“åº”ã€‚
     """
     llm = ChatOpenAI(temperature = 0.7, openai_api_key = openai_api_key)
     output = llm.invoke(input_text)
     output_parser = StrOutputParser()
     output = output_parser.invoke(output)
     # st.info(output)
     return output


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
     # åˆ›å»ºåº”ç”¨ç¨‹åºçš„æ ‡é¢˜
     st.title("ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘")

     # æ·»åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ï¼Œä¾›ç”¨æˆ·è¾“å…¥å…¶ OpenAI API å¯†é’¥
     openai_api_key = st.sidebar.text_input("OpenAI API Key", type = "password")

     # ä½¿ç”¨ st.form() åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡† st.text_area() ä¾›ç”¨æˆ·è¾“å…¥ã€‚
     # å½“ç”¨æˆ·ç‚¹å‡» Submit æ—¶ï¼Œgenerate_response å°†ä½¿ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°æ¥è°ƒç”¨è¯¥å‡½æ•°
     with st.form("my_form"):
          text = st.text_area(
               "Enter text:", 
               "What are the three key pieces of advice for learning how to code?"
          )
          
          submitted = st.form_submit_button("Submit")

          if not openai_api_key.startswith("sk-"):
               st.warning("Please enter your OpenAI API key!", icon = "")
          if submitted and openai_api_key.startswith("sk-"):
               generate_response(text, openai_api_key)
  
     # é€šè¿‡ä½¿ç”¨ st.session_state æ¥å­˜å‚¨å¯¹è¯å†å²ï¼Œ
     # å¯ä»¥åœ¨ç”¨æˆ·ä¸åº”ç”¨ç¨‹åºäº¤äº’æ—¶ä¿ç•™æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡ï¼Œ
     # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
     if "messages" not in st.session_state:
          st.session_state.messages = []
     
     messages = st.container(height = 300)
     if prompt := st.chat_input("Say something"):
          # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
          st.session_state.messages.appen({
               "role": "user",
               "text": prompt,
          })
          
          # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
          answer = generate_response(prompt, openai_api_key)
          # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
          if answer is not None:
               # å°† LLM çš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
               st.session_state.messages.append({
                    "role": "assistant",
                    "text": answer,
               })
          
          # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
          for message in st.session_state.messages:
               if message["role"] == "user":
                    messages.chat_message("user").write(message["text"])
               else:
                    messages.chat_message("assistant").write(message["text"])
```

```bash
$ streamlit run streamlit_app.py
```

## æ·»åŠ æ£€ç´¢å›ç­”

æ„å»ºæ£€ç´¢é—®ç­”é“¾ä»£ç ï¼š

* `get_vectordb` å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
* `get_chat_qa_chain` å‡½æ•°è¿”å›è°ƒç”¨å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ
* `get_qa_chain` å‡½æ•°è¿”å›è°ƒç”¨ä¸å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ

```python
def get_vectordb():
     """
     å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
     """
     # å®šä¹‰ Embeddings
     embedding = ZhipuAIEmbeddings()
     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
     persist_directory = "data_base/vector_db/chroma"
     # åŠ è½½æ•°æ®åº“
     vectordb = Chroma(
          persist_directory = persist_directory,
          embedding_function = embedding,
     )
     return vectordb

def get_chat_qa_chain(question: str, openai_api_key: str):
     """
     å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model_name = "gpt-3.5-turbo", 
          temperature = 0, 
          openai_api_key = openai_api_key
     )
     memory = ConversationBufferMemory(
          memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
          return_messages = True,  # å°†æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
     )
     retriever = vectordb.as_retriever()
     qa = ConversationBufferMemory.from_llm(
          llm, 
          retriever = retriever,
          memory = memory,
     )
     result = qa({
          "question": question
     })
     
     return result["answer"]


def get_qa_chain(question: str, openai_api_key: str):
     """
     ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model = "gpt-3.5-turbo",
          temperature = 0,
          opanai_api_key = oepnai_api_key,
     )
     template = """"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
     QA_CHAIN_PROMPT = PromptTemplate(
          input_variables = ["context", "question"],
          template = template,
     )
     qa_chain = RetrievalQA.from_chain_type(
          llm,
          retriever = vectordb.as_retriever(),
          return_source_documents = True,
          chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT}
     )
     result = qa_chain({"query": question})

     return result["result"]
```

ç„¶åï¼Œæ·»åŠ ä¸€ä¸ªå•é€‰æŒ‰é’®éƒ¨ä»¶ `st.radio`ï¼Œé€‰æ‹©è¿›è¡Œå›ç­”æ¨¡å¼ï¼š

* `None` ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼
* `qa_chain` ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼
* `chat_qa_chain` å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼

```python
selected_method = st.radio(
     "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
     [
          "None", 
          "qa_chain", 
          "chat_qa_chain"
     ],
     caption = [
          "ä¸ä½¿ç”¨æ£€ç´¢å›ç­”çš„æ™®é€šæ¨¡å¼", 
          "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", 
          "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
     ]
)
```

æœ€åï¼Œè¿›å…¥é¡µé¢ï¼Œé¦–å…ˆå…ˆè¾“å…¥ `OPEN_API_KEY`ï¼ˆé»˜è®¤ï¼‰ï¼Œ
ç„¶åç‚¹å‡»å•é€‰æŒ‰é’®é€‰æ‹©è¿›è¡Œé—®ç­”çš„æ¨¡å¼ï¼Œæœ€ååœ¨è¾“å…¥æ¡†è¾“å…¥ä½ çš„é—®é¢˜ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚

å®Œæ•´ä»£ç ï¼š

```python
# python libraries
import os
import sys
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

from embedding_api.zhipuai_embedding import ZhipuAIEmbeddings
from dotenv import load_dotenv, find_dotenv

# å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
sys.path.append("../knowledge_lib") 
# è¯»å–æœ¬åœ° .env æ–‡ä»¶
_ = load_dotenv(find_dotenv())
# è½½å…¥ ***_API_KEY
os.environ["OPENAI_API_BASE"] = "https://api.chatgptid.net/v1"
zhipuai_api_key = os.environ["ZHIPUAI_API_KEY"]
# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]


def generate_response(input_text, openai_api_key):
     """
     å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç”¨æˆ·å¯†é’¥å¯¹ OpenAI API è¿›è¡Œèº«ä»½éªŒè¯ã€å‘é€æç¤ºå¹¶è·å– AI ç”Ÿæˆçš„å“åº”ã€‚
     è¯¥å‡½æ•°æ¥å—ç”¨æˆ·çš„æç¤ºä½œä¸ºå‚æ•°ï¼Œå¹¶ä½¿ç”¨ st.info æ¥åœ¨è“è‰²æ¡†ä¸­æ˜¾ç¤º AI ç”Ÿæˆçš„å“åº”ã€‚
     """
     llm = ChatOpenAI(temperature = 0.7, openai_api_key = openai_api_key)
     output = llm.invoke(input_text)
     output_parser = StrOutputParser()
     output = output_parser.invoke(output)
     # st.info(output)
     return output


def get_vectordb():
     """
     å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
     """
     # å®šä¹‰ Embeddings
     embedding = ZhipuAIEmbeddings()
     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
     persist_directory = "data_base/vector_db/chroma"
     # åŠ è½½æ•°æ®åº“
     vectordb = Chroma(
          persist_directory = persist_directory,
          embedding_function = embedding,
     )

     return vectordb


def get_chat_qa_chain(question: str, openai_api_key: str):
     """
     å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model_name = "gpt-3.5-turbo", 
          temperature = 0, 
          openai_api_key = openai_api_key
     )
     memory = ConversationBufferMemory(
          memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
          return_messages = True,  # å°†æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
     )
     retriever = vectordb.as_retriever()
     qa = ConversationBufferMemory.from_llm(
          llm, 
          retriever = retriever,
          memory = memory,
     )
     result = qa({
          "question": question
     })
     
     return result["answer"]


def get_qa_chain(question: str, openai_api_key: str):
     """
     ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model = "gpt-3.5-turbo",
          temperature = 0,
          opanai_api_key = openai_api_key,
     )
     template = """"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
     QA_CHAIN_PROMPT = PromptTemplate(
          input_variables = ["context", "question"],
          template = template,
     )
     qa_chain = RetrievalQA.from_chain_type(
          llm,
          retriever = vectordb.as_retriever(),
          return_source_documents = True,
          chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT}
     )
     result = qa_chain({"query": question})

     return result["result"]




# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
     # åˆ›å»ºåº”ç”¨ç¨‹åºçš„æ ‡é¢˜
     st.title("ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘")
     # æ·»åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ï¼Œä¾›ç”¨æˆ·è¾“å…¥å…¶ OpenAI API å¯†é’¥
     openai_api_key = st.sidebar.text_input("OpenAI API Key", type = "password")
     # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
     # selected_method = st.sidebar.selectbox(
     #      "é€‰æ‹©æ¨¡å¼", 
     #      [
     #           "None", 
     #           "qa_chain", 
     #           "chat_qa_chain"
     #      ]
     # )
     selected_method = st.radio(
          "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
          [
               "None", 
               "qa_chain", 
               "chat_qa_chain"
          ],
          caption = [
               "ä¸ä½¿ç”¨æ£€ç´¢å›ç­”çš„æ™®é€šæ¨¡å¼", 
               "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", 
               "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
          ]
     )

     # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
     # é€šè¿‡ä½¿ç”¨ st.session_state æ¥å­˜å‚¨å¯¹è¯å†å²ï¼Œ
     # å¯ä»¥åœ¨ç”¨æˆ·ä¸åº”ç”¨ç¨‹åºäº¤äº’æ—¶ä¿ç•™æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡
     if "messages" not in st.session_state:
          st.session_state.messages = []
     
     # å½“ç”¨æˆ·ç‚¹å‡» Submit æ—¶ï¼Œgenerate_response å°†ä½¿ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°æ¥è°ƒç”¨è¯¥å‡½æ•°
     # with st.form("my_form"):
     #      text = st.text_area("Enter text:", "What are the three key pieces of advice for learning how to code?")
     #      submitted = st.form_submit_button("Submit")
     #      if not openai_api_key.startswith("sk-"):
     #           st.warning("Please enter your OpenAI API key!", icon = "")
     #      if submitted and openai_api_key.startswith("sk-"):
     #           generate_response(text, openai_api_key)
   
     messages = st.container(height = 300)
     if prompt := st.chat_input("Say something"):
          # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
          st.session_state.messages.appen({
               "role": "user",
               "text": prompt,
          })
          # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
          if selected_method == "None":
               answer = generate_response(prompt, openai_api_key)
          if selected_method == "qa_chain":
               answer = get_qa_chain(prompt, openai_api_key)
          elif selected_method == "chat_qa_chain":
               answer = get_chat_qa_chain(prompt, openai_api_key)
          
          # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
          if answer is not None:
               # å°† LLM çš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
               st.session_state.messages.append({
                    "role": "assistant",
                    "text": answer,
               })
          
          # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
          for message in st.session_state.messages:
               if message["role"] == "user":
                    messages.chat_message("user").write(message["text"])
               else:
                    messages.chat_message("assistant").write(message["text"])

if __name__ == "__main__":
    main()
```

## éƒ¨ç½²åº”ç”¨ç¨‹åº

è¦å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Streamlit Cloudï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. ä¸ºåº”ç”¨ç¨‹åºåˆ›å»º GitHub å­˜å‚¨åº“ï¼Œå­˜å‚¨åº“åº”åŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š

```
your-repository/
     |_ streamlit_app.py
     |_ requirements.txt
```

2. è½¬åˆ° [Streamlit Community Cloud](https://share.streamlit.io/)ï¼Œå•å‡»å·¥ä½œåŒºä¸­çš„ `New app` æŒ‰é’®ï¼Œ
   ç„¶åæŒ‡å®šå­˜å‚¨åº“ã€åˆ†æ”¯å’Œä¸»æ–‡ä»¶è·¯å¾„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡é€‰æ‹©è‡ªå®šä¹‰å­åŸŸæ¥è‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„ URLã€‚
3. ç‚¹å‡» `Deploy!` æŒ‰é’®ã€‚
4. åº”ç”¨ç¨‹åºç°åœ¨å°†éƒ¨ç½²åˆ° Streamlit Community Cloudï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®åº”ç”¨ã€‚

ä¼˜åŒ–æ–¹å‘ï¼š

* ç•Œé¢ä¸­æ·»åŠ ä¸Šä¼ æœ¬åœ°æ–‡æ¡£ï¼Œå»ºç«‹å‘é‡æ•°æ®åº“çš„åŠŸèƒ½
* æ·»åŠ å¤šç§LLM ä¸ embeddingæ–¹æ³•é€‰æ‹©çš„æŒ‰é’®
* æ·»åŠ ä¿®æ”¹å‚æ•°çš„æŒ‰é’®
* æ›´å¤š...

# å‚è€ƒ

* [llm-universe](https://github.com/datawhalechina/llm-universe?tab=readme-ov-file)
