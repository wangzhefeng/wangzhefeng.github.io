---
title: LLM RAG åº”ç”¨--æ­å»ºçŸ¥è¯†åº“ã€æ£€ç´¢é—®ç­”é“¾ã€éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹
author: ç‹å“²å³°
date: '2024-08-03'
slug: llm-rag-app-knowledge
categories:
  - llm
tags:
  - article
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [æ­å»ºçŸ¥è¯†åº“](#æ­å»ºçŸ¥è¯†åº“)
     - [è¯å‘é‡](#è¯å‘é‡)
          - [è¯å‘é‡ç®€ä»‹](#è¯å‘é‡ç®€ä»‹)
          - [é€šç”¨æ–‡æœ¬å‘é‡](#é€šç”¨æ–‡æœ¬å‘é‡)
          - [è¯å‘é‡çš„ä¼˜åŠ¿](#è¯å‘é‡çš„ä¼˜åŠ¿)
          - [æ„å»ºè¯å‘é‡çš„æ–¹æ³•](#æ„å»ºè¯å‘é‡çš„æ–¹æ³•)
     - [å‘é‡æ•°æ®åº“](#å‘é‡æ•°æ®åº“)
          - [å‘é‡æ•°æ®åº“ç®€ä»‹](#å‘é‡æ•°æ®åº“ç®€ä»‹)
- [è°ƒç”¨ Embedding API](#è°ƒç”¨-embedding-api)
     - [OpenAI API](#openai-api)
     - [æ–‡å¿ƒåƒå¸† API](#æ–‡å¿ƒåƒå¸†-api)
     - [è®¯é£æ˜Ÿç« API](#è®¯é£æ˜Ÿç«-api)
     - [æ™ºè°± API](#æ™ºè°±-api)
- [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
     - [æ•°æ®é€‰å–](#æ•°æ®é€‰å–)
          - [PDF æ–‡æ¡£](#pdf-æ–‡æ¡£)
          - [Markdown æ–‡æ¡£](#markdown-æ–‡æ¡£)
     - [æ•°æ®æ¸…æ´—](#æ•°æ®æ¸…æ´—)
     - [æ–‡æ¡£åˆ†å‰²](#æ–‡æ¡£åˆ†å‰²)
          - [æ–‡æ¡£åˆ†å‰²ç®€ä»‹](#æ–‡æ¡£åˆ†å‰²ç®€ä»‹)
          - [æ–‡æ¡£åˆ†å‰² API](#æ–‡æ¡£åˆ†å‰²-api)
          - [æ–‡æ¡£åˆ†å‰²ç¤ºä¾‹](#æ–‡æ¡£åˆ†å‰²ç¤ºä¾‹)
- [æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“](#æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“)
     - [é…ç½®](#é…ç½®)
     - [æ„å»º Chroma å‘é‡åº“](#æ„å»º-chroma-å‘é‡åº“)
     - [å‘é‡æ£€ç´¢](#å‘é‡æ£€ç´¢)
          - [ç›¸ä¼¼åº¦æ£€ç´¢](#ç›¸ä¼¼åº¦æ£€ç´¢)
          - [MMR æ£€ç´¢](#mmr-æ£€ç´¢)
- [åŸºäº LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾](#åŸºäº-langchain-æ„å»ºæ£€ç´¢é—®ç­”é“¾)
     - [åŠ è½½æ•°æ®åº“å‘é‡](#åŠ è½½æ•°æ®åº“å‘é‡)
     - [åˆ›å»ºä¸€ä¸ª LLM](#åˆ›å»ºä¸€ä¸ª-llm)
     - [æ„å»ºæ£€ç´¢é—®ç­”é“¾](#æ„å»ºæ£€ç´¢é—®ç­”é“¾)
     - [æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•](#æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•)
     - [æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½](#æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½)
- [åŸºäº Streamlit éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹](#åŸºäº-streamlit-éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹)
     - [æ„å»ºåº”ç”¨ç¨‹åº](#æ„å»ºåº”ç”¨ç¨‹åº)
     - [æ·»åŠ æ£€ç´¢å›ç­”](#æ·»åŠ æ£€ç´¢å›ç­”)
     - [éƒ¨ç½²åº”ç”¨ç¨‹åº](#éƒ¨ç½²åº”ç”¨ç¨‹åº)
- [å‚è€ƒ](#å‚è€ƒ)
</p></details><p></p>

# æ­å»ºçŸ¥è¯†åº“

## è¯å‘é‡

### è¯å‘é‡ç®€ä»‹

åœ¨æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ï¼Œè¯å‘é‡(Word Embeddings)æ˜¯ä¸€ç§å°†éç»“æ„åŒ–æ•°æ®ï¼Œ
å¦‚å•è¯ã€å¥å­æˆ–è€…æ•´ä¸ªæ–‡æ¡£ï¼Œè½¬åŒ–ä¸ºå®æ•°å‘é‡çš„æŠ€æœ¯ã€‚è¿™äº›å®æ•°å‘é‡å¯ä»¥è¢«è®¡ç®—æœºæ›´å¥½åœ°ç†è§£å’Œå¤„ç†ã€‚

è¯å‘é‡(Embedding)èƒŒåçš„ä¸»è¦æƒ³æ³•æ˜¯ï¼Œç›¸ä¼¼æˆ–ç›¸å…³çš„å¯¹è±¡åœ¨åµŒå…¥ç©ºé—´ä¸­çš„è·ç¦»åº”è¯¥å¾ˆè¿‘ã€‚
ä¸¾ä¸ªä¾‹å­ï¼Œå¯ä»¥ä½¿ç”¨è¯å‘é‡æ¥è¡¨ç¤ºæ–‡æœ¬æ•°æ®ã€‚åœ¨è¯å‘é‡ä¸­ï¼Œæ¯ä¸ªå•è¯è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ï¼Œ
è¿™ä¸ªå‘é‡æ•è·äº†è¿™ä¸ªå•è¯çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œ"king" å’Œ "queen" è¿™ä¸¤ä¸ªå•è¯åœ¨å‘é‡ç©ºé—´ä¸­çš„ä½ç½®å°†ä¼šéå¸¸æ¥è¿‘ï¼Œ
å› ä¸ºå®ƒä»¬çš„å«ä¹‰ç›¸ä¼¼ã€‚è€Œ "apple" å’Œ "orange" ä¹Ÿä¼šå¾ˆæ¥è¿‘ï¼Œ
å› ä¸ºå®ƒä»¬éƒ½æ˜¯æ°´æœã€‚è€Œ "king" å’Œ "apple" è¿™ä¸¤ä¸ªå•è¯åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»å°±ä¼šæ¯”è¾ƒè¿œï¼Œå› ä¸ºå®ƒä»¬çš„å«ä¹‰ä¸åŒã€‚

### é€šç”¨æ–‡æœ¬å‘é‡

è¯å‘é‡å®é™…ä¸Šæ˜¯å°†å•è¯è½¬åŒ–ä¸ºå›ºå®šçš„é™æ€çš„å‘é‡ï¼Œè™½ç„¶å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ•æ‰å¹¶è¡¨è¾¾æ–‡æœ¬ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œ
ä½†å¿½ç•¥äº†å•è¯åœ¨ä¸åŒè¯­å¢ƒä¸­çš„æ„æ€ä¼šå—åˆ°å½±å“è¿™ä¸€ç°å®ã€‚

å› æ­¤åœ¨ RAG åº”ç”¨ä¸­ä½¿ç”¨çš„å‘é‡æŠ€æœ¯ä¸€èˆ¬ä¸º**é€šç”¨æ–‡æœ¬å‘é‡(Universal Text Embedding)**ï¼Œ
è¯¥æŠ€æœ¯å¯ä»¥å¯¹ä¸€å®šèŒƒå›´å†…ä»»æ„é•¿åº¦çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ï¼Œä¸è¯å‘é‡ä¸åŒçš„æ˜¯å‘é‡åŒ–çš„å•ä½ä¸å†æ˜¯å•è¯è€Œæ˜¯**è¾“å…¥çš„æ–‡æœ¬**ï¼Œ
è¾“å‡ºçš„å‘é‡ä¼šæ•æ‰æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚

### è¯å‘é‡çš„ä¼˜åŠ¿

åœ¨ RAG é‡Œé¢è¯å‘é‡çš„ä¼˜åŠ¿ä¸»è¦æœ‰ä¸¤ç‚¹ï¼š

* è¯å‘é‡æ¯”æ–‡å­—æ›´é€‚åˆæ£€ç´¢
    - å½“åœ¨æ•°æ®åº“æ£€ç´¢æ—¶ï¼Œå¦‚æœæ•°æ®åº“å­˜å‚¨çš„æ˜¯æ–‡å­—ï¼Œ
      ä¸»è¦é€šè¿‡æ£€ç´¢å…³é”®è¯ï¼ˆè¯æ³•æœç´¢ï¼‰ç­‰æ–¹æ³•æ‰¾åˆ°ç›¸å¯¹åŒ¹é…çš„æ•°æ®ï¼Œ
      åŒ¹é…çš„ç¨‹åº¦æ˜¯å–å†³äºå…³é”®è¯çš„æ•°é‡æˆ–è€…æ˜¯å¦å®Œå…¨åŒ¹é…æŸ¥è¯¢å¥çš„ï¼›
    - è¯å‘é‡ä¸­åŒ…å«äº†åŸæ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—é—®é¢˜ä¸æ•°æ®åº“ä¸­æ•°æ®çš„ç‚¹ç§¯ã€
      ä½™å¼¦è·ç¦»ã€æ¬§å‡ é‡Œå¾—è·ç¦»ç­‰æŒ‡æ ‡ï¼Œç›´æ¥è·å–é—®é¢˜ä¸æ•°æ®åœ¨è¯­ä¹‰å±‚é¢ä¸Šçš„ç›¸ä¼¼åº¦ã€‚
* è¯å‘é‡æ¯”å…¶å®ƒåª’ä»‹çš„ç»¼åˆä¿¡æ¯èƒ½åŠ›æ›´å¼º
    - å½“ä¼ ç»Ÿæ•°æ®åº“å­˜å‚¨æ–‡å­—ã€å£°éŸ³ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§åª’ä»‹æ—¶ï¼Œ
      å¾ˆéš¾å»å°†ä¸Šè¿°å¤šç§åª’ä»‹æ„å»ºèµ·å…³è”ä¸è·¨æ¨¡æ€çš„æŸ¥è¯¢æ–¹æ³•ï¼›
    - è¯å‘é‡å¯ä»¥é€šè¿‡å¤šç§å‘é‡æ¨¡å‹å°†å¤šç§æ•°æ®æ˜ å°„æˆç»Ÿä¸€çš„å‘é‡å½¢å¼ã€‚

### æ„å»ºè¯å‘é‡çš„æ–¹æ³•

åœ¨æ­å»º RAG ç³»ç»Ÿæ—¶ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ Embedding æ¨¡å‹æ¥æ„å»ºè¯å‘é‡ï¼Œå¯ä»¥é€‰æ‹©ï¼š

* ä½¿ç”¨å„ä¸ªå…¬å¸çš„ Embedding APIï¼›
* åœ¨æœ¬åœ°ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ•°æ®æ„å»ºä¸ºè¯å‘é‡ã€‚

## å‘é‡æ•°æ®åº“

### å‘é‡æ•°æ®åº“ç®€ä»‹

å‘é‡æ•°æ®åº“ä»‹ç»åœ¨[è¿™é‡Œ]()ã€‚

# è°ƒç”¨ Embedding API

ä¸ºäº†æ–¹ä¾¿ Embedding API è°ƒç”¨ï¼Œåº”å°† API key å¡«å…¥ `.env` æ–‡ä»¶ï¼Œä»£ç å°†è‡ªåŠ¨è¯»å–å¹¶åŠ è½½ç¯å¢ƒå˜é‡ã€‚

## OpenAI API

GPT æœ‰å°è£…å¥½çš„æ¥å£ï¼Œä½¿ç”¨æ—¶ç®€å•å°è£…å³å¯ã€‚ç›®å‰ GPT Embedding model æœ‰ä¸‰ç§ï¼Œæ€§èƒ½å¦‚ä¸‹

| æ¨¡å‹                   | æ¯ç¾å…ƒé¡µæ•° | MTEBå¾—åˆ† | MIRACLå¾—åˆ†  |
|------------------------|-----------|---------|-------------|
| text-embedding-3-large | 9,615     | 64.6    | 54.9        |
| text-embedding-3-small | 62,500    | 62.3    | 44.0        |
| text-embedding-ada-002 | 12,500    | 61.0    | 31.4        |

å…¶ä¸­ï¼š

* MTEB å¾—åˆ†ä¸º Embedding model åˆ†ç±»ã€èšç±»ã€é…å¯¹ç­‰å…«ä¸ªä»»åŠ¡çš„å¹³å‡å¾—åˆ†
* MIRACL å¾—åˆ†ä¸º Embedding model åœ¨æ£€ç´¢ä»»åŠ¡ä¸Šçš„å¹³å‡å¾—åˆ†

ä»ä»¥ä¸Šä¸‰ä¸ª Embedding model å¯ä»¥çœ‹å‡ºï¼š

* `text-embedding-3-large` æœ‰æœ€å¥½çš„æ€§èƒ½å’Œæœ€è´µçš„ä»·æ ¼ï¼Œ
  å½“æ­å»ºçš„åº”ç”¨éœ€è¦æ›´å¥½çš„è¡¨ç°ä¸”æˆæœ¬å……è¶³çš„æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨ï¼›
* `text-embedding-3-small` æœ‰è¾ƒå¥½çš„æ€§ä»·æ¯”ï¼Œå½“é¢„ç®—æœ‰é™æ—¶å¯ä»¥é€‰æ‹©è¯¥æ¨¡å‹ï¼›
* `text-embedding-ada-002` æ˜¯ OpenAI ä¸Šä¸€ä»£çš„æ¨¡å‹ï¼Œ
  æ— è®ºåœ¨æ€§èƒ½è¿˜æ˜¯ä»·æ ¼éƒ½ä¸åŠå‰ä¸¤è€…ï¼Œå› æ­¤ä¸æ¨èä½¿ç”¨ã€‚

```python
import os

from openai import OpenAI
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv())

os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"


def openai_embedding(text: str, model: str = None):
    # è·å–ç¯å¢ƒå˜é‡ OPENAI_API_KEY
    api_key = os.environ["OPENAI_API_KEY"]
    client = OpenAI(api_key = api_key)
    # embedding model
    if model == None:
        model = "text-embedding-3-small"
    # æ¨¡å‹è°ƒç”¨    
    response = client.embeddings.create(
        input = text,
        model = model,
    )

    return response


response = openai_embedding(text = "è¦ç”Ÿæˆ embedding çš„è¾“å…¥æ–‡æœ¬ï¼Œå­—ç¬¦ä¸²å½¢å¼ã€‚")
print(f"è¿”å›çš„ embedding ç±»å‹ä¸ºï¼š{response.object}")
print(f"embedding é•¿åº¦ä¸ºï¼š{len(response.data[0].embedding)}")
print(f"embedding (å‰ 10) ä¸ºï¼š{response.data[0].embedding[:10]}")
print(f"æœ¬æ¬¡ embedding model ä¸ºï¼š{response.model}")
print(f"æœ¬æ¬¡ token ä½¿ç”¨æƒ…å†µä¸ºï¼š{response.usage}")
```

API è¿”å›çš„æ•°æ®ä¸º JSON æ ¼å¼ï¼Œé™¤ `object` å‘é‡ç±»å‹å¤–è¿˜æœ‰å­˜æ”¾æ•°æ®çš„ `data`ã€
embedding model å‹å· `model` ä»¥åŠæœ¬æ¬¡ token ä½¿ç”¨æƒ…å†µ `usage` ç­‰æ•°æ®ï¼Œ
å…·ä½“å¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [
        -0.006929283495992422,
        ... (çœç•¥)
        -4.547132266452536e-05,
      ],
    }
  ],
  "model": "text-embedding-3-small",
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 5
  }
}
```

## æ–‡å¿ƒåƒå¸† API

Embedding-V1 æ˜¯åŸºäºç™¾åº¦æ–‡å¿ƒå¤§æ¨¡å‹æŠ€æœ¯çš„æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹ï¼ŒAccess token ä¸ºè°ƒç”¨æ¥å£çš„å‡­è¯ï¼Œ
ä½¿ç”¨ Embedding-V1 æ—¶åº”å…ˆå‡­ API Keyã€Secret Key è·å– Access tokenï¼Œ
å†é€šè¿‡ Access token è°ƒç”¨æ¥å£æ¥ Embedding textã€‚
åŒæ—¶åƒå¸†å¤§æ¨¡å‹å¹³å°è¿˜æ”¯æŒ `bge-large-zh` ç­‰ Embedding modelã€‚

```python
import json
import requests

def wenxin_embedding(text: str):
    # è·å–ç¯å¢ƒå˜é‡ wenxin_api_key, wenxin_secret_key
    api_key = os.environ["QIANFN_AK"]
    secret_key = os.environp["QIANFAN_SK"]
    # ä½¿ç”¨ API Keyã€Secret Key å‘ https://aip.baidubce.com/oauth/2.0/token è·å– Access token
    url = f"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={api_key}&client_secret={secret_key}"
    payload = json.dumps("")
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    response = requests.request("POST", url, headers = headers, data = payload)
    # é€šè¿‡è·å–çš„ Access token æ¥ embedding text
    url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/embedding-v1?access_token={str(response.json().get('access_token'))}"
    input = []
    input.append(text)
    payload = json.dumps({"input": input})
    headers = {
        "Content-Type": "application/json"
    }
    response = requests.request("POST", url, headers = headers, data = payload)

    return json.loads(response.text)


# text åº”ä¸º List(str)
text = "è¦ç”Ÿæˆ embedding çš„è¾“å…¥æ–‡æœ¬ï¼Œå­—ç¬¦ä¸²å½¢å¼ã€‚"
response = wenxin_embedding(text = text)

print(f"æœ¬æ¬¡ embedding id ä¸ºï¼š{response["id"]}")
print(f"æœ¬æ¬¡ embedding äº§ç”Ÿçš„æ—¶é—´æˆ³ä¸ºï¼š{response["created"]}")
print(f"è¿”å›çš„ embedding ç±»å‹ä¸ºï¼š{response["object"]}")
print(f"embedding é•¿åº¦ä¸ºï¼š{response["data"][0]["embedding"]}")
print(f"embedding (å‰ 10) ä¸ºï¼š{response["data"][0]["embedding"][:10]}")
```

## è®¯é£æ˜Ÿç« API

æœªå¼€æ”¾

## æ™ºè°± API

æ™ºè°±æœ‰å°è£…å¥½çš„ SDKï¼Œç›´æ¥è°ƒç”¨å³å¯ã€‚

```python
from zhipuai import ZhipuAI


def zhipu_embedding(text: str):
    api_key = os.environ["ZHIPUAI_API_KEY"]
    client = ZhipuAI(api_key = api_key)
    response = client.embeddings.create(
        model = "embedding-2",
        input = text,
    )
    
    return response

text = "è¦ç”Ÿæˆ embedding çš„è¾“å…¥æ–‡æœ¬ï¼Œå­—ç¬¦ä¸²å½¢å¼ã€‚"
response = zhipu_embedding(text = text)

print(f"response ç±»å‹ä¸ºï¼š{type(response)}")
print(f"embedding ç±»å‹ä¸ºï¼š{response.object}")
print(f"ç”Ÿæˆ embedding çš„ model ä¸ºï¼š{response.model}")
print(f"ç”Ÿæˆçš„ embedding é•¿åº¦ä¸ºï¼š{len(response.data[0].embedding)}")
print(f"embedding(å‰ 10)ä¸º: {response.data[0].embedding[:10]}")
```

# æ•°æ®å¤„ç†

ä¸ºæ„å»ºæœ¬åœ°çŸ¥è¯†åº“ï¼Œéœ€è¦å¯¹ä»¥å¤šç§ç±»å‹å­˜å‚¨çš„æœ¬åœ°æ–‡æ¡£è¿›è¡Œå¤„ç†ï¼Œ
è¯»å–**æœ¬åœ°æ–‡æ¡£**å¹¶é€šè¿‡ **Embedding æ–¹æ³•**å°†**æœ¬åœ°æ–‡æ¡£çš„å†…å®¹**è½¬åŒ–ä¸º**è¯å‘é‡**æ¥æ„å»º**å‘é‡æ•°æ®åº“**ã€‚

## æ•°æ®é€‰å–

### PDF æ–‡æ¡£

ä½¿ç”¨ LangChain çš„ `PyMuPDFLoader` æ¥è¯»å–çŸ¥è¯†åº“çš„ PDF æ–‡ä»¶ã€‚
`PyMuPDFLoader` æ˜¯ PDF è§£æå™¨ä¸­é€Ÿåº¦æœ€å¿«çš„ä¸€ç§ï¼Œ
ç»“æœåŒ…å« PDF åŠé¡µé¢çš„è¯¦ç»†å…ƒæ•°æ®ï¼Œå¹¶ä¸”æ¯é¡µè¿”å›ä¸€ä¸ªæ–‡æ¡£ã€‚

```python
from langchain.document_loaders.pdf import PyMuPDFLoader

# åˆ›å»ºä¸€ä¸ª PyMuPDFLoader Class å®ä¾‹ï¼Œè¾“å…¥ä¸ºå¾…åŠ è½½çš„ PDF æ–‡æ¡£è·¯å¾„
loader = PyMuPDFLoader("data_base/knowledge_db/pumpkin_book/pumpkin_book.pdf")
# è°ƒç”¨ PyMuPDFLoader Class çš„å‡½æ•° load å¯¹ PDF æ–‡ä»¶è¿›è¡ŒåŠ è½½
pdf_pages = loader.load()
```

æ–‡æ¡£åŠ è½½åå‚¨å­˜åœ¨ `pdf_pages` å˜é‡ä¸­ï¼š

* `pdf_pages` çš„å˜é‡ç±»å‹ä¸º `List`
* æ‰“å° `pdf_pages` çš„é•¿åº¦å¯ä»¥çœ‹åˆ° PDF ä¸€å…±åŒ…å«å¤šå°‘é¡µ

```python
print(f"è½½å…¥åçš„å˜é‡ç±»å‹ä¸ºï¼š{type(pdf_pages)}, è¯¥ PDF ä¸€å…±åŒ…å« {len(pdf_pages)} é¡µã€‚")
```

`pdf_pages` ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œå˜é‡ç±»å‹ä¸º `langchain_core.documents.base.Document`ï¼Œ
æ–‡æ¡£å˜é‡ç±»å‹åŒ…å«ä¸¤ä¸ªå±æ€§ï¼š

* `meta_data` ä¸ºæ–‡æ¡£ç›¸å…³çš„æè¿°æ€§æ•°æ®
* `page_content` åŒ…å«è¯¥æ–‡æ¡£çš„å†…å®¹

```python
pdf_page = pdf_pages[1]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(pdf_page)}", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{pdf_page.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹ï¼š\n{pdf_page.page_content}",
    sep = "\n------\n"
)
```

### Markdown æ–‡æ¡£

å¯ä»¥æŒ‰ç…§è¯»å– PDF æ–‡æ¡£å‡ ä¹ä¸€è‡´çš„æ–¹å¼è¯»å– Markdown æ–‡æ¡£ã€‚

```python
from langchain.document_loaders.markdown import UnstructureMarkdownLoader

loader = UnstructureMarkdownLoader("data_base/knowledge_db/prompt_engineering/1.ç®€ä»‹ Instroduction.md")
md_pages = loader.load()
```

è¯»å–çš„å¯¹è±¡å’Œ PDF æ–‡æ¡£è¯»å–å‡ºæ¥æ˜¯å®Œå…¨ä¸€è‡´çš„ï¼š

```python
print(f"è½½å…¥åçš„å˜é‡ç±»å‹ä¸ºï¼š{type(md_pages)}, è¯¥ Markdown ä¸€å…±åŒ…å« {len(md_pages)} é¡µã€‚")
```

```python
md_page = md_pages[0]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(md_page)}", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{md_page.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹ï¼š\n{md_page.page_content[0:][:200]}", 
    sep = "\n------\n"
)
```

## æ•°æ®æ¸…æ´—

æœŸæœ›çŸ¥è¯†åº“çš„æ•°æ®å°½é‡æ˜¯æœ‰åºçš„ã€ä¼˜è´¨çš„ã€ç²¾ç®€çš„ï¼Œå› æ­¤è¦åˆ é™¤ä½è´¨é‡çš„ã€ç”šè‡³å½±å“ç†è§£æ–‡æœ¬æ•°æ®ã€‚
å¯ä»¥çœ‹åˆ°ä¸Šä¸‹æ–‡ä¸­è¯»å–çš„ PDF æ–‡ä»¶ä¸ä»…å°†ä¸€å¥è¯æŒ‰ç…§åŸæ–‡çš„åˆ†è¡Œæ·»åŠ äº†æ¢è¡Œç¬¦ `\n`ï¼Œ
ä¹Ÿåœ¨åŸæœ¬ä¸¤ä¸ªç¬¦å·ä¸­æ’å…¥äº† `\n`ï¼Œå¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹¶åˆ é™¤æ‰ `\n`ã€‚

```python
import re
pattern = re.compile(r"[^\u4e00-\u9fff](\n)[^\u4e00-\u9fff]", re.DOTALL)
pdf_page.page_content = re.sub(
    pattern, 
    lambda match: match.group(0).replace("\n", ""), 
    pdf_page.page_content
)
```

è¿›ä¸€æ­¥åˆ†ææ•°æ®ï¼Œå‘ç°æ•°æ®ä¸­è¿˜æœ‰ä¸å°‘çš„ `â€¢` å’Œç©ºæ ¼ï¼Œç®€å•å®ç”¨çš„ `replace` æ–¹æ³•å³å¯ã€‚

```python
pdf_page.page_content = pdf_page.page_content.replace("â€¢", "")
pdf_page.page_content = pdf_page.page_content.replace(" ", "")
print(pdf_page.page_content)
```

ä¸Šä¸‹æ–‡ä¸­è¯»å–çš„ Markdown æ–‡ä»¶æ¯ä¸€æ®µä¸­é—´éš”äº†ä¸€ä¸ªæ¢è¡Œç¬¦ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ `replace` æ–¹æ³•å»é™¤ã€‚

```python
md_page.page_content = md_page.page_content.replace("\n\n", "\n")
print(md_page.page_content)
```

## æ–‡æ¡£åˆ†å‰²

### æ–‡æ¡£åˆ†å‰²ç®€ä»‹

ç”±äºå•ä¸ªæ–‡æ¡£çš„é•¿åº¦å¾€å¾€ä¼šè¶…è¿‡æ¨¡å‹æ”¯æŒçš„ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ£€ç´¢å¾—åˆ°çš„çŸ¥è¯†å¤ªé•¿è¶…å‡ºæ¨¡å‹çš„å¤„ç†èƒ½åŠ›ï¼Œ
å› æ­¤ï¼Œåœ¨æ„å»ºå‘é‡çŸ¥è¯†åº“çš„è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€éœ€è¦å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ï¼Œ
**å°†å•ä¸ªæ–‡æ¡£æŒ‰é•¿åº¦æˆ–è€…æŒ‰å›ºå®šçš„è§„åˆ™åˆ†å‰²æˆè‹¥å¹²ä¸ª chunkï¼Œç„¶åå°†æ¯ä¸ª chunk è½¬åŒ–ä¸ºè¯å‘é‡ï¼Œ
å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­**ã€‚åœ¨æ£€ç´¢æ—¶ï¼Œä¼šä»¥ chunk ä½œä¸ºæ£€ç´¢çš„å…ƒå•ä½ï¼Œ
ä¹Ÿå°±æ˜¯æ¯ä¸€æ¬¡æ£€ç´¢åˆ° `k` ä¸ª chunk ä½œä¸ºæ¨¡å‹å¯ä»¥å‚è€ƒæ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†ï¼Œ
è¿™ä¸ª `k` æ˜¯å¯ä»¥è‡ªç”±è®¾å®šçš„ã€‚

### æ–‡æ¡£åˆ†å‰² API

Langchain ä¸­æ–‡æœ¬åˆ†å‰²å™¨éƒ½æ ¹æ® `chunk_size`(å—å¤§å°)å’Œ `chunk_overlap`(å—ä¸å—ä¹‹é—´çš„é‡å å¤§å°)è¿›è¡Œåˆ†å‰²ã€‚
`CharacterTextSpitter` API ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
langchain.text_splitter.CharacterTextSpitter(
    separator: str = "\n\n",
    chunk_size = 4000,
    chunk_overlap = 200,
    length_function = <buildin function len>,
)
```

æ–¹æ³•ï¼š

* `create_documents()`: Create documents from a list of texts.
* `split_documents()`: Split documents.

å‚æ•°ï¼š

* `chunk_size` æŒ‡æ¯ä¸ªå—åŒ…å«çš„å­—ç¬¦æˆ– Token(å¦‚å•è¯ã€å¥å­ç­‰)çš„æ•°é‡
* `chunk_overlap` æŒ‡ä¸¤ä¸ªå—ä¹‹é—´å…±äº«çš„å­—ç¬¦æ•°é‡ï¼Œç”¨äºä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ï¼Œé¿å…åˆ†å‰²ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯

Langchain æä¾›å¤šç§æ–‡æ¡£åˆ†å‰²æ–¹å¼ï¼ŒåŒºåˆ«åœ¨æ€ä¹ˆç¡®å®šå—ä¸å—ä¹‹é—´çš„è¾¹ç•Œã€
å—ç”±å“ªäº›å­—ç¬¦/token ç»„æˆã€ä»¥åŠå¦‚ä½•æµ‹é‡å—å¤§å°ï¼š

* `RecursiveCharacterTextSplitter()`: æŒ‰å­—ç¬¦ä¸²åˆ†å‰²æ–‡æœ¬ï¼Œé€’å½’åœ°å°è¯•æŒ‰ä¸åŒçš„åˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²æ–‡æœ¬
* `CharacterTextSplitter()`: æŒ‰å­—ç¬¦æ¥åˆ†å‰²æ–‡æœ¬
* `MarkdownHeaderTextSplitter()`: åŸºäºæŒ‡å®šçš„æ ‡é¢˜æ¥åˆ†å‰² Markdown æ–‡ä»¶
* `TokenTextSplitter()`: æŒ‰ token æ¥åˆ†å‰²æ–‡æœ¬
* `SentenceTransformersTokenTextSplitter()`: æŒ‰ token æ¥åˆ†å‰²æ–‡æœ¬
* `Language()`: ç”¨äº CPPã€Pythonã€Rubyã€Markdown ç­‰
* `NLTKTextSplitter()`: ä½¿ç”¨ `NLTK`ï¼ˆè‡ªç„¶è¯­è¨€å·¥å…·åŒ…ï¼‰æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
* `SpacyTextSplitter()`: ä½¿ç”¨ `Spacy` æŒ‰å¥å­çš„åˆ‡å‰²æ–‡æœ¬

### æ–‡æ¡£åˆ†å‰²ç¤ºä¾‹

```python
''' 
* RecursiveCharacterTextSplitter é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²ã€‚
  å°†æŒ‰ä¸åŒçš„å­—ç¬¦é€’å½’åœ°åˆ†å‰²(æŒ‰ç…§è¿™ä¸ªä¼˜å…ˆçº§ ["\n\n", "\n", " ", ""])ï¼Œ
  è¿™æ ·å°±èƒ½å°½é‡æŠŠæ‰€æœ‰å’Œè¯­ä¹‰ç›¸å…³çš„å†…å®¹å°½å¯èƒ½é•¿æ—¶é—´åœ°ä¿ç•™åœ¨åŒä¸€ä½ç½®
* RecursiveCharacterTextSplitter éœ€è¦å…³æ³¨çš„æ˜¯4ä¸ªå‚æ•°ï¼š
    - separators: åˆ†éš”ç¬¦å­—ç¬¦ä¸²æ•°ç»„
    - chunk_size: æ¯ä¸ªæ–‡æ¡£çš„å­—ç¬¦æ•°é‡é™åˆ¶
    - chunk_overlap: ä¸¤ä»½æ–‡æ¡£é‡å åŒºåŸŸçš„é•¿åº¦
    - length_function: é•¿åº¦è®¡ç®—å‡½æ•°
'''

# å¯¼å…¥æ–‡æœ¬åˆ†å‰²å™¨
from langchain.text_splitter import RecursiveCharacterTextSplitter

# çŸ¥è¯†åº“ä¸­å•æ®µæ–‡æœ¬é•¿åº¦
CHUNK_SIZE = 500
# çŸ¥è¯†åº“ä¸­ç›¸é‚»æ–‡æœ¬é‡åˆé•¿åº¦
OVERLAP_SIZE = 50

# ä½¿ç”¨é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = CHUNK_SIZE,
    chunk_overlap = OVERLAP_SIZE,
)
text_splitter.split_text(pdf_page.page_content[0:1000])

split_docs = text_splitter.split_documents(pdf_pages)
print(f"åˆ‡åˆ†åçš„æ–‡ä»¶æ•°é‡ï¼š{len(split_docs)}")
print(f"åˆ‡åˆ†åçš„å­—ç¬¦æ•°ï¼ˆå¯ä»¥ç”¨æ¥å¤§è‡´è¯„ä¼° token æ•°ï¼‰ï¼š{sum([len(doc.page_content) for doc in split_docs])}")
```

å¦‚ä½•å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ï¼Œå…¶å®æ˜¯æ•°æ®å¤„ç†ä¸­æœ€æ ¸å¿ƒçš„ä¸€æ­¥ï¼Œå…¶å¾€å¾€å†³å®šäº†æ£€ç´¢ç³»ç»Ÿçš„ä¸‹é™ã€‚
ä½†æ˜¯ï¼Œå¦‚ä½•é€‰æ‹©åˆ†å‰²æ–¹å¼ï¼Œå¾€å¾€å…·æœ‰å¾ˆå¼ºçš„ä¸šåŠ¡ç›¸å…³æ€§â€”â€”é’ˆå¯¹ä¸åŒçš„ä¸šåŠ¡ã€ä¸åŒçš„æºæ•°æ®ï¼Œ
å¾€å¾€éœ€è¦è®¾å®šä¸ªæ€§åŒ–çš„æ–‡æ¡£åˆ†å‰²æ–¹å¼ã€‚å› æ­¤ï¼Œè¿™é‡Œä»…ç®€å•æ ¹æ® `chunk_size` å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å‰²ã€‚

# æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“

## é…ç½®

```python
import os
from dotenv import load_dotenv, find_dotenv
from langchain.document_loaders.pdf import PyMuPDFLoader
from langchain.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡
_ = load_dotenv(find_dotenv())

# å¦‚æœéœ€è¦é€šè¿‡ä»£ç†ç«¯å£è®¿é—®ï¼Œä½ éœ€è¦å¦‚ä¸‹é…ç½®
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ["HTTP_PROXY"] = 'http://127.0.0.1:7890'


# è·å– folder_path ä¸‹æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼Œå‚¨å­˜åœ¨ file_paths é‡Œ
file_paths = []
folder_path = '../../data_base/knowledge_db'
for root, dirs, files in os.walk(folder_path):
    for file in files:
        file_path = os.path.join(root, file)
        file_paths.append(file_path)
print(file_paths[:3])

# éå†æ–‡ä»¶è·¯å¾„å¹¶æŠŠå®ä¾‹åŒ–çš„loaderå­˜æ”¾åœ¨loadersé‡Œ
loaders = []
for file_path in file_paths:
    file_type = file_path.split('.')[-1]
    if file_type == 'pdf':
        loaders.append(PyMuPDFLoader(file_path))
    elif file_type == 'md':
        loaders.append(UnstructuredMarkdownLoader(file_path))

# ä¸‹è½½æ–‡ä»¶å¹¶å­˜å‚¨åˆ°text
texts = []
for loader in loaders: 
    texts.extend(loader.load())

# æŸ¥çœ‹æ•°æ®
text = texts[1]
print(
    f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(text)}.", 
    f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{text.metadata}", 
    f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹:\n{text.page_content[0:]}", 
    sep="\n------\n"
)

# åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500, 
    chunk_overlap = 50
)
split_docs = text_splitter.split_documents(texts)
```

## æ„å»º Chroma å‘é‡åº“

Langchain é›†æˆäº†è¶…è¿‡ 30 ä¸ªä¸åŒçš„å‘é‡å­˜å‚¨åº“ã€‚é€‰æ‹© Chroma æ˜¯å› ä¸ºå®ƒè½»é‡çº§ä¸”æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œ
è¿™ä½¿å¾—å®ƒéå¸¸å®¹æ˜“å¯åŠ¨å’Œå¼€å§‹ä½¿ç”¨ã€‚LangChain å¯ä»¥ç›´æ¥ä½¿ç”¨ OpenAI å’Œç™¾åº¦åƒå¸†çš„ Embeddingï¼Œ
åŒæ—¶ï¼Œä¹Ÿå¯ä»¥é’ˆå¯¹å…¶ä¸æ”¯æŒçš„ Embedding API è¿›è¡Œè‡ªå®šä¹‰ã€‚

```python
# ä½¿ç”¨ OpenAI Embedding
# from langchain.embeddings.openai import OpenAIEmbeddings

# ä½¿ç”¨ç™¾åº¦åƒå¸† Embedding
# from langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint

# ä½¿ç”¨æˆ‘ä»¬è‡ªå·±å°è£…çš„æ™ºè°± Embeddingï¼Œéœ€è¦å°†å°è£…ä»£ç ä¸‹è½½åˆ°æœ¬åœ°ä½¿ç”¨
from zhipuai_embedding import ZhipuAIEmbeddings

# å®šä¹‰ Embeddings
# embedding = OpenAIEmbeddings() 
# embedding = QianfanEmbeddingsEndpoint()
embedding = ZhipuAIEmbeddings()

# å®šä¹‰æŒä¹…åŒ–è·¯å¾„
persist_directory = '../../data_base/vector_db/chroma'
```

åˆ é™¤æ—§çš„æ•°æ®åº“æ–‡ä»¶ï¼ˆå¦‚æœæ–‡ä»¶å¤¹ä¸­æœ‰æ–‡ä»¶çš„è¯ï¼‰ï¼Œwindows ç”µè„‘è¯·æ‰‹åŠ¨åˆ é™¤

```bash
$ !rm -rf '../../data_base/vector_db/chroma'
```

```python
from langchain.vectorstores.chroma import Chroma

vectordb = Chroma.from_documents(
    documents = split_docs,
    embedding = embedding,
    persist_directory = persist_directory  # å…è®¸å°† persist_directory ç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
)
```

åœ¨æ­¤ä¹‹åï¼Œè¦ç¡®ä¿é€šè¿‡è¿è¡Œ `vectordb.persist` æ¥æŒä¹…åŒ–å‘é‡æ•°æ®åº“ï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨ã€‚

```python
vectordb.persist()
print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")
```

## å‘é‡æ£€ç´¢

### ç›¸ä¼¼åº¦æ£€ç´¢

Chroma çš„ç›¸ä¼¼åº¦æœç´¢ä½¿ç”¨çš„æ˜¯ä½™å¼¦è·ç¦»ï¼Œå³ï¼š

`$$\begin{align}
similarity
&=cos(A, B) \\
&=\frac{A \cdot B}{||A|| ||B||} \\
&=\frac{\sum_{i=1}^{n}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{n}a_{i}^{2}}\sqrt{\sum_{i=1}^{n}b_{i}^{2}}}
\end{align}$$`

å…¶ä¸­ `$a_{i}, b_{i}$` åˆ†åˆ«æ˜¯å‘é‡ `$A, B$` çš„åˆ†é‡ã€‚

å½“éœ€è¦æ•°æ®åº“è¿”å›ä¸¥è°¨çš„æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºçš„ç»“æœæ—¶å¯ä»¥ä½¿ç”¨ `similarity_search` å‡½æ•°ã€‚

```python
question = "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹"
sim_docs = vectordb.similarity_search(question, k = 3)
print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(sim_docs)}")
for i, sim_docs in enumerate(sim_docs):
    print(f"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªå†…å®¹ï¼š\n{sim_doc.page_content[:200]}", end = "\n-------------\n")

```

### MMR æ£€ç´¢

å¦‚æœåªè€ƒè™‘æ£€ç´¢å‡ºå†…å®¹çš„ç›¸å…³æ€§ä¼šå¯¼è‡´å†…å®¹è¿‡äºå•ä¸€ï¼Œå¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚
æœ€å¤§è¾¹é™…ç›¸å…³æ€§(MMR, Maximum Marginal Relevance)å¯ä»¥å¸®åŠ©åœ¨ä¿æŒç›¸å…³æ€§çš„åŒæ—¶ï¼Œ
å¢åŠ å†…å®¹çš„ä¸°å¯Œåº¦ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨å·²ç»é€‰æ‹©äº†ä¸€ä¸ªç›¸å…³æ€§æå¾—æ–‡æ¡£ä¹‹åï¼Œ
å†é€‰æ‹©ä¸€ä¸ªä¸å·²é€‰æ–‡æ¡£ç›¸å…³æ€§è¾ƒä½ä½†æ˜¯ä¿¡æ¯ä¸°å¯Œçš„æ–‡æ¡£ã€‚
è¿™æ ·å¯ä»¥åœ¨ä¿æŒç›¸å…³æ€§çš„åŒæ—¶ï¼Œå¢åŠ å†…å®¹çš„å¤šæ ·æ€§ï¼Œé¿å…è¿‡äºå•ä¸€çš„ç»“æœã€‚

```python
mmr_docs = vector_db.max_marginal_relevance_search(question, k = 3)
for i, sim_doc in enumerate(mmr_docs):
    print(f"MMR æ£€ç´¢åˆ°çš„ç¬¬ {i} ä¸ªå†…å®¹ï¼š\n{sim_doc.page_content[:200]}", end = "\n-----------\n")
```

# åŸºäº LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾

åœ¨[è¿™é‡Œ]()ä»‹ç»äº†å¦‚ä½•æ ¹æ®è‡ªå·±çš„æœ¬åœ°çŸ¥è¯†æ–‡æ¡£ï¼Œæ­å»ºä¸€ä¸ªå‘é‡çŸ¥è¯†åº“ã€‚
ä½¿ç”¨æ­å»ºå¥½çš„å‘é‡æ•°æ®åº“ï¼Œå¯¹ query æŸ¥è¯¢é—®é¢˜è¿›è¡Œå¬å›ï¼Œ
å¹¶å°†å¬å›ç»“æœå’Œ query ç»“åˆèµ·æ¥æ„å»º promptï¼Œè¾“å…¥åˆ°å¤§æ¨¡å‹ä¸­è¿›è¡Œé—®ç­”ã€‚

## åŠ è½½æ•°æ®åº“å‘é‡



## åˆ›å»ºä¸€ä¸ª LLM


## æ„å»ºæ£€ç´¢é—®ç­”é“¾



## æ£€ç´¢é—®ç­”é“¾æ•ˆæœæµ‹è¯•


## æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½


# åŸºäº Streamlit éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹

å½“å¯¹çŸ¥è¯†åº“å’Œ LLM å·²ç»æœ‰äº†åŸºæœ¬çš„ç†è§£ï¼Œç°åœ¨æ˜¯å°†å®ƒä»¬å·§å¦™åœ°èåˆå¹¶æ‰“é€ æˆä¸€ä¸ªå¯Œæœ‰è§†è§‰æ•ˆæœçš„ç•Œé¢çš„æ—¶å€™äº†ã€‚
è¿™æ ·çš„ç•Œé¢ä¸ä»…å¯¹æ“ä½œæ›´åŠ ä¾¿æ·ï¼Œè¿˜èƒ½ä¾¿äºä¸ä»–äººåˆ†äº«ã€‚

> Streamlit æ˜¯ä¸€ç§å¿«é€Ÿä¾¿æ·çš„æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨ Python ä¸­é€šè¿‡å‹å¥½çš„ Web ç•Œé¢æ¼”ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
> åœ¨æ„å»ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹åï¼Œå¦‚æœæƒ³æ„å»ºä¸€ä¸ª demo ç»™å…¶ä»–äººçœ‹ï¼Œä¹Ÿè®¸æ˜¯ä¸ºäº†è·å¾—åé¦ˆå¹¶æ¨åŠ¨ç³»ç»Ÿçš„æ”¹è¿›ï¼Œ
> æˆ–è€…åªæ˜¯å› ä¸ºè§‰å¾—è¿™ä¸ªç³»ç»Ÿå¾ˆé…·ï¼Œæ‰€ä»¥æƒ³æ¼”ç¤ºä¸€ä¸‹ï¼šStreamlit å¯ä»¥é€šè¿‡ Python æ¥å£ç¨‹åºå¿«é€Ÿå®ç°è¿™ä¸€ç›®æ ‡ï¼Œ
> è€Œæ— éœ€ç¼–å†™ä»»ä½•å‰ç«¯ã€ç½‘é¡µæˆ– JavaScript ä»£ç ã€‚

## æ„å»ºåº”ç”¨ç¨‹åº

```python
# streamlit_app.py
import streamlit as st
from langchain_openai import ChatOpenAI


def generate_response(input_text, openai_api_key):
     """
     å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç”¨æˆ·å¯†é’¥å¯¹ OpenAI API è¿›è¡Œèº«ä»½éªŒè¯ã€å‘é€æç¤ºå¹¶è·å– AI ç”Ÿæˆçš„å“åº”ã€‚
     è¯¥å‡½æ•°æ¥å—ç”¨æˆ·çš„æç¤ºä½œä¸ºå‚æ•°ï¼Œå¹¶ä½¿ç”¨ st.info æ¥åœ¨è“è‰²æ¡†ä¸­æ˜¾ç¤º AI ç”Ÿæˆçš„å“åº”ã€‚
     """
     llm = ChatOpenAI(temperature = 0.7, openai_api_key = openai_api_key)
     output = llm.invoke(input_text)
     output_parser = StrOutputParser()
     output = output_parser.invoke(output)
     # st.info(output)
     return output


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
     # åˆ›å»ºåº”ç”¨ç¨‹åºçš„æ ‡é¢˜
     st.title("ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘")

     # æ·»åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ï¼Œä¾›ç”¨æˆ·è¾“å…¥å…¶ OpenAI API å¯†é’¥
     openai_api_key = st.sidebar.text_input("OpenAI API Key", type = "password")

     # ä½¿ç”¨ st.form() åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡† st.text_area() ä¾›ç”¨æˆ·è¾“å…¥ã€‚
     # å½“ç”¨æˆ·ç‚¹å‡» Submit æ—¶ï¼Œgenerate_response å°†ä½¿ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°æ¥è°ƒç”¨è¯¥å‡½æ•°
     with st.form("my_form"):
          text = st.text_area(
               "Enter text:", 
               "What are the three key pieces of advice for learning how to code?"
          )
          
          submitted = st.form_submit_button("Submit")

          if not openai_api_key.startswith("sk-"):
               st.warning("Please enter your OpenAI API key!", icon = "")
          if submitted and openai_api_key.startswith("sk-"):
               generate_response(text, openai_api_key)
  
     # é€šè¿‡ä½¿ç”¨ st.session_state æ¥å­˜å‚¨å¯¹è¯å†å²ï¼Œ
     # å¯ä»¥åœ¨ç”¨æˆ·ä¸åº”ç”¨ç¨‹åºäº¤äº’æ—¶ä¿ç•™æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡ï¼Œ
     # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
     if "messages" not in st.session_state:
          st.session_state.messages = []
     
     messages = st.container(height = 300)
     if prompt := st.chat_input("Say something"):
          # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
          st.session_state.messages.appen({
               "role": "user",
               "text": prompt,
          })
          
          # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
          answer = generate_response(prompt, openai_api_key)
          # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
          if answer is not None:
               # å°† LLM çš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
               st.session_state.messages.append({
                    "role": "assistant",
                    "text": answer,
               })
          
          # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
          for message in st.session_state.messages:
               if message["role"] == "user":
                    messages.chat_message("user").write(message["text"])
               else:
                    messages.chat_message("assistant").write(message["text"])
```

```bash
$ streamlit run streamlit_app.py
```

## æ·»åŠ æ£€ç´¢å›ç­”

æ„å»ºæ£€ç´¢é—®ç­”é“¾ä»£ç ï¼š

* `get_vectordb` å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
* `get_chat_qa_chain` å‡½æ•°è¿”å›è°ƒç”¨å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ
* `get_qa_chain` å‡½æ•°è¿”å›è°ƒç”¨ä¸å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ

```python
def get_vectordb():
     """
     å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
     """
     # å®šä¹‰ Embeddings
     embedding = ZhipuAIEmbeddings()
     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
     persist_directory = "data_base/vector_db/chroma"
     # åŠ è½½æ•°æ®åº“
     vectordb = Chroma(
          persist_directory = persist_directory,
          embedding_function = embedding,
     )
     return vectordb

def get_chat_qa_chain(question: str, openai_api_key: str):
     """
     å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model_name = "gpt-3.5-turbo", 
          temperature = 0, 
          openai_api_key = openai_api_key
     )
     memory = ConversationBufferMemory(
          memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
          return_messages = True,  # å°†æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
     )
     retriever = vectordb.as_retriever()
     qa = ConversationBufferMemory.from_llm(
          llm, 
          retriever = retriever,
          memory = memory,
     )
     result = qa({
          "question": question
     })
     
     return result["answer"]


def get_qa_chain(question: str, openai_api_key: str):
     """
     ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model = "gpt-3.5-turbo",
          temperature = 0,
          opanai_api_key = oepnai_api_key,
     )
     template = """"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
     QA_CHAIN_PROMPT = PromptTemplate(
          input_variables = ["context", "question"],
          template = template,
     )
     qa_chain = RetrievalQA.from_chain_type(
          llm,
          retriever = vectordb.as_retriever(),
          return_source_documents = True,
          chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT}
     )
     result = qa_chain({"query": question})

     return result["result"]
```

ç„¶åï¼Œæ·»åŠ ä¸€ä¸ªå•é€‰æŒ‰é’®éƒ¨ä»¶ `st.radio`ï¼Œé€‰æ‹©è¿›è¡Œå›ç­”æ¨¡å¼ï¼š

* `None` ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼
* `qa_chain` ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼
* `chat_qa_chain` å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼

```python
selected_method = st.radio(
     "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
     [
          "None", 
          "qa_chain", 
          "chat_qa_chain"
     ],
     caption = [
          "ä¸ä½¿ç”¨æ£€ç´¢å›ç­”çš„æ™®é€šæ¨¡å¼", 
          "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", 
          "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
     ]
)
```

æœ€åï¼Œè¿›å…¥é¡µé¢ï¼Œé¦–å…ˆå…ˆè¾“å…¥ `OPEN_API_KEY`ï¼ˆé»˜è®¤ï¼‰ï¼Œ
ç„¶åç‚¹å‡»å•é€‰æŒ‰é’®é€‰æ‹©è¿›è¡Œé—®ç­”çš„æ¨¡å¼ï¼Œæœ€ååœ¨è¾“å…¥æ¡†è¾“å…¥ä½ çš„é—®é¢˜ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚

å®Œæ•´ä»£ç ï¼š

```python
# python libraries
import os
import sys
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

from embedding_api.zhipuai_embedding import ZhipuAIEmbeddings
from dotenv import load_dotenv, find_dotenv

# å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
sys.path.append("../knowledge_lib") 
# è¯»å–æœ¬åœ° .env æ–‡ä»¶
_ = load_dotenv(find_dotenv())
# è½½å…¥ ***_API_KEY
os.environ["OPENAI_API_BASE"] = "https://api.chatgptid.net/v1"
zhipuai_api_key = os.environ["ZHIPUAI_API_KEY"]
# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]


def generate_response(input_text, openai_api_key):
     """
     å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç”¨æˆ·å¯†é’¥å¯¹ OpenAI API è¿›è¡Œèº«ä»½éªŒè¯ã€å‘é€æç¤ºå¹¶è·å– AI ç”Ÿæˆçš„å“åº”ã€‚
     è¯¥å‡½æ•°æ¥å—ç”¨æˆ·çš„æç¤ºä½œä¸ºå‚æ•°ï¼Œå¹¶ä½¿ç”¨ st.info æ¥åœ¨è“è‰²æ¡†ä¸­æ˜¾ç¤º AI ç”Ÿæˆçš„å“åº”ã€‚
     """
     llm = ChatOpenAI(temperature = 0.7, openai_api_key = openai_api_key)
     output = llm.invoke(input_text)
     output_parser = StrOutputParser()
     output = output_parser.invoke(output)
     # st.info(output)
     return output


def get_vectordb():
     """
     å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
     """
     # å®šä¹‰ Embeddings
     embedding = ZhipuAIEmbeddings()
     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
     persist_directory = "data_base/vector_db/chroma"
     # åŠ è½½æ•°æ®åº“
     vectordb = Chroma(
          persist_directory = persist_directory,
          embedding_function = embedding,
     )

     return vectordb


def get_chat_qa_chain(question: str, openai_api_key: str):
     """
     å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model_name = "gpt-3.5-turbo", 
          temperature = 0, 
          openai_api_key = openai_api_key
     )
     memory = ConversationBufferMemory(
          memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
          return_messages = True,  # å°†æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
     )
     retriever = vectordb.as_retriever()
     qa = ConversationBufferMemory.from_llm(
          llm, 
          retriever = retriever,
          memory = memory,
     )
     result = qa({
          "question": question
     })
     
     return result["answer"]


def get_qa_chain(question: str, openai_api_key: str):
     """
     ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
     """
     vectordb = get_vectordb()
     llm = ChatOpenAI(
          model = "gpt-3.5-turbo",
          temperature = 0,
          opanai_api_key = openai_api_key,
     )
     template = """"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
     QA_CHAIN_PROMPT = PromptTemplate(
          input_variables = ["context", "question"],
          template = template,
     )
     qa_chain = RetrievalQA.from_chain_type(
          llm,
          retriever = vectordb.as_retriever(),
          return_source_documents = True,
          chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT}
     )
     result = qa_chain({"query": question})

     return result["result"]




# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
     # åˆ›å»ºåº”ç”¨ç¨‹åºçš„æ ‡é¢˜
     st.title("ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘")
     # æ·»åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ï¼Œä¾›ç”¨æˆ·è¾“å…¥å…¶ OpenAI API å¯†é’¥
     openai_api_key = st.sidebar.text_input("OpenAI API Key", type = "password")
     # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
     # selected_method = st.sidebar.selectbox(
     #      "é€‰æ‹©æ¨¡å¼", 
     #      [
     #           "None", 
     #           "qa_chain", 
     #           "chat_qa_chain"
     #      ]
     # )
     selected_method = st.radio(
          "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
          [
               "None", 
               "qa_chain", 
               "chat_qa_chain"
          ],
          caption = [
               "ä¸ä½¿ç”¨æ£€ç´¢å›ç­”çš„æ™®é€šæ¨¡å¼", 
               "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", 
               "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
          ]
     )

     # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
     # é€šè¿‡ä½¿ç”¨ st.session_state æ¥å­˜å‚¨å¯¹è¯å†å²ï¼Œ
     # å¯ä»¥åœ¨ç”¨æˆ·ä¸åº”ç”¨ç¨‹åºäº¤äº’æ—¶ä¿ç•™æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡
     if "messages" not in st.session_state:
          st.session_state.messages = []
     
     # å½“ç”¨æˆ·ç‚¹å‡» Submit æ—¶ï¼Œgenerate_response å°†ä½¿ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°æ¥è°ƒç”¨è¯¥å‡½æ•°
     # with st.form("my_form"):
     #      text = st.text_area("Enter text:", "What are the three key pieces of advice for learning how to code?")
     #      submitted = st.form_submit_button("Submit")
     #      if not openai_api_key.startswith("sk-"):
     #           st.warning("Please enter your OpenAI API key!", icon = "")
     #      if submitted and openai_api_key.startswith("sk-"):
     #           generate_response(text, openai_api_key)
   
     messages = st.container(height = 300)
     if prompt := st.chat_input("Say something"):
          # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
          st.session_state.messages.appen({
               "role": "user",
               "text": prompt,
          })
          # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
          if selected_method == "None":
               answer = generate_response(prompt, openai_api_key)
          if selected_method == "qa_chain":
               answer = get_qa_chain(prompt, openai_api_key)
          elif selected_method == "chat_qa_chain":
               answer = get_chat_qa_chain(prompt, openai_api_key)
          
          # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
          if answer is not None:
               # å°† LLM çš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
               st.session_state.messages.append({
                    "role": "assistant",
                    "text": answer,
               })
          
          # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
          for message in st.session_state.messages:
               if message["role"] == "user":
                    messages.chat_message("user").write(message["text"])
               else:
                    messages.chat_message("assistant").write(message["text"])

if __name__ == "__main__":
    main()
```

## éƒ¨ç½²åº”ç”¨ç¨‹åº

è¦å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Streamlit Cloudï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. ä¸ºåº”ç”¨ç¨‹åºåˆ›å»º GitHub å­˜å‚¨åº“ï¼Œå­˜å‚¨åº“åº”åŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š

```
your-repository/
     |_ streamlit_app.py
     |_ requirements.txt
```

2. è½¬åˆ° [Streamlit Community Cloud](https://share.streamlit.io/)ï¼Œå•å‡»å·¥ä½œåŒºä¸­çš„ `New app` æŒ‰é’®ï¼Œ
   ç„¶åæŒ‡å®šå­˜å‚¨åº“ã€åˆ†æ”¯å’Œä¸»æ–‡ä»¶è·¯å¾„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡é€‰æ‹©è‡ªå®šä¹‰å­åŸŸæ¥è‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„ URLã€‚
3. ç‚¹å‡» `Deploy!` æŒ‰é’®ã€‚
4. åº”ç”¨ç¨‹åºç°åœ¨å°†éƒ¨ç½²åˆ° Streamlit Community Cloudï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®åº”ç”¨ã€‚

ä¼˜åŒ–æ–¹å‘ï¼š

* ç•Œé¢ä¸­æ·»åŠ ä¸Šä¼ æœ¬åœ°æ–‡æ¡£ï¼Œå»ºç«‹å‘é‡æ•°æ®åº“çš„åŠŸèƒ½
* æ·»åŠ å¤šç§LLM ä¸ embeddingæ–¹æ³•é€‰æ‹©çš„æŒ‰é’®
* æ·»åŠ ä¿®æ”¹å‚æ•°çš„æŒ‰é’®
* æ›´å¤š...

# å‚è€ƒ

* [llm-universe](https://github.com/datawhalechina/llm-universe?tab=readme-ov-file)
