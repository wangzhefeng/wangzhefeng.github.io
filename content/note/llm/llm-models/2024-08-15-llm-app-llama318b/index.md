---
title: LLM åº”ç”¨-Llama 3.1 8B
author: wangzf
date: '2024-08-15'
slug: llm-app-llama318b
categories:
  - llm
tags:
  - model
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [ç¯å¢ƒ](#ç¯å¢ƒ)
    - [æœ¬åœ°ç¯å¢ƒ](#æœ¬åœ°ç¯å¢ƒ)
    - [äº‘æœåŠ¡å™¨](#äº‘æœåŠ¡å™¨)
- [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
- [æ„å»º LLM åº”ç”¨](#æ„å»º-llm-åº”ç”¨)
    - [æ¨¡å‹æ„å»º](#æ¨¡å‹æ„å»º)
    - [è°ƒç”¨æ¨¡å‹](#è°ƒç”¨æ¨¡å‹)
- [LoRA å¾®è°ƒ](#lora-å¾®è°ƒ)
    - [æŒ‡ä»¤é›†æ”¶é›†](#æŒ‡ä»¤é›†æ”¶é›†)
        - [æŒ‡ä»¤å¾®è°ƒ](#æŒ‡ä»¤å¾®è°ƒ)
        - [æŒ‡ä»¤é›†](#æŒ‡ä»¤é›†)
        - [åŠ è½½æŒ‡ä»¤é›†](#åŠ è½½æŒ‡ä»¤é›†)
    - [åŠ è½½ tokenizer å’ŒåŠç²¾åº¦æ¨¡å‹](#åŠ è½½-tokenizer-å’ŒåŠç²¾åº¦æ¨¡å‹)
    - [æŒ‡ä»¤é›†æ•°æ®æ ¼å¼åŒ–](#æŒ‡ä»¤é›†æ•°æ®æ ¼å¼åŒ–)
    - [åˆ›å»ºå¾®è°ƒæ¨¡å‹](#åˆ›å»ºå¾®è°ƒæ¨¡å‹)
    - [æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)
    - [åŠ è½½å¾®è°ƒæƒé‡æ¨ç†](#åŠ è½½å¾®è°ƒæƒé‡æ¨ç†)
    - [å®Œæ•´è„šæœ¬ä»£ç ](#å®Œæ•´è„šæœ¬ä»£ç )
- [FastAPI éƒ¨ç½²å’Œè°ƒç”¨](#fastapi-éƒ¨ç½²å’Œè°ƒç”¨)
    - [æ„å»ºæœåŠ¡ API](#æ„å»ºæœåŠ¡-api)
    - [æœåŠ¡ API è°ƒç”¨](#æœåŠ¡-api-è°ƒç”¨)
        - [curl è°ƒç”¨](#curl-è°ƒç”¨)
        - [requests è°ƒç”¨](#requests-è°ƒç”¨)
- [Instruct WebDemo éƒ¨ç½²](#instruct-webdemo-éƒ¨ç½²)
    - [æ„å»ºåº”ç”¨é¡µé¢](#æ„å»ºåº”ç”¨é¡µé¢)
    - [è¿è¡Œåº”ç”¨](#è¿è¡Œåº”ç”¨)
- [èµ„æ–™](#èµ„æ–™)
</p></details><p></p>

# ç¯å¢ƒ

## æœ¬åœ°ç¯å¢ƒ

* Ubuntu 22.04
* Python 3.12
* CUDA 12.1
* PyTorch 2.3.0
* Python Libs

    ```bash
    # å‡çº§ pip
    $ pip install --upgrade pip
    # æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
    $ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

    # å®‰è½¬ä¾èµ–åº“
    $ pip install modelscope==1.11.0
    $ pip install langchain==0.2.3
    $ pip install transformers==4.43.1
    $ pip install accelerate==0.33.0
    $ pip install peft==0.11.1
    $ pip install datasets==2.20.0
    $ pip install fastapi==0.111.1
    $ pip install uvicorn==0.30.3
    $ pip install streamlit==1.36.0
    ```

## äº‘æœåŠ¡å™¨

* [AutoDL å¹³å° LlaMA3.1 é•œåƒ](https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-llama3.1)

# æ¨¡å‹ä¸‹è½½

Llama-3.1-8B-Instruct æ¨¡å‹å¤§å°ä¸º 16 GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 12 åˆ†é’Ÿã€‚

æ–°å»º `model_download.py` è„šæœ¬å¦‚ä¸‹ï¼š

```python
# model_download.py
import os
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

# æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download(
    "LLM-Research/Meta-Llama-3.1-8B-Instruct", 
    cache_dir = "/root/autodl-tmp",  # win10: downloaded_models
    revision = "master",
)
```

ä¸‹è½½çš„æ¨¡å‹ç»“æ„ï¼š

![img](images/downloaded_models.png)

# æ„å»º LLM åº”ç”¨

ä¸ºä¾¿æ·æ„å»º LLM åº”ç”¨ï¼Œéœ€è¦åŸºäºæœ¬åœ°éƒ¨ç½²çš„ LLaMA3_1_LLMï¼Œè‡ªå®šä¹‰ä¸€ä¸ª `LLM` ç±»ï¼Œ
å°† LLaMA3.1 æ¥å…¥åˆ° LangChain æ¡†æ¶ä¸­ã€‚å®Œæˆè‡ªå®šä¹‰ LLM ç±»ä¹‹åï¼Œ
å¯ä»¥ä»¥å®Œå…¨ä¸€è‡´çš„æ–¹å¼è°ƒç”¨ LangChain çš„æ¥å£ï¼Œè€Œæ— éœ€è€ƒè™‘åº•å±‚æ¨¡å‹è°ƒç”¨çš„ä¸ä¸€è‡´ã€‚

## æ¨¡å‹æ„å»º

åŸºäºæœ¬åœ°éƒ¨ç½²çš„ LLaMA3.1 è‡ªå®šä¹‰ LLM ç±»å¹¶ä¸å¤æ‚ï¼Œåªéœ€ä» `langchain.llms.base.LLM` ç±»ç»§æ‰¿ä¸€ä¸ªå­ç±»ï¼Œ
å¹¶é‡å†™æ„é€ å‡½æ•°ä¸ `_call` å‡½æ•°å³å¯ã€‚

æ–°å»º `LLM.py` è„šæœ¬å¦‚ä¸‹ï¼š

```python
from typing import Any, List, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun


class LLaMA3_1_LLM(LLM):
    """
    åŸºäºæœ¬åœ° llama3.1 è‡ªå®šä¹‰ LLM ç±»
    """
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, model_name_or_path: str):
        super().__init__()
        print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
        # tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, 
            use_fast = False
        )
        # model
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path, 
            torch_dtype = torch.bfloat16, 
            device_map = "auto"
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        print("å®Œæˆæœ¬åœ°æ¨¡å‹çš„åŠ è½½")

    def _call(self, 
              prompt : str, 
              stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None,
              **kwargs: Any):
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            {
                "role": "user",
                "content": prompt,
            }
        ]
        input_ids = self.tokenizer.apply_chat_template(
            messages, 
            tokenize = False, 
            add_generation_prompt = True
        )
        model_inputs = self.tokenizer(
            [input_ids],
            return_tensors = "pt",
        ).to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids, 
            max_new_tokens = 512
        )
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(
            generated_ids, 
            skip_special_tokens = True
        )[0]
        
        return response

    @property
    def _llm_type(self) -> str:
        return "LLaMA3_1_LLM" 
```

## è°ƒç”¨æ¨¡å‹

```python
# æµ‹è¯•ä»£ç  main å‡½æ•°
def main():
    from LLM import LLaMA3_1_LLM

    llm = LLaMA3_1_LLM(model_name_or_path = "D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct")
    print(llm("ä½ å¥½"))

if __name__ == "__main__":
    main()
```

# LoRA å¾®è°ƒ

## æŒ‡ä»¤é›†æ”¶é›†

### æŒ‡ä»¤å¾®è°ƒ

LLM å¾®è°ƒä¸€èˆ¬æŒ‡ **æŒ‡ä»¤å¾®è°ƒ** çš„è¿‡ç¨‹ã€‚æ‰€è°“æŒ‡ä»¤å¾®è°ƒï¼Œæ˜¯è¯´ä½¿ç”¨çš„å¾®è°ƒæ•°æ®å½¢å¦‚ï¼š

```json
{
    "instruction": "å›ç­”ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œä»…è¾“å‡ºç­”æ¡ˆã€‚",
    "input": "1+1ç­‰äºå‡ ï¼Ÿ",
    "output": "2"
}
```

å…¶ä¸­ï¼š

* `instruction`: æ˜¯ç”¨æˆ·æŒ‡ä»¤ï¼Œå‘ŠçŸ¥æ¨¡å‹å…¶éœ€è¦å®Œæˆçš„ä»»åŠ¡
* `input`: æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œæ˜¯å®Œæˆç”¨æˆ·æŒ‡ä»¤æ‰€å¿…é¡»çš„è¾“å…¥å†…å®¹
* `output`: æ˜¯æ¨¡å‹åº”è¯¥ç»™å‡ºçš„è¾“å‡º

å¾®è°ƒçš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡æ˜¯è®©æ¨¡å‹å…·æœ‰ç†è§£å¹¶éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨æŒ‡ä»¤é›†æ„å»ºæ—¶ï¼Œ
åº”é’ˆå¯¹æˆ‘ä»¬çš„ç›®æ ‡ä»»åŠ¡ï¼Œé’ˆå¯¹æ€§æ„å»ºä»»åŠ¡æŒ‡ä»¤é›†ã€‚

### æŒ‡ä»¤é›†

å‡†å¤‡å¾®è°ƒæ•°æ®ï¼Œæ”¾åœ¨é¡¹ç›® `dataset` ç›®å½•ä¸‹ã€‚æ•°æ®ç¤ºä¾‹å¦‚ä¸‹ï¼š

```json
[
    {
        "instruction": "å°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€”",
        "input": "",
        "output": "å˜˜â€”â€”éƒ½è¯´è®¸æ„¿è¯´ç ´æ˜¯ä¸çµçš„ã€‚"
    },
    {
        "instruction": "è¿™ä¸ªæ¸©å¤ªåŒ»å•Šï¼Œä¹Ÿæ˜¯å¤æ€ªï¼Œè°ä¸çŸ¥å¤ªåŒ»ä¸å¾—çš‡å‘½ä¸èƒ½ä¸ºçš‡æ—ä»¥å¤–çš„äººè¯·è„‰è¯Šç—…ï¼Œä»–å€’å¥½ï¼Œåå¤©åŠæœˆä¾¿å¾€å’±ä»¬åºœé‡Œè·‘ã€‚",
        "input": "",
        "output": "ä½ ä»¬ä¿©è¯å¤ªå¤šäº†ï¼Œæˆ‘è¯¥å’Œæ¸©å¤ªåŒ»è¦ä¸€å‰‚è¯ï¼Œå¥½å¥½æ²»æ²»ä½ ä»¬ã€‚"
    },
    {
        "instruction": "å¬›å¦¹å¦¹ï¼Œåˆšåˆšæˆ‘å»åºœä¸Šè¯·è„‰ï¼Œå¬ç”„ä¼¯æ¯è¯´ä½ æ¥è¿™é‡Œè¿›é¦™äº†ã€‚",
        "input": "",
        "output": "å‡ºæ¥èµ°èµ°ï¼Œä¹Ÿæ˜¯æ•£å¿ƒã€‚"
    },
    ...
]
```

### åŠ è½½æŒ‡ä»¤é›†

```python
import pandas as pd

# å¾®è°ƒæ•°æ®åœ°å€
tuning_data_path = "D:\projects\llms_proj\llm_proj\dataset\huanhuan.json"

# åŠ è½½å¾®è°ƒæ•°æ®åŠ è½½
tuning_df = pd.read_json(tuning_data_path)
tuning_ds = Dataset.from_pandas(tuning_df)
print(tuning_ds[:3])
```

## åŠ è½½ tokenizer å’ŒåŠç²¾åº¦æ¨¡å‹

æ¨¡å‹ä»¥ **åŠç²¾åº¦** å½¢å¼åŠ è½½ï¼Œå¦‚æœæ˜¾å¡æ¯”è¾ƒæ–°çš„è¯ï¼Œå¯ä»¥ç”¨ `torch.bfloat16` å½¢å¼åŠ è½½ã€‚
å¯¹äºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸€å®šè¦æŒ‡å®š `trust_remote_code = True`ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    use_fast = False,
    trust_remote_code = True
)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,  
    torch_dtype = torch.bfloat16, 
    device_map = "auto",
    # trust_remote_code = True,
)
print(model)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
print(model.dtype)
```

## æŒ‡ä»¤é›†æ•°æ®æ ¼å¼åŒ–

LoRA è®­ç»ƒçš„æ•°æ®æ˜¯éœ€è¦ç»è¿‡æ ¼å¼åŒ–ã€ç¼–ç ä¹‹åå†è¾“å…¥ç»™æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„ã€‚å°±åƒ PyTorch æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ
ä¸€èˆ¬éœ€è¦å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸º `input_ids`ï¼Œå°†è¾“å‡ºæ–‡æœ¬ç¼–ç ä¸º `labels`ï¼Œç¼–ç ä¹‹åçš„ç»“æœéƒ½æ˜¯å¤šç»´çš„å‘é‡ã€‚
ä¸‹é¢å®šä¹‰ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ç”¨äºå¯¹æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œç¼–ç å…¶è¾“å…¥ã€è¾“å‡ºæ–‡æœ¬å¹¶è¿”å›ä¸€ä¸ªç¼–ç åçš„å­—å…¸ã€‚

```python
from pandas as pd
from datasets import Dataset

def process_func(example):
    """
    æ•°æ®æ ¼å¼åŒ–
    """
    # LlaMA åˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ª tokenï¼Œ
    # å› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§
    MAX_LENGTH = 384
    input_ids, attention_mask, labels = [], [], []

    instruction = tokenizer(
        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n", 
        add_special_tokens = False
    )  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens
    response = tokenizer(
        f"{example["output"]}<|eot_id|>", 
        add_special_tokens = False
    )

    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # å› ä¸º eos token å’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„ï¼Œæ‰€ä»¥è¡¥å……ä¸º 1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]

    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH] 
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "label": labels,
    }

# æ•°æ®æ ¼å¼åŒ–å¤„ç†
tokenized_id = tuning_ds.map(process_func, remove_columns = tuning_ds.column_names)
print(tokenized_id)
print(tokenizer.decode(tokenized_id[0]["input_ids"]))
print(tokenizer.decode(filter(lambda x: x != -100, tokenized_id[1]["labels"])))
```

LlaMA 3.1 é‡‡ç”¨çš„ Prompt Tempalte æ ¼å¼å¦‚ä¸‹ï¼š

```
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›<|eot_id|>
<|start_header_id|>user<|end_header_id|>ä½ å¥½å‘€<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>ä½ å¥½ï¼Œæˆ‘æ˜¯ç”„å¬›ï¼Œä½ æœ‰ä»€ä¹ˆäº‹æƒ…è¦é—®æˆ‘å—ï¼Ÿ<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
```

## åˆ›å»ºå¾®è°ƒæ¨¡å‹

`LoarConfig` è¿™ä¸ªç±»ä¸­å¯ä»¥è®¾ç½®å¾ˆå¤šå‚æ•°ï¼Œä½†ä¸»è¦çš„å‚æ•°æ²¡å¤šå°‘ï¼š

* `task_type`: æ¨¡å‹ç±»å‹
* `target_modules`: éœ€è¦è®­ç»ƒçš„æ¨¡å‹å±‚çš„åå­—ï¼Œä¸»è¦å°±æ˜¯ `attention` éƒ¨åˆ†çš„å±‚ï¼Œ
  ä¸åŒçš„æ¨¡å‹å¯¹åº”çš„å±‚çš„åå­—ä¸åŒï¼Œå¯ä»¥ä¼ å…¥æ•°ç»„ï¼Œä¹Ÿå¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯æ­£åˆ™è¡¨è¾¾å¼
* `r`: LoRA çš„ç§©
* `lora_alpha`: LoRA alpha

LoRA çš„ç¼©æ”¾æ˜¯ `lora_alpha/r`ï¼Œåœ¨è¿™ä¸ª `LoraConfig` ä¸­ç¼©æ”¾å°±æ˜¯ 4 å€ã€‚

```python
config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    target_modules = [
        "q_proj", "k_proj", "v_proj", 
        "o_proj", "gate_proj", 
        "up_proj", "down_proj"
    ],
    inference_mode = False,  # è®­ç»ƒæ¨¡å¼
    r = 8,  # LoRA ç§©
    lora_alpha = 32,  # LoRA alpha
    lora_dropout = 0.1,  # dropout æ¯”ä¾‹
)
print(config)

model = get_peft_model(model, config)
print(model.print_trainable_parameters())
```

## æ¨¡å‹å¾®è°ƒ

ä»‹ç»ä¸€ä¸‹ `TrainingArguments` è¿™ä¸ªç±»çš„æºç æ¯ä¸ªå‚æ•°çš„å…·ä½“ä½œç”¨ï¼š

* `output_dir`: æ¨¡å‹çš„è¾“å‡ºè·¯å¾„
* `per_device_train_batch_size`: é¡¾åæ€ä¹‰ `batch_size`
* `gradient_accumulation_steps`: æ¢¯åº¦ç´¯åŠ ï¼Œå¦‚æœæ˜¾å­˜æ¯”è¾ƒå°ï¼Œ
  å¯ä»¥æŠŠ `batch_size` è®¾ç½®å°ä¸€ç‚¹ï¼Œæ¢¯åº¦ç´¯åŠ å¢å¤§ä¸€äº›
* `logging_steps`: å¤šå°‘æ­¥ï¼Œè¾“å‡ºä¸€æ¬¡ `log`
* `num_train_epochs`: é¡¾åæ€ä¹‰ `epoch`
* `gradient_checkpointing`: æ¢¯åº¦æ£€æŸ¥ï¼Œè¿™ä¸ªä¸€æ—¦å¼€å¯ï¼Œ
  æ¨¡å‹å°±å¿…é¡»æ‰§è¡Œ `model.enable_input_require_grads()`

```python
# é…ç½® LoRA è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir = lora_path,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 4,
    logging_steps = 10,
    num_train_epochs = 3,
    save_steps = 100,  # å¿«é€Ÿæ¼”ç¤ºè®¾ç½® 10ï¼Œå»ºè®®è®¾ç½®ä¸º 100
    learning_rate = 1e-4,
    save_on_each_node = True,
    gradient_checkpointing = True,
)

trainer = Trainer(
    model = model,
    args = args,
    train_dataset = tokenized_id,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, padding = True),
)
trainer.train()
```

## åŠ è½½å¾®è°ƒæƒé‡æ¨ç†

è®­ç»ƒå¥½ä¹‹åå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼åŠ è½½ LoRA æƒé‡è¿›è¡Œæ¨ç†ã€‚

```python
from torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ------------------------------
# æ•°æ®ã€æ¨¡å‹ã€å‚æ•°åœ°å€
# ------------------------------
# å¾®è°ƒæ•°æ®åœ°å€
tuning_data_path = "D:\projects\llms_proj\llm_proj\dataset\huanhuan.json"
# æ¨¡å‹åœ°å€
model_path = 'D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct'
# LoRA è¾“å‡ºå¯¹åº” checkpoint åœ°å€
lora_path = 'D:\projects\llms_proj\llm_proj\output\llama3_1_instruct_lora'

# ------------------------------
# åŠ è½½ LoRA æƒé‡æ¨ç†
# ------------------------------
# åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    # use_fast = True,
    trust_remote_code = True
)
# åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    torch_dtype = torch.bfloat16,
    device_map = "auto",  
    trust_remote_code = True
).eval()

# åŠ è½½ loRA æƒé‡
model = PeftModel.from_pretrained(
    model, 
    model_id = os.path.join(lora_path, "checkpoint-100"),
)

# æ„å»º prompt template
prompt = "ä½ å¥½å‘€"
messages = [
    {
        "role": "system",
        "content": "å‡è®¾ä½ æ˜¯çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›ã€‚",
    },
    {
        "role": "user",
        "content": prompt,
    },
]

# æ¨¡å‹æ¨ç† 
input_ids = tokenizer.apply_chat_template(
    messages, 
    tokenize = False
)
model_inputs = tokenizer(
    [input_ids], 
    return_tensors = "pt"
).to('cuda')
generated_ids = model.generate(
    model_inputs.input_ids, 
    max_new_tokens = 512
)
generated_ids = [
    output_ids[len(input_ids):] 
    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
# è¾“å‡º
response = tokenizer.batch_decode(
    generated_ids, 
    skip_special_tokens = True
)[0]
print(response)

```

## å®Œæ•´è„šæœ¬ä»£ç 

```python
# -*- coding: utf-8 -*-

# ***************************************************
# * File        : LLM_LoRA.py
# * Author      : Zhefeng Wang
# * Email       : wangzhefengr@163.com
# * Date        : 2024-08-16
# * Version     : 0.1.081622
# * Description : description
# * Link        : link
# * Requirement : ç›¸å…³æ¨¡å—ç‰ˆæœ¬éœ€æ±‚(ä¾‹å¦‚: numpy >= 2.1.0)
# ***************************************************

# python libraries
import os
import sys
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    DataCollatorForSeq2Seq, 
    TrainingArguments, 
    Trainer, 
    GenerationConfig
)
from peft import PeftModel, LoraConfig, TaskType, get_peft_model

# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]

# ------------------------------
# æ•°æ®ã€æ¨¡å‹ã€å‚æ•°åœ°å€
# ------------------------------
# å¾®è°ƒæ•°æ®åœ°å€
tuning_data_path = "D:\projects\llms_proj\llm_proj\dataset\huanhuan.json"
# æ¨¡å‹åœ°å€
model_path = 'D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct'
# LoRA è¾“å‡ºå¯¹åº” checkpoint åœ°å€
lora_path = 'D:\projects\llms_proj\llm_proj\output\llama3_1_instruct_lora'

# ------------------------------
# åŠ è½½æœ¬åœ° LlaMA-3.1-8B-Instruct æ¨¡å‹
# ------------------------------
# åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    use_fast = False,
    trust_remote_code = True
)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,  
    torch_dtype = torch.bfloat16, 
    device_map = "auto",
    # trust_remote_code = True,
)
print(model)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
print(model.dtype)
# ------------------------------
# LoRA å¾®è°ƒæ•°æ®æ ¼å¼åŒ– 
# ------------------------------
def process_func(example, tokenizer):
    """
    æ•°æ®æ ¼å¼åŒ–
    """
    # LlaMA åˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ª tokenï¼Œ
    # å› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§
    MAX_LENGTH = 384
    input_ids, attention_mask, labels = [], [], []

    instruction = tokenizer(
        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n", 
        add_special_tokens = False
    )  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens
    response = tokenizer(
        f"{example["output"]}<|eot_id|>", 
        add_special_tokens = False
    )

    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # å› ä¸º eos token å’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„ï¼Œæ‰€ä»¥è¡¥å……ä¸º 1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]

    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH] 
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "label": labels,
    }

# åŠ è½½å¾®è°ƒæ•°æ®åŠ è½½
tuning_df = pd.read_json(tuning_data_path)
tuning_ds = Dataset.from_pandas(tuning_df)
print(tuning_ds[:3])

# æ•°æ®æ ¼å¼åŒ–å¤„ç†
tokenized_id = tuning_ds.map(process_func, remove_columns = tuning_ds.column_names)
print(tokenized_id)
print(tokenizer.decode(tokenized_id[0]["input_ids"]))
print(tokenizer.decode(filter(lambda x: x != -100, tokenized_id[1]["labels"])))
# ------------------------------
# LoRA å¾®è°ƒ
# ------------------------------
# å®šä¹‰ LoraConfig
config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    target_modules = [
        "q_proj", "k_proj", "v_proj", 
        "o_proj", "gate_proj", 
        "up_proj", "down_proj"
    ],
    inference_mode = False,  # è®­ç»ƒæ¨¡å¼
    r = 8,  # LoRA ç§©
    lora_alpha = 32,  # LoRA alpha
    lora_dropout = 0.1,  # dropout æ¯”ä¾‹
)
print(config)

# åˆ›å»º Peft æ¨¡å‹
model = get_peft_model(model, config)
print(model.print_trainable_parameters())

# é…ç½® LoRA è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir = lora_path,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 4,
    logging_steps = 10,
    num_train_epochs = 3,
    save_steps = 100,  # å¿«é€Ÿæ¼”ç¤ºè®¾ç½® 10ï¼Œå»ºè®®è®¾ç½®ä¸º 100
    learning_rate = 1e-4,
    save_on_each_node = True,
    gradient_checkpointing = True,
)

## ä½¿ç”¨ Trainer è®­ç»ƒ
trainer = Trainer(
    model = model,
    args = args,
    train_dataset = tokenized_id,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, padding = True),
)
trainer.train()
# ------------------------------
# åŠ è½½ LoRA æƒé‡æ¨ç†
# ------------------------------
# åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    # use_fast = True,
    trust_remote_code = True
)
# åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    torch_dtype = torch.bfloat16,
    device_map = "auto",  
    trust_remote_code = True
).eval()

# åŠ è½½ loRA æƒé‡
model = PeftModel.from_pretrained(
    model, 
    model_id = os.path.join(lora_path, "checkpoint-100"),
)

# æ„å»º prompt template
prompt = "ä½ å¥½å‘€"
messages = [
    {
        "role": "system",
        "content": "å‡è®¾ä½ æ˜¯çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›ã€‚",
    },
    {
        "role": "user",
        "content": prompt,
    },
]

# æ¨¡å‹æ¨ç† 
input_ids = tokenizer.apply_chat_template(
    messages, 
    tokenize = False
)
model_inputs = tokenizer(
    [input_ids], 
    return_tensors = "pt"
).to('cuda')
generated_ids = model.generate(
    model_inputs.input_ids, 
    max_new_tokens = 512
)
generated_ids = [
    output_ids[len(input_ids):] 
    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
# è¾“å‡º
response = tokenizer.batch_decode(
    generated_ids, 
    skip_special_tokens = True
)[0]
print(response)




# æµ‹è¯•ä»£ç  main å‡½æ•°
def main():
    pass

if __name__ == "__main__":
    main()
```

# FastAPI éƒ¨ç½²å’Œè°ƒç”¨

## æ„å»ºæœåŠ¡ API

1. æ–°å»º `api.py` è„šæœ¬å¦‚ä¸‹ï¼š

```python
import json
import datetime

import uvicorn
from fastapi import FastAPI, Request
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨ CUDA
DEVICE_ID = "0"  # CUDA è®¾å¤‡ IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆ CUDA è®¾å¤‡ä¿¡æ¯

# æ¸…ç† GPU å†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():
        with torch.cuda.device(CUDA_DEVICE):
            torch.cuda.empty_cache()  # æ¸…ç©º CUDA ç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›† CUDA å†…å­˜ç¢ç‰‡

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI()

# å¤„ç† POST è¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    # global model, tokenizer
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    model_name_or_path = "D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, 
        use_fast = False
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path, 
        device_map = "auto", 
        torch_dtype = torch.bfloat16
    )

    # è·å– POST è¯·æ±‚çš„ JSON æ•°æ®
    json_post_raw = await request.json()
    # å°† JSON æ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post = json.dumps(json_post_raw)
    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º Python å¯¹è±¡
    json_post_list = json.loads(json_post)
    # è·å–è¯·æ±‚ä¸­çš„æç¤º
    prompt = json_post_list.get("prompt")

    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant.",
        },
        {
            "role": "user",
            "content": prompt
        }
    ]
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    input_ids = tokenizer.apply_chat_template(
        messages, 
        tokenize = False, 
        add_generation_prompt = True
    )
    model_inputs = tokenizer(
        [input_ids],
        return_tensors = "pt",
    ).to(DEVICE)
    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens = 512)
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)[0]

    # æ„å»ºå“åº” JSON
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {
        "response": response,
        "status": 200,
        "time": time,
    }

    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = f"[{time}], prompt:{prompt}, response:{repr(response)}"
    print(log)

    # æ‰§è¡Œ GPU å†…å­˜æ¸…ç†
    torch_gc()

    return answer




# æµ‹è¯•ä»£ç  main å‡½æ•°
def main(): 
    # å¯åŠ¨ FastAPI åº”ç”¨ï¼Œåœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
    # ç”¨ 6006 ç«¯å£å¯ä»¥å°† autodl çš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨ API
    uvicorn.run(app, host = "0.0.0.0", port = 6006, workers = 1)

if __name__ == "__main__":
    main()
```

2. å¯åŠ¨ API æœåŠ¡

```bash
$ python api.py
```

![img](images/fastAPI.png)

## æœåŠ¡ API è°ƒç”¨

### curl è°ƒç”¨

æœåŠ¡é»˜è®¤éƒ¨ç½²åœ¨ `6006` ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨ `curl` è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
$ curl -X POST "http://127.0.0.1:6006" \
    -H 'Content-Type: application/json' \
    -d '{"prompt": "ä½ å¥½"}'
```

```
{"response":"ä½ å¥½ï¼å¾ˆé«˜å…´èƒ½ä¸ºä½ æä¾›å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å›ç­”æˆ–è€…ååŠ©ä½ å®Œæˆå—ï¼Ÿ","status":200,"time":"2024-06-07 12:24:31"}
```

### requests è°ƒç”¨

ä¹Ÿå¯ä»¥ä½¿ç”¨ Python çš„ `requests` åº“è¿›è¡Œè°ƒç”¨ï¼š

```python
import json
import requests

def get_completion(prompt):
    headers = {
        "Content-Type": "application/jons",
    }
    data = {
        "prompt": prompt,
    }
    response = requests.post(
        url = "http://127.0.0.1:6006", 
        headers = headers, 
        data = json.dumps(data)
    )

    return response.json()["response"]


def main():
    print(get_completion(prompt = "ä½ å¥½"))

if __name__ == "__main__":
    main()
```

```
{"response":"ä½ å¥½ï¼å¾ˆé«˜å…´èƒ½ä¸ºä½ æä¾›å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å›ç­”æˆ–è€…ååŠ©ä½ å®Œæˆå—ï¼Ÿ","status":200,"time":"2024-06-07 12:24:31"}
```

# Instruct WebDemo éƒ¨ç½²

## æ„å»ºåº”ç”¨é¡µé¢

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st

# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]


# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## LlaMA3.1 LLM")
    "[å¼€æºå¤§æ¨¡å‹æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"


# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ LLaMA3.1 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œ tokenizer
@st.cache_resource
def get_model():
    # æ¨¡å‹è·¯å¾„
    model_name_or_path = "D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct"
    # ------------------------------
    # åŠ è½½æœ¬åœ° LlaMA-3.1-8B-Instruct æ¨¡å‹
    # ------------------------------
    # åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, 
        # use_fast = False,
        trust_remote_code = True
    )
    tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,  
        torch_dtype = torch.bfloat16, 
        # device_map = "auto",
        # trust_remote_code = True,
    ).cuda()

    return tokenizer, model


# åŠ è½½ LlaMA3.1 çš„æ¨¡å‹å’Œ tokenizer
tokenizer, model = get_model()


# å¦‚æœ session_state ä¸­æ²¡æœ‰ "messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# éå† session_state ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})
    # å°†å¯¹è¯è¾“å…¥æ¨¡å‹ï¼Œè·å¾—è¿”å›
    input_ids = tokenizer.apply_chat_template(st.session_state["messages"],tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ° session_state ä¸­çš„ messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
    print(st.session_state)



# æµ‹è¯•ä»£ç  main å‡½æ•°
def main():
    pass

if __name__ == "__main__":
    main()
```

## è¿è¡Œåº”ç”¨

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä¸€ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨ StreamLit æœåŠ¡ï¼š

```bash
$ streamlit run chatBot.py --server.address 127.0.0.1 --server.port 6006
```

è¿è¡ŒæˆåŠŸåï¼Œåœ¨æœ¬åœ°æµè§ˆå™¨ä¸­æ‰“å¼€ [http://127.0.0.1:6006](http://127.0.0.1:6006)ï¼Œ
å³å¯æŸ¥çœ‹éƒ¨ç½²çš„ WebDemoï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![img](images/chatBot.png)

# èµ„æ–™

* [AutoDL å¹³å° LlaMA3.1 é•œåƒ](https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-llama3.1)
