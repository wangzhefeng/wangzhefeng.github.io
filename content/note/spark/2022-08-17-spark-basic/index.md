---
title: Spark åŸºæœ¬åŸç†
author: ç‹å“²å³°
date: '2022-08-17'
slug: spark-basic
categories:
  - spark
tags:
  - tool
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [Spark ä¼˜åŠ¿ç‰¹ç‚¹](#spark-ä¼˜åŠ¿ç‰¹ç‚¹)
  - [é«˜æ•ˆæ€§](#é«˜æ•ˆæ€§)
  - [æ˜“ç”¨æ€§](#æ˜“ç”¨æ€§)
  - [é€šç”¨æ€§](#é€šç”¨æ€§)
  - [å…¼å®¹æ€§](#å…¼å®¹æ€§)
- [Spark åŸºæœ¬æ¦‚å¿µ](#spark-åŸºæœ¬æ¦‚å¿µ)
- [Spark æ¶æ„è®¾è®¡](#spark-æ¶æ„è®¾è®¡)
- [Spark è¿è¡Œæµç¨‹](#spark-è¿è¡Œæµç¨‹)
- [Spark éƒ¨ç½²æ–¹å¼](#spark-éƒ¨ç½²æ–¹å¼)
- [RDD æ•°æ®ç»“æ„](#rdd-æ•°æ®ç»“æ„)
- [Apache Spark](#apache-spark)
  - [Spark çš„è®¾è®¡å“²å­¦å’Œå†å²](#spark-çš„è®¾è®¡å“²å­¦å’Œå†å²)
  - [Spark å¼€å‘ç¯å¢ƒ](#spark-å¼€å‘ç¯å¢ƒ)
  - [Spark's Interactive Consoles](#sparks-interactive-consoles)
  - [äº‘å¹³å°ã€æ•°æ®](#äº‘å¹³å°æ•°æ®)
- [Spark](#spark)
  - [Spark's Architecture](#sparks-architecture)
  - [Spark's Language API](#sparks-language-api)
  - [Spark's API](#sparks-api)
  - [Spark ä½¿ç”¨](#spark-ä½¿ç”¨)
    - [SparkSession](#sparksession)
    - [DataFrames](#dataframes)
    - [Partitions](#partitions)
    - [Transformation](#transformation)
      - [Lazy Evaluation](#lazy-evaluation)
    - [Action](#action)
    - [Spark UI](#spark-ui)
    - [ä¸€ä¸ª ğŸŒ°](#ä¸€ä¸ª-)
- [Spark å·¥å…·](#spark-å·¥å…·)
  - [Spark åº”ç”¨ç¨‹åº](#spark-åº”ç”¨ç¨‹åº)
  - [Dataset: ç±»å‹å®‰å…¨çš„ç»“æœåŒ– API](#dataset-ç±»å‹å®‰å…¨çš„ç»“æœåŒ–-api)
  - [Spark Structured Streaming](#spark-structured-streaming)
    - [åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›† DataFrame ä»¥åŠ Schema](#åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›†-dataframe-ä»¥åŠ-schema)
    - [å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ](#å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ)
    - [è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®](#è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®)
    - [å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç ](#å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç )
  - [Spark æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ](#spark-æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ)
  - [Spark ä½é˜¶ API](#spark-ä½é˜¶-api)
  - [SparkR](#sparkr)
  - [Spark ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…](#spark-ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…)
</p></details><p></p>

# Spark ä¼˜åŠ¿ç‰¹ç‚¹

ä½œä¸ºå¤§æ•°æ®è®¡ç®—æ¡†æ¶ MapReduce çš„ç»§ä»»è€…ï¼ŒSpark å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ç‰¹æ€§

## é«˜æ•ˆæ€§

ä¸åŒäº MapReduce å°†ä¸­é—´è®¡ç®—ç»“æœæ”¾å…¥ç£ç›˜ä¸­ï¼ŒSpark é‡‡ç”¨å†…å­˜å­˜å‚¨ä¸­é—´è®¡ç®—ç»“æœï¼Œ
å‡å°‘äº†è¿­ä»£è¿ç®—ç£ç›˜ IOï¼Œå¹¶é€šè¿‡å¹¶è¡Œè®¡ç®— DAG å›¾çš„ä¼˜åŒ–ï¼Œå‡å°‘äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„ä¾èµ–ï¼Œ
é™ä½äº†å»¶è¿Ÿç­‰å¾…æ—¶é—´ã€‚å†…å­˜è®¡ç®—ä¸‹ï¼ŒSpark æ¯” MapReduce å— 100 å€

## æ˜“ç”¨æ€§

ä¸åŒäº MapReduce ä»…æ”¯æŒ Map å’Œ Reduce ä¸¤ç§ç¼–ç¨‹ç®—å­ï¼Œ
Spark æä¾›äº†è¶…è¿‡ 80 ä¸­ä¸åŒçš„ Transformation å’Œ Action ç®—å­ï¼Œ
å¦‚ map, reduce, filter, groupByKey, sortByKey, foreach ç­‰ï¼Œ
å¹¶ä¸”é‡‡ç”¨å‡½æ•°å¼ç¼–ç¨‹é£æ ¼ï¼Œå®ç°ç›¸åŒçš„åŠŸèƒ½éœ€è¦çš„ä»£ç é‡æå¤§ç¼©å°

MapReduce å’Œ Spark å¯¹æ¯”ï¼š

| Item              | MapReduce                               | Spark                                         |
|-------------------|-----------------------------------------|-----------------------------------------------|
| æ•°æ®å­˜å‚¨ç»“æ„        | ç£ç›˜ HDFS æ–‡ä»¶ç³»ç»Ÿçš„åˆ†å‰²                   | ä½¿ç”¨å†…å­˜æ„å»ºå¼¹æ€§åˆ†å¸ƒæ•°æ®é›†(RDD)å¯¹æ•°æ®è¿›è¡Œè¿ç®—å’Œ cache |
| ç¼–ç¨‹èŒƒå¼           | Map + Reduce                             | Transformation + Action                       |
| è®¡ç®—ä¸­é—´ç»“æœå¤„ç†æ–¹å¼ | è®¡ç®—ä¸­é—´ç»“æœå†™å…¥ç£ç›˜ï¼ŒIOåŠåºåˆ—åŒ–ã€ååºåˆ—åŒ–ä»£ä»·å¤§ | ä¸­é—´è®¡ç®—ç»“æœåœ¨å†…å­˜ä¸­ç»´æŠ¤ï¼Œå­˜å–é€Ÿåº¦æ¯”ç£ç›˜é«˜å‡ ä¸ªæ•°é‡çº§   |
| Task ç»´æŠ¤æ–¹å¼      | Task ä»¥è¿›ç¨‹çš„æ–¹å¼ç»´æŠ¤                       | Task ä»¥çº¿ç¨‹çš„æ–¹å¼ç»´æŠ¤                            |


## é€šç”¨æ€§

Spark æä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚Spark å¯ä»¥ç”¨äºæ‰¹å¤„ç†ã€äº¤äº’å¼æŸ¥è¯¢(Spark SQL)ã€
å®æ—¶æµå¼è®¡ç®—(Spark Streaming)ã€æœºå™¨å­¦ä¹ (Spark MLlib)å’Œå›¾è®¡ç®—(GraphX)ã€‚
è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨ï¼Œè¿™å¯¹äºä¼ä¸šåº”ç”¨æ¥è¯´ï¼Œ
å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¹³å°æ¥è¿›è¡Œä¸åŒçš„å·¥ç¨‹å®ç°ï¼Œå‡å°‘äº†äººåŠ›å¼€å‘å’Œå¹³å°éƒ¨ç½²æˆæœ¬

## å…¼å®¹æ€§

Spark èƒ½å¤Ÿè·Ÿå¾ˆå¤šå¼€æºå·¥ç¨‹å…¼å®¹ä½¿ç”¨ï¼Œå¦‚ Spark å¯ä»¥ä½¿ç”¨ Hadoop çš„ YARN å’Œ Apache Mesos ä½œä¸ºå®ƒçš„èµ„æºç®¡ç†å’Œè°ƒåº¦å™¨ï¼Œ
å¹¶ä¸” Spark å¯ä»¥è¯»å–å¤šç§æ•°æ®æºï¼Œå¦‚ HDFSã€HBaseã€MySQL ç­‰

# Spark åŸºæœ¬æ¦‚å¿µ

* RDD
* DAG
* Driver Program
* Cluster Manager
* Worker Node
* Executor
* Application
* Job
* Stage
* Task

æ€»ç»“ï¼š

Application ç”±å¤šä¸ª Job ç»„æˆï¼ŒJob ç”±å¤šä¸ª Stage ç»„æˆï¼ŒStage ç”±å¤šä¸ª Task ç»„æˆã€‚Stage æ˜¯ Task è°ƒåº¦çš„åŸºæœ¬å•ä½

```
Application
    - Job 1
        - Stage 1
            - Task 1
            - Task 2
            - ...
            - Task p
        - Stage 2
        - ...
        - Stage n
    - Job 2
    - ...
    - Job m
```

# Spark æ¶æ„è®¾è®¡


# Spark è¿è¡Œæµç¨‹



# Spark éƒ¨ç½²æ–¹å¼



# RDD æ•°æ®ç»“æ„


# Apache Spark

## Spark çš„è®¾è®¡å“²å­¦å’Œå†å²

Apache Spark is **a unified computing engine** and **a set of libraries 
for parallel data processing(big data) on computer cluster**, and Spark 
**support multiple widely used programming language** (Python, Java,
Scala, and R), and Spark **runs anywhere** from a laptop to a cluster of
thousand of servers. This makes it an easy system to start with and
scale-up to big data processing or incredibly large scale.

- **A Unified Computing Engine**
    - [Unified]
        - Spark's key driving goal is to offer a unified platform for
        writing big data applications. Spark is designed to support a
        wide range of data analytics tasks, range from simple data
        loading and SQL queries to machine learning and streaming
        computation, over the same computing engine and with a
        consistent set of APIs.
    - [Computing Engine]
        - Spark handles loading data from storage system and performing
        computation on it, not permanent storage as the end itself, you
        can use Spark with a wide variety of persistent storage
        systems.
        - cloud storage system
            - Azure Stroage
            - Amazon S3
        - distributed file systems
            - Apache Hadoop
        - key-value stroes
            - Apache Cassandra
        - message buses
            - Apache Kafka
- **A set of libraries for parallel data processing on computer cluster**
    - Standard Libraries
        - SQL and sturctured data
        - SparkSQL
        - machine learning
        - MLlib
        - stream processing
        - Spark Streaming
        - Structured Streaming
        - graph analytics
        - GraphX
    - [External Libraries](https://spark-packages.org/) published as third-party packages by open source communities

## Spark å¼€å‘ç¯å¢ƒ


- Language API
    - Python
    - Java
    - Scala
    - R
    - SQL
- Dev Env
    - local
         - [Java(JVM)](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
         - [Scala](https://www.scala-lang.org/download/)
         - [Python interpreter(version 2.7 or later)](https://repo.continuum.io/archive/)
         - [R](https://www.r-project.org/)
         - [Spark](https://spark.apache.org/downloads.html)
    - web-based version in [Databricks Community Edition](https://community.cloud.databricks.com/)

## Spark's Interactive Consoles

- Python

```bash
./bin/pyspark
```

- Scala

```bash
./bin/spark-shell
```

- SQL

```bash
./bin/spark-sql
```

## äº‘å¹³å°ã€æ•°æ®

- [Project's Github](https://github.com/databricks/Spark-The-Definitive-Guide)
- [Databricks](https://community.cloud.databricks.com/)

# Spark


## Spark's Architecture

- **Cluster**
  - **Cluser**:
     - Single machine do not have enough power and resources to perform
        computations on huge amounts of information, or the user probably
        dose not have the time to wait for the computation to finish;
     - A cluster, or group, of computers, pools the resources of many
        machines together, giving us the ability to use all the cumulative
        resources as if they were a single computer.
     - A group of machines alone is not powerful, you need a framework to
        coordinate work across them. Spark dose just that, managing and
        coordinating the execution of task on data across a cluster of
        computers.
  - **Cluster manager**:
     - Spark's standalone cluster manager
     - YARN
     - Mesos
- **Spark Application**
    - **Cluster Manager**
         - A **Driver** process
            - the heart of a Spark Appliction and maintains all relevant
               information during the lifetime of the application;
            - runs ``main()`` functions;
            - sits on a node in the cluster;
            - responsible for:
                - maintaining information about the Spark Application
                - responding to user's program or input
                - analyzing, distributing and scheduling work across the **executors**
         - A Set of **Executor** process
            - responsible for actually carrying out the work that the **driver** assigns them
            - repsonsible for :
                - executing code assigned to it by the driver
                - reporting the state of the computation on that executor back to the dirver node
    - **Spark Application**
        - Spark employs a **cluster manager** that keeps track of the **resources** available;
        - The **dirver** process is responsible for executing the **dirver program's commands** 
            across the **executors** to complete a given task;
        - The **executors** will be running Spark code;

## Spark's Language API


- Scala
    - Spark's "default" language.
- Java
- Python
    - ``pyspark``
- SQL
    - Spark support a subset of the ANSI SQL 2003 standard.
- R
    - Spark core
        - ``SparkR``
    - R community-driven package
        - ``sparklyr``

## Spark's API

Spark has two fundamental sets of APIS:

- Low-level "unstructured" APIs
    - RDD
    - Streaming
- Higher-level structured APIs
    - Dataset
    - DataFrame
        - ``org.apache.spark.sql.functions``
        - Partitions
        - DataFrame(Dataset) Methods
        - DataFrameStatFunctions
        - DataFrameNaFunctions
        - Column Methods
        - alias
        - contains
    - Spark SQL
    - Structured Streaming

## Spark ä½¿ç”¨

- å¯åŠ¨ Spark's local mode
    - äº¤äº’æ¨¡å¼
        - ``./bin/spark-shell``
        - ``./bin/pyspark``
    - æäº¤é¢„ç¼–è¯‘çš„ Spark Application
        - ``./bin/spark-submit``
- åˆ›å»º ``SparkSession``
    - äº¤äº’æ¨¡å¼, å·²åˆ›å»º
        - ``spark``
    - ç‹¬ç«‹çš„ APP
        - Scala:

        ```scala
        import org.apache.spark.SparkSession

        val spark = SparkSession
            .builder()
            .master()
            .appName()
            .config()
            .getOrCreate()
        ```

        - Python:

        ```python
        from pyspark import SparkSession

        spark = SparkSession \
            .builder() \
            .master() \
            .appName() \
            .config() \
            .getOrCreate()
        ```
### SparkSession

- SparkSession ç®€ä»‹
   - **Spark Application** controled by a **Driver** process called the **SparkSession**ï¼›
   - **SparkSession** instance is the way Spark executes user-defined manipulations across the cluster, 
      and there is a one-to-one correspondence between a **SparkSession** and a **Spark Application**;
- SparkSession ç¤ºä¾‹
   - Scala äº¤äº’æ¨¡å¼ï¼š

    ```bash
    # in shell
    $ spark-shell
    ```
    
    ```scala
    // in Scala
    scala> val myRange = spark.range(1000).toDF("number")
    ```

   - Scala APP æ¨¡å¼ï¼š

    ```scala
    // in Scala
    import org.apache.spark.SparkSession
    val spark = SparkSession 
        .builder()
        .master()
        .appName()
        .config()
        .getOrCreate()
    ```

   - Python äº¤äº’æ¨¡å¼ï¼š

    ```bash
    # in shell
    $ pyspark
    ```

    ```python
    # in Pyton
    >>> myRange = spark.range(1000).toDF("number")
    ```

   - Python APP æ¨¡å¼ï¼š

    ```python
    # in Python
    from pyspark import SparkSession
    spark = SparkSession \
        .builder() \
        .master() \
        .appName() \
        .config() \
        .getOrCreate()
    ```

### DataFrames

- A DataFrame is the most common Structured API;
- A DataFrame represents a table of data with rows and columns;
- The list of DataFrame defines the columns, the types within those columns is called the schema;
- Spark DataFrame can span thousands of computers:
- the data is too large to fit on one machine
- the data would simply take too long to perform that computation on one machine

### Partitions


### Transformation

#### Lazy Evaluation


### Action

è½¬æ¢æ“ä½œèƒ½å¤Ÿå»ºç«‹é€»è¾‘è½¬æ¢è®¡åˆ’, ä¸ºäº†è§¦å‘è®¡ç®—, éœ€è¦è¿è¡Œä¸€ä¸ªåŠ¨ä½œæ“ä½œ(action)ã€‚ä¸€ä¸ªåŠ¨ä½œæŒ‡ç¤º Spark åœ¨ä¸€ç³»åˆ—è½¬æ¢æ“ä½œåè®¡ç®—ä¸€ä¸ªç»“æœã€‚

### Spark UI

- **Spark job** represents **a set of transformations** triggered by **an individual action**, and can monitor the Spark job from the Spark UI;
- User can monitor the progress of a Spark job through the **Spark web UI**:
- Spark UI is available on port ``4040`` of the **dirver node**;
- Local Mode: ``http://localhost:4040``
- Spark UI displays information on the state of:
   - Spark jobs
   - Spark environment
   - cluster state
   - tunning
   - debugging

### ä¸€ä¸ª ğŸŒ°

(1)æŸ¥çœ‹æ•°æ®é›†

```bash
$ head /data/flight-data/csv/2015-summary.csv
```

(2)è¯»å–æ•°æ®é›†

```scala
// in Scala
val flightData2015 = spark
    .read
    .option("inferSchema", "true")
    .option("header", "true")
    .csv("/data/flight-data/csv/2015-summary.csv")
```

```python
# in Python
flightData2015 = spark \
    .read \
    .option("inferSchema", "true") \
    .option("header", "true") \
    .csv("/data/flight-data/csv/2015-summary.csv")
```

(3)åœ¨æ•°æ®ä¸Šæ‰§è¡Œè½¬æ¢æ“ä½œå¹¶æŸ¥çœ‹ Spark æ‰§è¡Œè®¡åˆ’

```scala
// in Scala
// è½¬æ¢æ“ä½œ .sort()
flightData2015.sort("count").explain()
flightData2015.sort("count")
```

(4)åœ¨æ•°æ®ä¸ŠæŒ‡å®šåŠ¨ä½œæ“ä½œæ‰§è¡ŒæŠ€æœ¯

```scala
// in Scala
// é…ç½® Spark shuffle
spark.conf.set("spark.sql.shuffle.partitions", "5")
// åŠ¨ä½œæ“ä½œ .take(n)
flightData2015.sort("count").take(2)
```

(5)DataFrame å’Œ SQL

```scala
// in Scala
flightData2015.createOrReplaceTempView("flight_data_2015")
```

```scala      
// in Scala
val sqlWay = spark.sql("""
    SELECT DEST_COUNTRY_NAME, count(1)
    FROM flight_data_2015
    GROUP BY DEST_COUNTRY_NAME
    """)

val dataFrameWay = flightData2015
    .groupBy("DEST_COUNTRY_NAME")
    .count()

sqlWay.explain()
dataFrameWay.explain()
```

```python
# in Python
sqlWay = spark.sql("""
    SELECT DEST_COUNTRY_NAME, count(1)
    FROM flight_data_2015
    GROUP BY DEST_COUNTRY_NAME
    """)

dataFrameWay = flightData2015 \
    .groupBy("DEST_COUNTRY_NAME") \
    .count()

sqlWay.explain()
dataFrameWay.explain()
```

```scala
// in Scala
spark.sql("""
    SELECT max(count) 
    FROM flight_data_2015
    """)
    .take(1)

import org.apache.spark.sql.functions.max
flightData2015
    .select(max("count"))
    .take(1)
```

```python
# in Python
spark.sql("""
    SELECT max(count)
    FROM flight_data_2015
    """) \
    .take(1)

from pyspark.sql.functions import max
flightData2015.select(max("count")).take(1)
```

# Spark å·¥å…·

## Spark åº”ç”¨ç¨‹åº

Spark å¯ä»¥é€šè¿‡å†…ç½®çš„å‘½ä»¤è¡Œå·¥å…· ``spark-submit`` è½»æ¾åœ°å°†æµ‹è¯•çº§åˆ«çš„äº¤äº’ç¨‹åºè½¬åŒ–ä¸ºç”Ÿäº§çº§åˆ«çš„åº”ç”¨ç¨‹åº.

é€šè¿‡ä¿®æ”¹ ``spark-submit`` çš„ ``master`` å‚æ•°, å¯ä»¥å°†å°†åº”ç”¨ç¨‹åºä»£ç å‘é€åˆ°ä¸€ä¸ªé›†ç¾¤å¹¶åœ¨é‚£é‡Œæ‰§è¡Œ, 
åº”ç”¨ç¨‹åºå°†ä¸€ç›´è¿è¡Œ, ç›´åˆ°æ­£ç¡®é€€å‡ºæˆ–é‡åˆ°é”™è¯¯ã€‚åº”ç”¨ç¨‹åºéœ€è¦åœ¨é›†ç¾¤ç®¡ç†å™¨çš„æ”¯æŒä¸‹è¿›è¡Œ, å¸¸è§çš„é›†ç¾¤ç®¡ç†å™¨æœ‰ 
Standalone, Mesos å’Œ YARN ç­‰.

- ç¤ºä¾‹ 1

```bash
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \      # è¿è¡Œçš„ç±» 
--master local \                                 # åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œç¨‹åº
./examples/jars/spark-examples_2.11-2.2.0.jar 10 # è¿è¡Œçš„ JAR åŒ…
```

- ç¤ºä¾‹ 2

```bash
./bin/spark-submit \
-- master local \
./examples/src/main/python/pi.py 10
```

## Dataset: ç±»å‹å®‰å…¨çš„ç»“æœåŒ– API

- ç¤ºä¾‹

```scala
case class Flight(DEST_COUNTRY_NAME: String, 
                 ORIGIN_COUNTRY_NAME: String,
                 count: BigInt)
val flightDF = spark
  .read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")

val flights = flightDF.as[Flight]

flights
  .fliter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

flights
  .take(5)
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))
```

## Spark Structured Streaming

Spark Structured Streaming(Spark ç»“æ„åŒ–æµå¤„ç†) æ˜¯ç”¨äºæ•°æ®æµå¤„ç†çš„é«˜é˜¶ API, 
åœ¨ Spark 2.2 ç‰ˆæœ¬ä¹‹åå¯ç”¨ã€‚å¯ä»¥åƒä½¿ç”¨ Spark ç»“æ„åŒ– API åœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹ä¸€æ ·, 
æ‰§è¡Œç»“æ„åŒ–æµå¤„ç†, å¹¶ä»¥æµå¼æ–¹å¼è¿è¡Œå®ƒä»¬, ä½¿ç”¨ç»“æ„åŒ–æµå¤„ç†å¯ä»¥å‡å°‘å»¶è¿Ÿå¹¶å…è®¸å¢é‡å¤„ç†.
æœ€é‡è¦çš„æ˜¯, å®ƒå¯ä»¥å¿«é€Ÿåœ°ä»æµå¼ç³»ç»Ÿä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯, è€Œä¸”å‡ ä¹ä¸éœ€è¦æ›´æ”¹ä»£ç ã€‚
å¯ä»¥æŒ‰ç…§ä¼ ç»Ÿæ‰¹å¤„ç†ä½œä¸šçš„æ¨¡å¼è¿›è¡Œè®¾è®¡, ç„¶åå°†å…¶è½¬æ¢ä¸ºæµå¼ä½œä¸š, å³å¢é‡å¤„ç†æ•°æ®, 
è¿™æ ·å°±ä½¿å¾—æµå¤„ç†å˜å¾—å¼‚å¸¸ç®€å•.

- æ•°æ®é›†ï¼šhttps://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data

### åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›† DataFrame ä»¥åŠ Schema

```scala
// in Scala
val staticDataFrame = spark
    .read
    .format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
cal staticSchema = staticDataFrame.schema
```

```python
# in Python
staticDataFrame = spark \
    .read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema
```

### å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ

```scala
// in Scala
import org.apache.spark.sql.functions.{window, column, desc, col}
staticDataFrame
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    )
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    )
    .sum("total_cost")
    .show(5)
```

```python
# in Python
from pyspark.sql.functions import window, column, desc, col
staticDataFrame \
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    ) \
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    ) \
    .sum("total_cost") \
    .show(5)
```


### è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®

```scala
// in Scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

```python
# in Python
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

### å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç 

(1)è¯»å–æµå¼æ•°æ®ï¼š

```scala
// in Scala
val streamingDataFrame = spark
    .readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)       // æŒ‡å®šä¸€æ¬¡åº”è¯¥è¯»å…¥çš„æ–‡ä»¶æ•°é‡, åœ¨å®é™…åœºæ™¯ä¸­è¢«çœç•¥
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

```python

# in Python
streamingDataFrame = spark \
    .readStream \
    .schema(staticSchema) \
    .option("maxFilesPerTrigger", 1) \
    .format("csv") \
    .option("header", "true") \
    .load("/data/retail-data/by-day/*.csv")
```

(2)æŸ¥çœ‹ DataFrame æ˜¯å¦ä»£è¡¨æµæ•°æ®ï¼š

```scala
// in Scala
streamingDataFrame.isStreaming // è¿”å› true
```

```python
# in Python
streamingDataFrame.isStreaming # è¿”å› true
```

(3)å¯¹æµå¼æ•°æ®æ‰§è¡Œåˆ†ç»„èšåˆæ“ä½œ(è½¬æ¢æ“ä½œ)

```scala
# in Scala
val purchaseByCustomerPerHour = streamingDataFrame
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    )
    .groupBy(
    $"CustomerId", 
    window($"InvoiceDate", "1 day")
    )
    .sum("total_cost")
```

```python
# in Python
purchaseByCustomerPerHour = streamingDataFrame \
    .selectExpr(
    "CustomerId", 
    "(UnitPrice * Quantity) as total_cost", 
    "InvoiceDate"
    ) \
    .groupBy(
    col("CustomerId"), 
    window(col("InvoiceDate"), "1 day")
    ) \
    .sum("total_cost") \
    .show(5)
```

(4)è°ƒç”¨å¯¹æµæ•°æ®çš„åŠ¨ä½œæ“ä½œ, å°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜ä¸­çš„ä¸€ä¸ªè¡¨ä¸­, åœ¨æ¯æ¬¡è¢«è§¦å‘åæ›´æ–°è¿™ä¸ªå†…å­˜ç¼“å­˜

```scala
// in Scala
purchaseByCustomerPerHour.writeStream
    .format("memory")               // memory ä»£è¡¨å°†è¡¨å­˜å…¥å†…å­˜
    .queryName("customer_purchases") // å­˜å…¥å†…å­˜çš„è¡¨çš„åç§°
    .outputMode("complete")         // complete è¡¨ç¤ºä¿å­˜è¡¨ä¸­æ‰€æœ‰è®°å½•
    .start()
```

```python
# in Python
purchaseByCustomerPerHour.writeStream \
    .format("memory") \
    .queryName("customer_purchases") \
    .outputMode("complete") \
    .start()
```

(5)è¿è¡ŒæŸ¥è¯¢è°ƒè¯•ç»“æœ

```scala
// in Scala
spark.sql("""
    SELECT * 
    FROM customer_purchases
    ORDER BY `sum(total_cost)` DESC
    """)
    .show(5)
```

```python
# in Python
spark.sql("""
    SELECT * 
    FROM customer_purchases
    ORDER BY `sum(total_cost)` DESC
    """) \
    .show(5)
```

(6)å°†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°

```scala
// in Scala
purchaseByCustomerPerHour.writeStream
    .format("console")
    .queryName("customer_purchases_2")
    .outputMode("complete")
    .start()
```

```python
# in Python
purchaseByCustomerPerHour.writeStream \
    .format("console") \
    .queryName("customer_purchases_2") \
    .outputMode("complete") \
    .start()
```

## Spark æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ



## Spark ä½é˜¶ API


Spark ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æ˜¯å»ºç«‹åœ¨ RDD ä¹‹ä¸Šçš„. Spark çš„é«˜é˜¶ API åŠæ‰€æ”¯æŒçš„é«˜çº§æ“ä½œéƒ½ä¼šè¢«ç¼–è¯‘åˆ°è¾ƒä½çº§çš„ RDD ä¸Šæ‰§è¡Œ, 
ä»¥æ–¹ä¾¿å’Œå®ç°å…¶è¾ƒé«˜æ•ˆçš„åˆ†å¸ƒå¼æ‰§è¡Œ. ä½¿ç”¨ RDD å¯ä»¥å¹¶è¡ŒåŒ–å·²ç»å­˜å‚¨åœ¨é©±åŠ¨å™¨æœºå™¨å†…å­˜ä¸­çš„åŸå§‹æ•°æ®.

å¤§å¤šæ•°æƒ…å†µä¸‹ç”¨æˆ·åªéœ€è¦ä½¿ç”¨ Spark çš„é«˜é˜¶ API æˆ–é«˜çº§æ“ä½œå°±å¯ä»¥å®ç°æ‰€éœ€çš„ä¸šåŠ¡é€»è¾‘, æœ‰æ—¶å€™å¯èƒ½éœ€è¦ä½¿ç”¨ RDD, 
ç‰¹åˆ«æ˜¯åœ¨è¯»å–æˆ–æ“ä½œåŸå§‹æ•°æ®(æœªå¤„ç†æˆ–éç»“æ„åŒ–çš„æ•°æ®)æ—¶.

- ç¤ºä¾‹ 1

```scala
// in Scala
spark.sparkContext.parallelize(Seq(1, 2, 3)).toDF() // å°† RDD è½¬åŒ–ä¸º DataFrame
```

- ç¤ºä¾‹ 2

```python
# in Python
from pyspark.sql import Row

spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()
```

## SparkR

SparkR æ˜¯ä¸€ä¸ªåœ¨ Spark ä¸Šè¿è¡Œçš„ R è¯­è¨€å·¥å…·, å®ƒå…·æœ‰ä¸ Spark å…¶ä»–æ”¯æŒè¯­è¨€ç›¸åŒçš„è®¾è®¡å‡†åˆ™. 
SparkR ä¸ Spark çš„ Python API éå¸¸ç›¸ä¼¼, åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹, SparkR æ”¯æŒ Python æ”¯æŒçš„æ‰€æœ‰åŠŸèƒ½.

- ç¤ºä¾‹ 1

```r
# in R
library(SparkR)
sparkDf <- read.df("/data/flight-data/csv/2015-summary.csv", source = "csv", header = "true", inferSchema = "true")
take(sparkDF, 5)
collect(orderBy(sparkDF, "count"), 20)
```

- ç¤ºä¾‹ 2

```r
# in R
library(magrittr)

sparkDF %>% 
    orderBy(desc(sparkDF$count)) %>%
    groupBy("ORIGIN_COUNTRY_NAME") %>%
    count() %>%
    limit(10) %>%
    collect()
```

## Spark ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…

å¯ä»¥åœ¨ [Spark Packages ç´¢å¼•](https://spark-packages.org) æ‰¾åˆ°æ‰€æœ‰çš„å¼€æºç¤¾åŒºç»´æŠ¤çš„å·¥å…·åŒ…, 
ç”¨æˆ·ä¹Ÿå¯ä»¥å°†è‡ªå·±å¼€å‘çš„å·¥å…·åŒ…å‘å¸ƒåˆ°æ­¤ä»£ç åº“ä¸­, ä¹Ÿå¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°å„ç§å…¶ä»–é¡¹ç›®å’Œå·¥å…·åŒ….