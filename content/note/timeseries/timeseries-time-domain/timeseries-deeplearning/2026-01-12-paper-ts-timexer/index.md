---
title: TimeXer：融合外生变量的时间序列预测 Transformer 模型
author: wangzf
date: '2026-01-12'
slug: paper-ts-timexer
categories:
  - timeseries
  - 论文阅读
tags:
  - paper
  - model
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>目录</summary><p>

- [一、研究背景与问题](#一研究背景与问题)
- [二、模型核心设计](#二模型核心设计)
    - [1. 变量嵌入策略](#1-变量嵌入策略)
    - [2. 注意力机制](#2-注意力机制)
    - [3. 预测与损失](#3-预测与损失)
- [三、实验设计与结果](#三实验设计与结果)
    - [1. 实验设置](#1-实验设置)
    - [2. 核心实验结果](#2-核心实验结果)
    - [3. 消融实验（验证核心模块有效性）](#3-消融实验验证核心模块有效性)
    - [4. 泛化性与鲁棒性验证](#4-泛化性与鲁棒性验证)
- [四、模型分析与讨论](#四模型分析与讨论)
- [五、结论与贡献](#五结论与贡献)
- [参控资料](#参控资料)
</p></details><p></p>


# 一、研究背景与问题

1. **时间序列预测的现实需求**：时间序列预测在气象、电力、交通等领域应用广泛，但现实场景中数据常存在部分观测特性，仅依赖目标变量（内生变量）难以保证预测准确性。
2. **外生变量的重要性**：外生变量（如经济指标、人口变化、社会事件等）能为内生变量预测提供关键外部信息。例如，电价预测需结合市场供需等外生变量，仅靠历史电价数据无法精准预测。
3. **现有方法的局限性**
    - 传统单变量/多变量预测范式：要么平等对待所有变量，增加计算复杂度；要么忽略外生变量信息，导致预测偏差。
    - Transformer 类模型缺陷：
        - PatchTST 等基于 patch 的模型擅长捕捉时间依赖但弱于多变量关联；
        - iTransformer 等基于变量的模型擅长变量间关联但难以捕捉时间内部变化。
    - 外生变量处理难题：现实中存在缺失值、时间错位、频率不匹配、长度差异等不规则问题，现有模型难以适配。


# 二、模型核心设计

TimeXer 在不修改标准 Transformer 结构的前提下，通过精巧的嵌入层和注意力机制，实现内生与外生变量的协同建模，核心包括三部分：

## 1. 变量嵌入策略

> 解决粒度不匹配问题

| 变量类型 | 嵌入方式 | 目的 |
|----------|----------|------|
| 内生变量 | 1. **Patch 级嵌入**：将内生序列分割为非重叠 patch，每个 patch 通过线性投影转为 D 维时间 token，捕捉细粒度时间依赖；<br>2. **全局 token 嵌入**：学习 1 个全局 token，作为宏观表征，衔接外生变量与内生 patch | 同时捕捉时间内部依赖和变量间关联 |
| 外生变量 | **变量级嵌入**：每个外生序列直接通过线性投影转为 1 个 D 维变量 token，不进行 patch 分割 | 适配不规则外生变量（如缺失、错位），降低计算复杂度 |

## 2. 注意力机制

> 双层次依赖捕捉

- **内生自注意力**：对内生的 patch token 与全局 token 拼接后施加自注意力，同时实现：
  1. Patch-to-Patch：捕捉 patch 间时间依赖；
  2. Patch-to-Global：全局 token 聚合所有 patch 信息；
  3. Global-to-Patch：每个 patch 从全局 token 获取全局关联。
- **外生-内生交叉注意力**：以生全局 token 为查询（Query），外生变量 token 为键（Key）和值（Value），将外生变量的因果信息传递到内生变量，避免内生变量对外生变量的无效交互。

## 3. 预测与损失

- 预测：对 Transformer 编码器输出的内生 patch token 和全局 token 拼接后，通过线性投影生成未来 S 步预测值。
- 损失：采用 L2 损失（平方损失）衡量预测值与真实值的差异，公式为：
  `$$\text{Loss}=\sum _{i=1}^{S}\left\| x_{i}-\hat{x}_{i}\right\| _{2}^{2}$$`
  其中 `$\hat{x}$` 为预测值。


# 三、实验设计与结果

## 1. 实验设置

- **数据集**：12 个真实世界数据集，涵盖两类任务：
  1. 短期预测：5 个电力价格数据集（EPF），内生变量为电价，外生变量为电网负荷、风电预测等；
  2. 长期预测：7 个多变量数据集（ECL、Weather、ETT 系列、Traffic），验证模型在多变量场景的通用性。
- **基线模型**：9 个主流模型，包括 Transformer 类（iTransformer、PatchTST、Crossformer 等）、CNN 类（TimesNet、SCINet）、线性类（RLinear、DLinear）及外生变量专用模型 TiDE。
- **参数配置**：短期预测输入长度 168、预测长度 24、patch 长度 24；长期预测输入长度 96、预测长度 96/192/336/720、patch 长度 16；优化器为 Adam，初始学习率 1e-4，训练 10 轮并早停。

## 2. 核心实验结果

- **短期电价预测**：TimeXer 在所有 5 个 EPF 数据集上均实现 SOTA，例如在 PJM 数据集上 MSE 仅 0.093（低于 TiDE 的 0.101、PatchTST 的 0.106），证明其有效融合外生变量的能力。
- **长期多变量预测**：在 7 个数据集的 4 种预测长度下，TimeXer 平均性能最优。例如在 Weather 数据集上，平均 MSE 0.241（低于 CrossGNN 的 0.247、MSGNet 的 0.249），且支持将其他变量作为外生变量，泛化性强。

## 3. 消融实验（验证核心模块有效性）

| 设计方案 | 核心修改 | 性能变化（以 EPF 数据集平均 MSE 为例） | 结论 |
|----------|----------|-----------------------------------|------|
| 原始模型（Ours） | Patch+Global（内生）+Variate（外生） | 0.307 | 基准性能 |
| 替换外生嵌入 | 外生变量用 Patch 嵌入替代 Variate 嵌入 | 0.316 | 外生变量的变量级嵌入更有效，Patch 嵌入增加噪声 |
| 移除全局 token | 仅保留内生 Patch 嵌入 | 0.316 | 全局 token 是衔接外生-内生变量的关键 |
| 替换交叉注意力 | 用“加法”或“拼接+自注意力”替代交叉注意力 | 0.329/0.312 | 交叉注意力能更精准传递外生信息，避免无效交互 |

## 4. 泛化性与鲁棒性验证

- **长度不匹配场景**：当内生/外生序列回溯长度（T/Tex）从 96 增至 720 时，TimeXer 性能持续提升，且支持 T≠Tex（如内生 96、外生 720），适配传感器新部署等现实场景。
- **缺失值场景**：对外生变量填充 0 或随机值后，TimeXer 性能仅轻微下降（MSE 从 0.307 升至 0.333），而内生变量缺失时性能大幅下降（MSE 升至 1.125），证明其对低质量外生数据的鲁棒性。
- **大规模数据场景**：在包含 3850 个气象站的温度预测任务中（内生为温度，外生为 3×3 网格气象指标），TimeXer 在频率不匹配（内生 hourly、外生 3 小时）场景下仍优于所有基线，验证 scalability。


# 四、模型分析与讨论

1. **注意力可解释性**：可视化注意力权重发现，TimeXer 会对与内生变量趋势相似的外生变量分配更高注意力（如 CO2 浓度预测中，对空气密度注意力高于最大风速），符合物理因果逻辑。
2. **效率优势**：在 ECL 数据集（320 个外生变量）上，TimeXer 内存占用（1.13GB）低于 iTransformer（1.23GB），训练时间（24ms/iter）接近 PatchTST（15ms/iter），因避免外生变量间的无效交互，复杂度为 O(C)（C 为外生变量数），低于 iTransformer 的 O(C²)。
3. **局限性**：在 Traffic 数据集上，TimeXer 的 MAE 接近 iTransformer，但 MSE 差距较大，原因是 patch 级表征更关注整体趋势，对尖峰（spike）数值预测精度不足，需通过调整 patch 长度或增加全局 token 数量优化。


# 五、结论与贡献

1. **核心贡献**
    - 提出 TimeXer 模型，首次在标准 Transformer 架构中实现内生与外生变量的协同建模，无需修改 Transformer 组件；
    - 设计 patch 级+变量级双粒度嵌入和层次化注意力，解决外生变量不规则性和变量间无效交互问题；
    - 在 12 个数据集上验证 SOTA 性能，同时具备通用性、鲁棒性和可扩展性。
2. **应用价值**：适配现实中缺失值、时间错位、频率不匹配等复杂场景，可应用于电力、气象、交通等领域的精准预测。
3. **代码开源**：https://github.com/thuml/TimeXer。

# 参控资料

1. [TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2402.19072)
2. [TimeXer GitHub](https://github.com/thuml/TimeXer)
