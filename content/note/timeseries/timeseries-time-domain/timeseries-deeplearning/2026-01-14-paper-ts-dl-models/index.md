---
title: Deep Time Series Models 总结
author: wangzf
date: '2026-01-14'
slug: paper-ts-dl-models
categories:
  - timeseries
  - 论文阅读
tags:
  - paper
  - model
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>目录</summary><p>

- [一、研究背景与意义](#一研究背景与意义)
- [二、基础概念](#二基础概念)
    - [（一）时间序列定义与核心要素](#一时间序列定义与核心要素)
    - [（二）时间序列分析任务](#二时间序列分析任务)
- [三、深度时间序列模型的基础模块](#三深度时间序列模型的基础模块)
    - [（一）平稳化（Stationarization）](#一平稳化stationarization)
    - [（二）分解（Decomposition）](#二分解decomposition)
    - [（三）傅里叶分析（Fourier Analysis）](#三傅里叶分析fourier-analysis)
- [四、深度时间序列模型架构](#四深度时间序列模型架构)
    - [（一）多层感知机（MLP）](#一多层感知机mlp)
    - [（二）循环神经网络（RNN）](#二循环神经网络rnn)
    - [（三）卷积神经网络（CNN）](#三卷积神经网络cnn)
    - [（四）图神经网络（GNN）](#四图神经网络gnn)
    - [（五）Transformer](#五transformer)
- [五、时间序列库（TSLib）](#五时间序列库tslib)
    - [（一）设计目标与优势](#一设计目标与优势)
    - [（二）关键组成](#二关键组成)
    - [（三）实验结果与分析](#三实验结果与分析)
- [六、未来研究方向](#六未来研究方向)
    - [（一）时间序列预训练](#一时间序列预训练)
    - [（二）大型时间序列模型](#二大型时间序列模型)
    - [（三）实际应用挑战](#三实际应用挑战)
- [七、结论](#七结论)
</p></details><p></p>


# 一、研究背景与意义

1. **时间序列数据特性**：时间序列是按离散时间顺序组织的数据点序列，广泛存在于金融风险评估、能源可持续性、天气预报等现实场景。
   与图像、文本数据不同，其语义信息主要源于时间变化，存在非线性模式与时间变异趋势交织的独特挑战，需复杂方法提取有意义的时间表征。
2. **研究发展历程**：时间序列分析已有数百年研究历史，早期依赖 ARIMA、指数平滑、谱分析等传统统计方法，但这些方法受线性和平稳性假设限制，
   难以处理现实中复杂的非线性关系和长期依赖。近年来，技术重心转向深度学习模型，尤其是原本为自然语言处理设计的 Transformer 模型，
   凭借注意力机制在处理大规模时间序列数据上展现出优势。
3. **现有研究不足**：现有综述多聚焦于特定模型架构或单一分析任务，缺乏对不同任务和模型的全面概述；部分基准测试虽能实现无偏评估，
   但未涵盖深度模型的架构设计细节，难以指导高效时间序列方法的设计与领域发展。

# 二、基础概念

## （一）时间序列定义与核心要素

1. **定义**：时间序列可表示为 `$X = \{x_1, x_2, ..., x_T\} \in \mathbb{R}^{T \times C}$`，
   其中 `$x_t \in \mathbb{R}^C$` 为时间戳 `$t$` 的观测值，`$C$` 为变量数。
   时空数据（如视频、气象记录）可视为特殊时间序列，但本文聚焦不假设变量维度空间结构的通用形式，核心是捕捉时间依赖和变量间相关性。
2. **核心要素**
    - **时间依赖**：指不同时间点或子序列间的复杂关联，传统 ARIMA 等自回归方法可建模此类依赖，但仅适用于时间动态复杂度低的场景。
    - **变量相关性**：多变量时间序列中不同变量间的复杂交互与关联，VAR 模型将自回归扩展到多变量，但只能捕捉线性关系，无法处理错位或非线性关联。

## （二）时间序列分析任务

1. **预测（Forecasting）**：挖掘时间依赖和动态模式，基于历史数据预测未来值或趋势，是时间序列分析的基础任务，应用于天气预测、能源/交通规划等。
2. **插补（Imputation）**：针对传感器故障、数据损坏等导致的缺失数据，利用上下文信息重建缺失值，与预测不同，聚焦“补全过去”而非“预测未来”。
3. **异常检测（Anomaly Detection）**：识别时间序列中的异常模式，可指示关键事件、系统故障等，应用于工业维护等领域。
4. **分类（Classification）**：根据时间序列特征分配标签或类别，广泛用于动作识别、心跳诊断等医疗、行为分析场景。

# 三、深度时间序列模型的基础模块

## （一）平稳化（Stationarization）

1. **概念**：平稳性指时间序列统计特性（均值、方差）随时间不变，是传统统计方法的核心假设，需将非平稳数据转换为平稳数据。
2. **实现方法**
    - **传统方法**：差分、对数变换等预处理手段。
    - **深度学习方法**：数据归一化（如标准化）替代传统平稳化；
        - DAIN 层自适应标准化数据；
        - RevIN 引入可逆实例归一化；
        - Non-Stationary Transformer 通过计算变量特定均值和方差实现序列平稳化，公式为：
        `$$\begin{aligned} 
        \mu_{x} & =\frac{1}{T} \sum_{i=1}^{T} x_{i}, \sigma_{x}^{2}=\sum_{i=1}^{T} \frac{1}{T}\left(x_{i}-\mu_{x}\right)^{2}, \\
        \overline{X} &=\frac{\left(X-\mu_{x}\right)}{\sqrt{\sigma_{x}^{2}+\epsilon}}, \\
        \overline{Y} &=\text{Model}(\overline{X}), \\
        \hat{Y}      &=\sigma_{x}^{2}\left(\overline{Y}+\mu_{x}\right)
        \end{aligned}$$`
        （`$\epsilon$` 为数值稳定性小值，最后通过反归一化恢复原始序列分布）

## （二）分解（Decomposition）

1. **季节-趋势分解（Seasonal-Trend Decomposition）**：将时间序列分离为趋势（长期模式）、周期（非固定周期波动）、季节（固定周期重复模式）、不规则（残差）成分。
    - 传统用滤波、指数平滑实现
    - Autoformer 首次将其融入深度学习架构，通过时间平均池化提取趋势和季节部分
2. **基扩展（Basis Expansion）**：用预定义基函数线性组合表示时间序列，揭示非线性时间关系。如：
    - NBEATS 通过全连接层生成系数实现分层分解
    - N-HiTs 加入下采样层优化分解
    - DEPTS 引入周期状态变量
    - DEWP 通过变量和时间扩展块分别捕捉变量和时间依赖
3. **矩阵分解（Matrix Factorization）**：针对多变量时间序列，将高维矩阵（行=变量，列=时间点）分解为低维 latent 空间矩阵乘积，
   即 `$$\dot{X} \approx F\hat{X}$$`
   其中：`$F \in \mathbb{R}^{k \times C}，\hat{X} \in \mathbb{R}^{T \times k}$`，`$k$` 为超参数
    - TRMF、NoTMF 等通过加入时间或空间正则化、向量自回归过程等优化分解效果

## （三）傅里叶分析（Fourier Analysis）

1. **核心作用**：将信号转换到频域，揭示周期性，通过 FFT、小波变换（WT）连接时域与频域，为深度模型提供全局时间动态视角。
2. **时域建模**：利用傅里叶变换分解序列为周期信号，识别主导周期和频率。如：
    - TimesNet 用 FFT 提取高振幅频率并将 1D 序列转为 2D 空间；
    - FiLM 结合低秩近似保留低频成分；
    - FEDformer 随机选择傅里叶成分（含高低频）捕捉全局信息；
    - Autoformer 基于维纳-辛钦定理用 FFT 实现自相关机制。
3. **频域建模**：同时在时频域分析数据，如：
    - ATFN 用序列模型学趋势+频域块学周期；
    - STFNet 对输入信号用短时傅里叶变换后在频域处理；
    - StemGNN 结合图傅里叶变换和离散傅里叶变换；
    - FourierDiffusion 在频域实现时间序列扩散模型。

# 四、深度时间序列模型架构

## （一）多层感知机（MLP）

1. **核心思想**：受自回归模型启发，用全连接层捕捉时间模式，结构简单但效果显著。
2. **代表模型**
    - **N-BEATS**：由堆叠全连接层构成，含回溯（拟合输入）和预测分支，无时间序列专用组件。
    - **DLinear**：主张在原始空间用简单线性回归，兼顾建模效果与效率。
    - **其他扩展**：
        - N-HiTs 通过多速率采样和分层插值优化预测；
        - TSMixer 用时间混合和特征混合 MLP 提取多视角信息；
        - Koopa 基于库普曼理论分层解耦动态系统。

## （二）循环神经网络（RNN）

1. **核心优势**：专为序列数据设计，解决 vanilla RNN 梯度消失问题，建模多变量相关性。
2. **代表模型与改进**
    - **LSTNet**：结合循环结构与卷积层，捕捉短程变量依赖和长程模式，引入循环跳跃组件缓解梯度消失。
    - **DeepAR**：自回归循环网络，预测未来时间点概率分布，可处理少历史数据场景。
    - **状态空间模型（SSM）融合**：
        - DSSM 用 RNN 参数化线性 SSM；
        - S4 通过低秩修正优化 SSM 参数；
        - Mamba 以线性复杂度处理长序列，通过选择机制识别有效信息。
    - **神经常微分方程（Neural ODEs）**：将 RNN 扩展到连续时间，参数化隐藏状态导数，适用于不规则采样数据。

## （三）卷积神经网络（CNN）

1. **核心能力**：捕捉局部特征，通过 1D 卷积适配时间序列连续性，部分模型扩展到 2D 空间建模。
2. **代表模型**
    - **1D CNN 变体**：
        - SCINet 用分层下采样-卷积-交互架构；
        - TCN 用扩张卷积扩大感受野；
        - MICN 结合多卷积核兼顾局部与全局时间关联。
    - **2D 空间建模**：TimesNet 根据估计周期将 1D 序列转为 2D 张量，用 Inception 块处理，同时捕捉周期内和周期间多尺度变化，公式为：
    `$$\begin{aligned} 
    & X_{2 D}^{i}=\text{Reshape}(\text{Padding}(X_{1 D})), i \in\{1, \cdots, k\} \\ 
    & \hat{X}_{2 D}^{i}=\text{Inception}(X_{2 D}^{i}), i \in\{1, \cdots, k\} \\ 
    & \hat{X}_{1 D}^{i}=\text{Trunc}(\text{Reshape}(\hat{X}_{2 D}^{i})), i \in\{1, \cdots, k\}
    \end{aligned}$$`
    （`$k$` 为超参数，对应不同周期的 1D-2D 转换）

## （四）图神经网络（GNN）

1. **核心逻辑**：将多变量时间序列建模为时空图（节点=变量），提取节点邻域关系和时间演化，捕捉变量非线性关联。
2. **分类与代表模型**
    - **预定义图结构**：
        - DCRNN 将交通空间依赖视为图上扩散过程；
        - STGCN 结合图卷积与时间卷积。
    - **动态学习图结构**：
        - Graph WaveNet 通过节点嵌入学习自适应依赖矩阵；
        - AGCRN 含节点自适应参数学习和数据自适应图生成；
        - MTGNN 用图学习层自适应学习邻接矩阵。

## （五）Transformer

1. **核心优势**：自注意力机制捕捉长期时间依赖和多变量关联，按注意力粒度分为三类。
2. **分类与代表模型**
    - **点级依赖（Point-wise）**：学习单个时间点表示，如：
        - Informer 用 ProbSparse 自注意力降低计算复杂度；
        - Pyraformer 构建多分辨率树实现线性复杂度注意力。
    - **块级依赖（Patch-wise）**：将序列分割为块，捕捉局部语义，如：
        - Autoformer 用自相关机制替代点级自注意力；
        - PatchTST 将序列分为重叠块，用线性投影和位置嵌入建模块级依赖；
        - Crossformer 通过跨时间和跨维度阶段捕捉块间依赖。
    - **序列级依赖（Series-wise）**：对整个时间序列 token 化，捕捉序列间关联，如：
        - iTransformer 用 VariateEmbed 生成变量全局表示；
        - TimeXer 结合块级和序列级表示，处理外生变量。

# 五、时间序列库（TSLib）

## （一）设计目标与优势

1. **解决现有问题**：弥补现有基准测试任务覆盖少、缺乏模型架构与基线方法总结的缺陷，提供公平、全面的深度时间序列模型评估平台。
2. **核心优势**
    - **多数据格式支持**：原生支持 `.csv`、`.npz`、`.txt` 等格式，涵盖 30+ 来自能源、交通、医疗等领域的数据集，
      含数据预处理（时间窗口分割、归一化等）模块。
    - **灵活模型构建**：将模型抽象为“基础模块+架构”组合，支持不同任务用“通用骨干网络+任务专用头”实现统一评估。
    - **全面性能评估**：支持预测（分长短时）、分类、插补、异常检测 4 类任务，每类任务有专属评估指标。

## （二）关键组成

1. **评估协议与指标**
    | 任务 | 核心指标 | 说明 |
    |---|---|---|
    | 长时预测/插补 | MSE、MAE | 衡量预测/插补值与真实值的误差 |
    | 短时预测 | SMAPE、MASE | 减少异常值影响，更侧重绝对误差 |
    | 分类 | 准确率（Accuracy） | 正确分类样本占比 |
    | 异常检测 | F1 分数 | 平衡精确率与召回率，适配不平衡数据 |
2. **支持数据集**
    - **预测**：长时（ETT、电力、天气等 9 个数据集）、短时（M4 数据集，含 6 个子集）。
    - **插补**：基于 ETT、电力、天气数据集，随机掩盖部分时间步构建缺失场景。
    - **异常检测**：SMD、MSL、SMAP、SWaT、PSM 等工业场景数据集。
    - **分类**：UEA 时间序列分类库的 10 个多变量数据集，涵盖手势识别、医疗诊断等。
3. **实验设置**
    - **训练配置**：基于 PyTorch 和 NVIDIA A100 GPU，用 Adam 优化器，超参数搜索隐藏层数量（2-4 层）和隐藏表示维度（`$2^4-2^9$`），
      不同任务有专属学习率、批大小和 epoch 数（如长时预测学习率 `$10^{-3}$`，批大小 32，epoch 10）。
    - **回溯长度搜索**：长时预测中系统搜索输入长度（96、192、336、512），确保模型性能最优。

## （三）实验结果与分析

1. **模型性能对比**
    - **预测任务**：
        - Transformer 类模型（iTransformer、PatchTST）表现最优，证明 token 化策略的重要性；
        - MLP 类模型（DLinear）虽简单，但在预测任务效率高。
    - **其他任务**：
        - TimesNet（CNN 类）在分类、插补、异常检测中表现全面；
        - CNN 类模型整体在非预测任务中优势显著；
        - RNN 类模型仅在异常检测中表现较好。
2. **效率分析**
    - **训练/推理时间**：
        - PatchTST（块级 token）、iTransformer（序列级 token）比 Autoformer（点级 token）快；
        - MLP 类（DLinear）和 Mamba 效率最高；
        - CNN 类（SCINet、TimesNet）因多卷积核耗时久。
    - **内存占用**：Transformer 类模型内存需求较高，MLP 类和 Mamba 更适合资源受限场景。

# 六、未来研究方向

## （一）时间序列预训练

1. **对比学习**：通过相似/不相似样本对学习表示，如：
    - CPC 用模型预测特征为正样本
    - TimeCLR 用 DTW 数据增强
    - TS2Vec 采用分层对比学习。
2. **掩码建模**：基于上下文预测掩码 token，如：
    - TST 借鉴 BERT 预训练框架
    - PatchTST 用块级掩码
    - HiMTM 实现分层掩码建模

## （二）大型时间序列模型

1. **时间序列基础模型**：借鉴 NLP/CV 基础模型思路，预训练通用模型，如 TimeGPT、Lag-LlaMa 聚焦单变量，
   MOIRAI 尝试多变量，但多模态融合（结合文本、日历数据）是未来方向。
2. **大语言模型（LLM）适配**
    - **微调**：LLM4TS 分“时间序列对齐+下游任务微调”两阶段；Chronos 将时间序列量化为 token，用交叉熵损失训练。
    - **提示学习**：PromptCast、TimeLLM 设计提示模板实现预测，但其他任务的提示方法仍待探索。

## （三）实际应用挑战

1. **概率预测**：突破预定义分布限制，生成更灵活的未来结果分布，辅助风险决策。
2. **超长序列处理**：现有分块方法易因块长度选择影响性能，需探索更高效的长序列建模架构。
3. **外生变量利用**：建立“内生变量-外生变量”统一建模框架，提升模型可解释性与性能。
4. **异质数据处理**：解决不同采样率、不规则性、长度的异质时间序列建模难题，适配动态输入。

# 七、结论

1. **综述价值**：首次从“基础模块+模型架构”双视角，全面覆盖预测、分类、插补、异常检测4类任务，揭示深度时间序列模型的设计原理。
2. **TSLib 价值**：提供 30 个实现模型、30+ 数据集、4 类任务支持，
   通过统一实验框架为研究与应用提供模型选择指导和代码基础（代码地址：https://github.com/thuml/Time-Series-Library）。
3. **未来展望**：预训练、大型模型、实际应用挑战（如超长序列、异质数据）将是深度时间序列模型的核心研究方向，有望推动该领域在更多现实场景的落地。

