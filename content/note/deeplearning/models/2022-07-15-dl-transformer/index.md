---
title: Transformer
subtitle: All Attention
author: ç‹å“²å³°
date: '2022-04-05'
slug: dl-transformer
categories:
  - deeplearning
tags:
  - model
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>

<details><summary>ç›®å½•</summary><p>

- [Transformer æ•´ä½“æ¶æ„æ¶æ„](#transformer-æ•´ä½“æ¶æ„æ¶æ„)
  - [Transformer æ¶æ„](#transformer-æ¶æ„)
  - [Transformer è¾“å…¥](#transformer-è¾“å…¥)
- [ç¼–ç æ¨¡å—](#ç¼–ç æ¨¡å—)
  - [ä»é«˜å¤„çœ‹ Self-attention](#ä»é«˜å¤„çœ‹-self-attention)
  - [ä»ç»†èŠ‚çœ‹ Self-attention](#ä»ç»†èŠ‚çœ‹-self-attention)
    - [Self-attention çš„å‘é‡è®¡ç®—](#self-attention-çš„å‘é‡è®¡ç®—)
    - [Self-attention çš„çŸ©é˜µè®¡ç®—](#self-attention-çš„çŸ©é˜µè®¡ç®—)
  - [Multi-head æœºåˆ¶](#multi-head-æœºåˆ¶)
  - [ä½¿ç”¨ä½ç½®ç¼–ç è¡¨å¾åºåˆ—é¡ºåº](#ä½¿ç”¨ä½ç½®ç¼–ç è¡¨å¾åºåˆ—é¡ºåº)
  - [æ®‹å·®é¡¹](#æ®‹å·®é¡¹)
- [è§£ç æ¨¡å—](#è§£ç æ¨¡å—)
  - [Encoder-Deocder Attention å±‚](#encoder-deocder-attention-å±‚)
  - [Linear å’Œ Softmax å±‚](#linear-å’Œ-softmax-å±‚)
- [è®­ç»ƒæ¦‚è¦](#è®­ç»ƒæ¦‚è¦)
- [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
- [Transformer ç»“æ„è§£æ](#transformer-ç»“æ„è§£æ)
- [Transformer æ•°å­¦è¡¨ç¤º](#transformer-æ•°å­¦è¡¨ç¤º)
- [Transformer è¦ç‚¹é—®é¢˜](#transformer-è¦ç‚¹é—®é¢˜)
- [PyTorch ç¤ºä¾‹](#pytorch-ç¤ºä¾‹)
- [å‚è€ƒ](#å‚è€ƒ)
</p></details><p></p>

Transformer æ˜¯ Google åœ¨ 2017 å¹´æå‡ºçš„ä¸€ä¸ª NLP æ¨¡å‹ï¼Œé€‚ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚
å®ƒä¸ä¾èµ–äºä½¿ç”¨ CNN å’Œ RNNï¼Œè€Œæ˜¯åŸºäºæ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)æ„å»ºç½‘ç»œç»“æ„

# Transformer æ•´ä½“æ¶æ„æ¶æ„

## Transformer æ¶æ„

å¦‚æœæŠŠ Transformer æ¨¡å‹å½“ä½œé»‘ç›’ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡é‡Œï¼Œæ¨¡å‹æ¥å—ä¸€ç§è¯­è¨€çš„å¥å­ï¼Œè¾“å‡ºå¦ä¸€ç§è¯­è¨€çš„ç¿»è¯‘å¥å­

![img](images/tf1.png)

æ‰“å¼€ä¸­é—´è¿™ä¸ª Transformer ç»“æ„ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°ä¸€ä¸ªç¼–ç æ¨¡å—å’Œä¸€ä¸ªè§£ç æ¨¡å—ï¼Œä¸”å®ƒä»¬äº’ç›¸è¿æ¥

![img](images/ed.png)

ç¼–ç æ¨¡å—æœ‰å¤šä¸ªç¼–ç å™¨å †å è€Œæˆ (è®ºæ–‡å †äº† 6 ä¸ªï¼Œæ•°å­— 6 æ²¡æœ‰ç‰¹åˆ«è§„å®šï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹è¿›è¡Œå®éªŒ)ï¼Œ
è§£ç æ¨¡å—ä¹Ÿå †å äº† 6 ä¸ªè§£ç å™¨

![img](images/ed2.png)

ç¼–ç å™¨ä¹‹é—´éƒ½æ˜¯ç›¸åŒç»“æ„ï¼Œä½†ä¸å…±äº«æƒé‡ã€‚æ¯ä¸ªç¼–ç å™¨æœ‰ä¸¤å±‚ï¼š

![img](images/ed3.png)

ç¼–ç å™¨çš„è¾“å…¥é¦–å…ˆæµå…¥ä¸€ä¸ªè‡ªå…³æ³¨å±‚ (self-attention layer) - å½“ç¼–ç å™¨å¯¹æŸä¸ªç‰¹å®šå•è¯è¿›è¡Œç¼–ç æ—¶ï¼Œ
è¯¥å±‚ä¼šå¸®åŠ©ç¼–ç å™¨å…³æ³¨è¾“å…¥å¥å­ä¸­çš„å…¶å®ƒå•è¯ã€‚åç»­ä¼šè¯¦ç»†è®²è¿™å—

è‡ªå…³æ³¨å±‚çš„è¾“å‡ºä¼šè¢«å–‚å…¥å‰é¦ˆç¥ç»ç½‘ç»œ (feed-forward neural network)ï¼Œæ¯ä¸ªè¾“å…¥ä½ç½®ä¸Šéƒ½æœ‰ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œ
å®ƒä»¬ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ (è¡¥å……: è®ºæ–‡è¯´å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯ point-wise)

è§£ç å™¨ä¹Ÿæœ‰ç¼–ç å™¨çš„è¿™ä¸¤å±‚ï¼Œä½†æ˜¯åœ¨å®ƒä»¬ä¸­é—´è¿˜æœ‰ä¸ªå…³æ³¨å±‚ï¼Œå¸®åŠ©è§£ç å™¨å…³æ³¨è¾“å…¥å¥å­çš„ç›¸å…³éƒ¨åˆ† (è·Ÿ seq2seq æ¨¡å‹é‡Œçš„å…³æ³¨æœºåˆ¶ç±»ä¼¼)

![img](images/ed4.png)

## Transformer è¾“å…¥

æˆ‘ä»¬å·²ç»çœ‹è¿‡æ¨¡å‹çš„ä¸»è¦æ¨¡å—ï¼Œç°åœ¨æ¥çœ‹ä¸‹ï¼Œå‘é‡/å¼ é‡æ˜¯å¦‚ä½•åœ¨è¿™äº›æ¨¡å—ä¸­ï¼Œä»è¾“å…¥åˆ°è¾“å‡ºã€‚
ä»¥ NLP å¸¸è§åº”ç”¨çš„ä¸ºä¾‹ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ª Embedding ç®—æ³•å°†æ¯ä¸ªè¾“å…¥è¯è½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡ï¼š

![img](images/vec1.png)

Word Embedding åªå‘ç”Ÿåœ¨ç¼–ç å™¨çš„æœ€åº•ç«¯ï¼Œå¯¹äºæ‰€æœ‰ç¼–ç å™¨æ¥è¯´ï¼Œå®ƒä»¬ä¼šæ¥æ”¶ä¸€ä¸ª listï¼Œlist å†…å«æœ‰å¤šä¸ªé•¿åº¦ä¸º 512 çš„è¯å‘é‡ã€‚
ä½†å¯¹å…¶ä»–ç¼–ç å™¨æ¥è¯´ï¼Œå®ƒä»¬çš„è¾“å…¥ä¼šæ˜¯ä¸Šä¸€å±‚ç¼–ç å™¨çš„è¾“å…¥ã€‚list çš„é•¿åº¦æ˜¯å¯è°ƒæ•´çš„è¶…å‚ï¼Œä¸€èˆ¬ä¸ºè®­ç»ƒé›†ä¸­æœ€é•¿å¥å­çš„é•¿åº¦

å¯¹è¾“å…¥åºåˆ—è¿›è¡Œè¯å‘é‡åŒ–åï¼Œæ¯ä¸ªè¯å‘é‡ä¼šä¾æ¬¡æµå…¥ä¸‹é¢ç¼–ç å™¨ä¸­çš„ä¸¤ä¸ªå­å±‚

![img](images/vec2.png)

ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹ Transformer çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ - å¥å­ä¸­çš„æ¯ä¸ªå¯¹åº”ä½ç½®ä¸Šçš„è¯æ˜¯æŒ‰ç…§å®ƒè‡ªæœ‰è·¯å¾„æµå…¥ç¼–ç å™¨çš„ï¼Œ
åœ¨ self-attention å±‚è¿™äº›è·¯å¾„æ˜¯ç›¸äº’ä¾èµ–çš„ã€‚ä½† Feed-forward å±‚ä¸å­˜åœ¨ä¾èµ–æ€§ï¼Œ
å› æ­¤ï¼Œå½“ self-attention å±‚çš„è¾“å…¥æµå…¥ feed-forward å±‚ï¼Œè¿™äº›ç‹¬ç«‹çš„è·¯å¾„å¯ä»¥å¹¶è¡Œ

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç›®å…‰è½¬å‘ä¸€ä¸ªæ›´çŸ­çš„å¥å­ï¼Œæˆ‘ä»¬çœ‹ä¸‹åœ¨ç¼–ç å™¨æ¯ä¸ªå­å±‚é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

# ç¼–ç æ¨¡å—

æ­£å¦‚ä¹‹å‰æåˆ°çš„ï¼Œç¼–ç å™¨æ¥æ”¶ä¸€ä¸ª list çš„è¯å‘é‡ä½œä¸ºè¾“å…¥ã€‚å®ƒå°† list é‡Œçš„å‘é‡ä¼ å…¥ self-attention å±‚ï¼Œ
ç„¶åå–‚å…¥ feed-forward å±‚ï¼Œæœ€åè¾“å‡ºç»™ä¸‹ä¸ªç¼–ç å™¨

![img](images/encoding1.png)

> æ³¨æ„å¯¹äºæ¯ä¸ªä½ç½®çš„è¯å‘é‡æ¥è¯´ï¼ŒFeed-forward å±‚éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹ï¼Œ
> Transformer ä½œè€…ä¹Ÿå› æ­¤ç§°ä¹‹ä¸º position-wise fully connected feed-forward networkã€‚
> è€Œ self-attention å±‚åˆ™æ˜¯å¤šä¸ªè¯å‘é‡å…±ç”¨çš„

## ä»é«˜å¤„çœ‹ Self-attention

ä¸è¦è¢« â€œself-attentionâ€ è¿™è¯è¿·æƒ‘äº†ï¼Œå®ƒä¸æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„å«ä¹‰ã€‚ç›´åˆ°é˜…è¯»äº†ã€ŠAttention is all you needã€‹åŸæ–‡ï¼Œ
æ‰å¼„æ˜ç™½äº†æ­¤æ¦‚å¿µã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸‹å®ƒå…·ä½“æ€ä¹ˆå·¥ä½œçš„

å¦‚æœè¯´ï¼Œæˆ‘ä»¬æƒ³ç¿»è¯‘ä»¥ä¸‹å¥å­ï¼š

> â€œThe animal didn't cross the street because it was too tiredâ€

å¥å­ä¸­çš„ â€œitâ€ æ˜¯æŒ‡ä»£ä»€ä¹ˆï¼ŸæŒ‡ä»£ `street` è¿˜æ˜¯ `animal`ï¼Ÿå¯¹äººæ¥è¯´å¾ˆç®€å•ï¼Œä½†å¯¹ç®—æ³•æ¥è¯´ä¸ç®€å•ã€‚
å½“æ¨¡å‹å¤„ç† â€œitâ€ æ—¶ï¼Œself-attention å…è®¸å°† â€œitâ€ å’Œ â€œanimalâ€ å…³è”èµ·æ¥

å½“æ¨¡å‹å¤„ç†æ¯ä¸ªè¯æ—¶ (å³åœ¨è¾“å…¥åºåˆ—çš„æ¯ä¸ªä½ç½®ä¸Š)ï¼Œself-attention å…è®¸å…³æ³¨è¾“å…¥åºåˆ—å…¶å®ƒä½ç½®ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œ
å¸®åŠ©å¯¹å½“å‰è¯æ›´å¥½åœ°ç¼–ç ã€‚å¦‚æœä½ ç†Ÿæ‚‰ RNNï¼Œæƒ³æƒ³éšè—å±‚æ˜¯å¦‚ä½•ä½¿å¾— RNN åˆ©ç”¨å†å²è¯/å‘é‡çš„è¡¨å¾å»è·å–å½“å‰è¯çš„è¡¨å¾å‘é‡ã€‚
Transformer çš„ self-attention æ˜¯å°†å¯¹ç›¸å…³è¯çš„ç†è§£èå…¥åˆ°å½“å‰è¯çš„å¤„ç†å½“ä¸­

æ¯”å¦‚ï¼šå½“æˆ‘ä»¬åœ¨ç¬¬ 5 ä¸ªç¼–ç å™¨ (å³å †å åœ¨ä¸Šé¢çš„ç¼–ç å™¨) ç¼–ç  â€œitâ€ æ—¶ï¼Œéƒ¨åˆ†å…³æ³¨ä¼šé›†ä¸­åœ¨ â€œThe Animalâ€ ä¸Šï¼Œ
ç„¶åå°†å®ƒçš„è¡¨å¾èåˆè¿› â€œitâ€ çš„ç¼–ç ä¸­

![img](images/self-attention1.png)

## ä»ç»†èŠ‚çœ‹ Self-attention

é¦–å…ˆï¼Œçœ‹ä¸‹å¦‚ä½•ä½¿ç”¨å‘é‡è®¡ç®— self-attentionï¼Œç„¶åè¿›ä¸€æ­¥çœ‹å¦‚ä½•ä½¿ç”¨çŸ©é˜µå®æ–½è®¡ç®—

### Self-attention çš„å‘é‡è®¡ç®—

1. ç¬¬ä¸€æ­¥åœ¨è®¡ç®— self-attention ä¸­ï¼Œæ˜¯å¯¹ç¼–ç å™¨çš„æ¯ä¸ªè¾“å…¥å‘é‡ (å³æ¯ä¸ªè¯çš„å‘é‡) åˆ›å»º 3 ä¸ªå‘é‡ã€‚
å¯¹äºæ¯ä¸ªè¯ï¼Œæˆ‘ä»¬ä¼šåˆ›å»º Query å‘é‡ã€Key å‘é‡å’Œ Value å‘é‡ï¼Œå°†ä¸‰ä¸ªæƒé‡çŸ©é˜µä¹˜ä»¥è¯å‘é‡ä¾¿å¯å¾—åˆ°è¿™ä¸‰ä¸ªå‘é‡ï¼Œ
è¿™ä¸‰ä¸ªçŸ©é˜µä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å­¦ä¹ 

é‚£ä»€ä¹ˆæ˜¯ Query å‘é‡ã€Key å‘é‡å’Œ Value å‘é‡å‘¢ï¼Ÿå¯¹äºè®¡ç®—å’Œæ€è€ƒ attention æ¥è¯´ï¼Œæå–å®ƒä»¬æ˜¯æœ‰ç›Šçš„ã€‚
ä¸€æ—¦ä½ å¾€ä¸‹äº†è§£ attention æ˜¯å¦‚ä½•è®¡ç®—ï¼Œä½ å°±ä¼šäº†è§£åˆ°æ¯ä¸ªå‘é‡æ˜¯æ‰®æ¼”ç€ä»€ä¹ˆè§’è‰²äº†

![img](images/self-attention2.png)

> æ³¨æ„ï¼Œç›¸æ¯”äºè¯å‘é‡ (512ç»´)ï¼Œè¿™ä¸‰ä¸ªå‘é‡çš„ç»´åº¦æ˜¯åå°çš„ (64ç»´)ã€‚ä¸‰å‘é‡ä¸æ˜¯ä¸€å®šè¦å°ï¼Œ
> è¿™æ˜¯ä¸ºäº†è®© Multi-head Attention çš„è®¡ç®—æ˜¯è¿ç»­çš„ (è¡¥å……: è¿™é‡Œå¯èƒ½æ¯”è¾ƒæ¨¡ç³Šï¼Œå…¶å®æ˜¯è¿™æ ·çš„ï¼Œself-attention å…¶å®æ˜¯ä¸ª multi-head çš„å½¢å¼ï¼Œ
> å³å¤šä¸ª attention æ¨¡å—ï¼ŒåŸè®ºæ–‡æ˜¯è¯´æœ‰ 8 ä¸ªå¹¶è¡Œçš„ attention å±‚ï¼Œè¿™æ ·è‹¥æ€»ç»´åº¦ä¸º 512ï¼Œé‚£ä¼šå˜ä¸º 512/8=64 ç»´ï¼Œ
> ç›¸å½“äºå…¨ 512 ç»´åº¦è¾“å…¥ single-head attention å˜ä¸ºäº† 64 ç»´è¾“å…¥ Multi-head Attentionï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º)

![img](images/self-attention3.png)

2. è®¡ç®— self-attention çš„ç¬¬äºŒæ­¥æ˜¯è®¡ç®—ä¸€ä¸ªåˆ†æ•°ã€‚ä¸¾ä¾‹ï¼Œæˆ‘ä»¬åœ¨å¯¹ â€œThinkingâ€ è¿™ç¬¬ä¸€ä¸ªè¯è®¡ç®— self-attentionï¼Œ
å½“å‰è¯è¦å¯¹è¾“å…¥å¥å­ä¸­å…¶å®ƒè¯å¯¹è¿›è¡Œæ‰“åˆ†ï¼Œè¿™ä¸ªåˆ†æ•°å†³å®šäº†ï¼šå½“æˆ‘ä»¬å¯¹æŸç»™å®šä½ç½®ä¸Šçš„è¯è¿›è¡Œç¼–ç æ—¶ï¼Œåº”è¯¥ç»™è¾“å…¥å¥å­ä¸­å…¶å®ƒä½ç½®ä¸Šçš„è¯å¤šå°‘å…³æ³¨ã€‚
Query å‘é‡å’Œ Key å‘é‡ç‚¹ä¹˜ä¾¿å¯å¾—åˆ°åˆ†æ•°ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬å¯¹ä½ç½® 1 çš„è¯è®¡ç®— self-attentionï¼Œ
`$q_{1}$` ç‚¹ä¹˜ `$k_{1}$` ä¾¿å¯å¾—åˆ°ç¬¬ä¸€ä¸ªåˆ†æ•°ï¼Œç¬¬äºŒä¸ªåˆ†æ•°åˆ™æ˜¯ `$q_{1}$` å’Œ `$k_{2}$` çš„ç‚¹ä¹˜

![img](images/self-attention4.png)

3. ç¬¬ä¸‰æ­¥æ˜¯å°†åˆ†æ•°é™¤ 8ï¼Œè®ºæ–‡æåˆ°è¿™é‡Œæ˜¯å¯¹ Key å‘é‡ç»´åº¦å¼€æ–¹ï¼Œå³å¯¹ 64 å¼€æ–¹ä¸º 8ï¼Œè¿™èƒ½å¸®åŠ©æ‹¥æœ‰æ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œ
   è¿™ä¹Ÿå¯ä»¥æ˜¯å…¶å®ƒå¯èƒ½å€¼ï¼Œä½†è¿™ä¸ªæ˜¯é»˜è®¤çš„(è¡¥å……: ä½œè€…æ˜¯æ‹…å¿ƒå¯¹äºå¤§çš„ Key å‘é‡ç»´åº¦ä¼šå¯¼è‡´ç‚¹ä¹˜ç»“æœå˜å¾—å¾ˆå¤§ï¼Œ
   å°† Softmax å‡½æ•°æ¨å‘å¾—åˆ°æå°æ¢¯åº¦çš„æ–¹å‘ï¼Œå› æ­¤æ‰å°†åˆ†æ•°é™¤ä»¥ Key å‘é‡ç»´åº¦å¼€æ–¹å€¼)
4. ç¬¬å››æ­¥æ˜¯åŸºäºä¸Šé¢çš„åˆ†æ•°æ±‚ Softmax åˆ†æ•°ï¼Œè¿™ä¸ª Softmax åˆ†æ•°å†³å®šå¯¹å½“å‰ä½ç½®ä¸Šçš„è¯ï¼Œå¥å­ä¸Šçš„å„è¯è¯¥è¡¨è¾¾å¤šå°‘ç¨‹åº¦ã€‚
   æ˜æ˜¾åœ¨å½“å‰ä½ç½®ä¸Šçš„è¯è·å–æœ€é«˜çš„ Softmax åˆ†æ•°ï¼Œä½†æœ‰æ—¶ï¼Œä¸å½“å‰è¯æœ‰å…³çš„å…¶å®ƒè¯å¦‚æœèƒ½å‚ä¸è¿›æ¥ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„

![img](images/self-attention5.png)

5. ç¬¬äº”æ­¥æ˜¯ Value å‘é‡ä¸ Softmax åˆ†æ•°ç›¸ä¹˜(ä»¥ä¾¿ç›¸åŠ )ï¼Œè¿™æ˜¯ä¸ºäº†ä¿ç•™æˆ‘ä»¬æƒ³å…³æ³¨çš„è¯ï¼Œ
   æ©ç›–æ‰ä¸ç›¸å¹²çš„è¯ï¼Œä¾‹å¦‚ï¼šç»™ä»–ä»¬ä¹˜ä¸Šæå°å€¼ 0.001
6. ç¬¬å…­æ­¥æ˜¯åŠ æ€»è¿™äº›åŠ æƒçš„ Value å‘é‡ï¼Œå¯¹äºç¬¬ä¸€ä¸ªè¯ï¼Œè¿™ä¾¿ç”Ÿæˆäº† self-attention å±‚åœ¨æ­¤ä½ç½®ä¸Šçš„è¾“å‡º

![img](images/self-attention6.png)

ä¸Šé¢å°±æ˜¯ self-attention å±‚çš„è®¡ç®—è¿‡ç¨‹ï¼Œç»“æœå‘é‡å¯ä»¥è¾“å…¥ç»™ feed-forword ç¥ç»ç½‘ç»œã€‚
åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œæ˜¯ä½¿ç”¨çŸ©é˜µè®¡ç®—åŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹ä¸‹å•è¯çº§åˆ«çš„çŸ©é˜µè®¡ç®—

### Self-attention çš„çŸ©é˜µè®¡ç®—

1. ç¬¬ä¸€æ­¥æ˜¯è®¡ç®— Queryã€Key å’Œ Value çŸ©é˜µï¼Œæˆ‘ä»¬æ˜¯é€šè¿‡æ‰“åŒ…è¯å‘é‡æˆçŸ©é˜µ `$X$`ï¼Œ
   ç„¶ååˆ†åˆ«ä¹˜ä¸Šä¸‰ä¸ªå¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ `$(W^{Q}, W^{K}, W^{V})$`ã€‚åœ¨çŸ©é˜µ `$X$` ä¸­ï¼Œæ¯ä¸€è¡Œå¯¹åº”è¾“å…¥å¥å­ä¸­æ¯ä¸€ä¸ªå•è¯ï¼Œ
   æˆ‘ä»¬å†æ¬¡çœ‹åˆ°è¯å‘é‡é•¿åº¦ (512ï¼Œå›¾ä¸­çš„ 4 ä¸ª box) å’Œ Q/K/V å‘é‡é•¿åº¦ (64, å›¾ä¸­çš„ 3 ä¸ª box) æ˜¯ä¸ä¸€æ ·çš„

![img](images/self-attention7.png)

2. æœ€åï¼Œå› ä¸ºæˆ‘ä»¬è¦å¤„ç†è¿™äº›çŸ©é˜µï¼Œæˆ‘ä»¬èƒ½å‹ç¼©ç¬¬äºŒæ­¥åˆ°ç¬¬å…­æ­¥åˆ°ä¸€ä¸ªæ–¹ç¨‹å¼ï¼Œä»è€Œè®¡ç®—å‡º self-attention å±‚çš„è¾“å‡ºç»“æœ

![img](images/self-attention8.png)

## Multi-head æœºåˆ¶

è®ºæ–‡åœ¨ self-attention å±‚å‰è¿˜åŠ å…¥äº† â€œmulti-headedâ€ çš„å…³æ³¨æœºåˆ¶ï¼Œå®ƒä»ä¸¤æ–¹é¢æå‡äº† attention å±‚çš„è¡¨ç°ï¼š

1. Multi-head Attention å¢å¼ºäº†æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›ã€‚
   - æ˜¯çš„ï¼Œåœ¨ä»¥ä¸Šä¾‹å­ä¸­ï¼Œ`$z_{1}$` æ²¡æ€ä¹ˆåŒ…æ‹¬å…¶ä»–è¯ç¼–ç çš„ä¿¡æ¯ (ä¾‹è¿‡äºå…³æ³¨ â€œitâ€ å¹¶ä¸èƒ½å¸¦æ¥å¾ˆå¤šä¿¡æ¯)ï¼Œ
     ä½†å®é™…ä¸Šå¯èƒ½æ˜¯ç”±çœŸå®å€¼æ‰€æ§åˆ¶çš„ (ä¾‹å¦‚å…³æ³¨ â€œitâ€ æŒ‡ä»£çš„ â€œThe animalâ€ ä¼šå¯¹æ¨¡å‹æ›´å¥½)ï¼Œ
     æ‰€ä»¥å¦‚æœæˆ‘ä»¬ç¿»è¯‘ â€œThe animal didnâ€™t cross the street because it was too tiredâ€ï¼Œ
     å¤šå¤´å…³æ³¨èƒ½å¸®åŠ©æˆ‘ä»¬çŸ¥é“ â€œitâ€ æŒ‡ä»£çš„æ˜¯å“ªä¸ªè¯ï¼Œä»è€Œæå‡æ¨¡å‹è¡¨ç°ã€‚
2. Multi-head æœºåˆ¶ç»™ attention å±‚å¸¦æ¥å¤šä¸ª â€œè¡¨å¾å­ç©ºé—´â€
    - æˆ‘ä»¬æ™šç‚¹ä¼šçœ‹åˆ°ï¼ŒMulti-head Attention ä¸åªæ˜¯ 1 ä¸ªï¼Œè€Œæ˜¯å¤šä¸ª Query/Key/Value çŸ©é˜µ (Transformer ç”¨äº† 8 ä¸ªå…³æ³¨å¤´ï¼Œ
      æ‰€ä»¥å¯¹äºæ¯ä¸ªç¼–ç å™¨/è§£ç å™¨ï¼Œæˆ‘ä»¬æœ‰ 8 ç»„)ï¼Œæ¯ç»„éƒ½æ˜¯éšæœºåˆå§‹åŒ–ã€‚ç„¶åï¼Œè®­ç»ƒä¹‹åï¼Œ
       æ¯ç»„ä¼šå°†è¾“å…¥å‘é‡ (æˆ–è€…æ˜¯æ¥è‡ªæ›´ä½éƒ¨ç¼–ç å™¨/è§£ç å™¨çš„å‘é‡) æ˜ å°„åˆ°ä¸åŒçš„è¡¨å¾ç©ºé—´

    ![img](images/m-attention1.png)

    - åœ¨ Multi-head Attention ä¸‹ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ Q/K/V æƒé‡çŸ©é˜µï¼Œå› æ­¤æ¯ä¸ªå¤´ä¼šç”Ÿæˆä¸åŒçš„ Q/K/V çŸ©é˜µã€‚
      æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšï¼Œæˆ‘ä»¬å°†Xå’Œ `$W^{Q}$`/ `$W^{K}$` / `$W^{V}$` çŸ©é˜µç›¸ä¹˜ä¾¿å¯å¾—åˆ° Q/K/V çŸ©é˜µã€‚
      å¦‚æœæˆ‘ä»¬æŒ‰ä¸Šé¢æ–¹å¼å»è®¡ç®—ï¼Œ8 æ¬¡ä¸ä¸åŒæƒé‡çŸ©é˜µç›¸ä¹˜ä¼šå¾—åˆ° 8 ä¸ªä¸åŒçš„ZçŸ©é˜µ

    ![img](images/m-attention2.png)

    - è¿™æœ‰ç‚¹å›°éš¾äº†ï¼Œå› ä¸º feed-forward å±‚ä¸æœŸæœ›è·å– 8 ä¸ªçŸ©é˜µï¼Œå®ƒå¸Œæœ›å¾—åˆ° 1 ä¸ªçŸ©é˜µ (å³ä¸€ä¸ªè¯ 1 ä¸ªå‘é‡)ï¼Œ
      æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ªæ–¹æ³•å°†è¿™ 8 ä¸ªçŸ©é˜µå˜ä¸º 1 ä¸ªçŸ©é˜µã€‚æ€ä¹ˆåšå‘¢ï¼Ÿæˆ‘ä»¬ç›´æ¥åˆå¹¶çŸ©é˜µï¼Œç„¶åä¹˜ä¸Šä¸€ä¸ªé¢å¤–çš„æƒé‡çŸ©é˜µ `$W^{O}$` å°±å¥½äº†

    ![img](images/m-attention3.png)

è¿™å·®ä¸å¤šå°±æ˜¯ Multi-head Self-attention å±‚çš„å…¨éƒ¨äº†ï¼Œè®©æˆ‘ä»¬æ¥æŠŠè¿™äº›çŸ©é˜µæ”¾åœ¨ä¸€å—çœ‹çœ‹ï¼š

![img](images/m-attention4.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æ¥è§¦äº† attention çš„å¤´äº†ï¼Œæˆ‘ä»¬é‡æ¸©ä¸‹ä¹‹å‰çš„ä¾‹å­ï¼Œå»çœ‹ä¸‹åœ¨ç¼–ç  â€œitâ€ æ—¶ï¼Œä¸åŒå…³æ³¨å¤´æ˜¯æ€ä¹ˆå…³æ³¨ï¼š

![img](images/m-attention5.png)

## ä½¿ç”¨ä½ç½®ç¼–ç è¡¨å¾åºåˆ—é¡ºåº

è¯´äº†é‚£ä¹ˆä¹…ï¼Œæœ‰ä»¶äº‹æ²¡è®²ï¼Œæ€ä¹ˆå»å°†è¾“å…¥åºåˆ—çš„è¯é¡ºåºè€ƒè™‘è¿›æ¨¡å‹å‘¢ï¼Ÿ(è¡¥å……ï¼šéœ€è¦è€ƒè™‘è¯é¡ºåºæ˜¯å› ä¸º Transformer æ²¡æœ‰å¾ªç¯ç½‘ç»œå’Œå·ç§¯ç½‘ç»œï¼Œ
å› æ­¤éœ€è¦å‘Šè¯‰æ¨¡å‹è¯çš„ç›¸å¯¹/ç»å¯¹ä½ç½®)

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTransformer ç»™æ¯ä¸ªè¾“å…¥ embedding åŠ ä¸Šä¸€ä¸ªå‘é‡ï¼Œè¯¥å‘é‡æœä»æ¨¡å‹å­¦ä¹ çš„ç‰¹å®šæ¨¡å¼ï¼Œ
è¿™å†³å®šäº†è¯ä½ç½®ï¼Œæˆ–è€…åºåˆ—ä¸­è¯ä¹‹é—´çš„è·ç¦»ã€‚å½“ embedding å‘é‡è¢«æ˜ å°„åˆ° Q/K/V å‘é‡å’Œç‚¹ä¹˜ attention æ—¶ï¼Œ
å¯¹ embedding å‘é‡åŠ ä¸Šä½ç½®å‘é‡æœ‰åˆ©äºæä¾›æœ‰æ„ä¹‰çš„è·ç¦»ä¿¡æ¯

![img](images/position_encoding.png)

å‡è®¾ embedding å‘é‡ç»´åº¦æ˜¯ 4ï¼Œé‚£ä¹ˆçœŸå®ä½ç½®ç¼–ç ä¼šå¦‚ä¸‹æ‰€ç¤ºï¼š

![img](images/position_encoding2.png)

è¿™ä¸ªæŒ‡å®šæ¨¡å¼æ˜¯æ€æ ·çš„ï¼Ÿå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¯è¡Œå¯¹åº”çš„æ˜¯ä¸€ä¸ªè¯å‘é‡çš„ä½ç½®ç¼–ç ï¼Œæ‰€ä»¥ç¬¬ä¸€è¡Œæ˜¯æˆ‘ä»¬è¦åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ç¬¬ä¸€ä¸ªè¯ embedding çš„å‘é‡ã€‚
æ¯è¡ŒåŒ…å« 512 ä¸ªå€¼ï¼Œæ¯ä¸ªå€¼èŒƒå›´åœ¨ -1 åˆ° 1 ä¹‹é—´ã€‚æˆ‘ä»¬å¯¹å€¼è¿›è¡Œç€è‰²å¯è§†åŒ–

![img](images/position_encoding3.png)

ä½ç½®ç¼–ç çš„å…¬å¼åœ¨è®ºæ–‡ (section 3.5) æœ‰æåŠï¼Œä½ ä¹Ÿèƒ½çœ‹åœ¨ `get_timing_signal_1d()` é‡Œä½ç½®ç¼–ç çš„ä»£ç ã€‚
è¿™ä¸æ˜¯ä½ç½®ç¼–ç çš„å”¯ä¸€æ–¹æ³•ï¼Œä½†å®ƒèƒ½å¤„ç†ä¸å¯è§é•¿åº¦çš„åºåˆ— (ä¾‹å¦‚æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹è¢«è¦æ±‚å»ç¿»è¯‘ä¸€ä¸ªè¶…è¿‡æˆ‘ä»¬è®­ç»ƒé›†å¥å­é•¿åº¦çš„å¥å­)ã€‚

ä»¥ä¸Šå±•ç¤ºçš„ä½ç½®ç¼–ç åœ¨ Tensor2Tensor (è®ºæ–‡çš„å¼€æºä»£ç ) å®ç°é‡Œæ˜¯åˆå¹¶ `$sin()$` å’Œ `$cos()$`ï¼Œ
ä½†æ˜¯è®ºæ–‡ä¸åœ¨è®ºæ–‡å±•ç¤ºçš„åˆä¸ä¸€æ ·ï¼Œè®ºæ–‡æ˜¯äº¤å‰ä½¿ç”¨ä¸¤ç§ signals (å³å¶æ•°ä½ç½®ä½¿ç”¨ `$sin()$`ï¼Œå¥‡æ•°ä½ç½®ä½¿ç”¨ `$cos()$`)ï¼Œ
ä¸‹å›¾ä¾¿æ˜¯è®ºæ–‡ç”Ÿæˆæ–¹å¼å¾—åˆ°çš„ï¼š

![img](images/position_encoding4.png)

ä½ç½®ç¼–ç çš„å…¬å¼å¦‚ä¸‹ï¼š

`$$PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$`

`$$PE_{(pos, 2i + 1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$`

å…¶ä¸­ï¼š

* `$pos$` æ˜¯ä½ç½®
* `$i$` æ˜¯ç»´åº¦
* `$2i$` ä»£è¡¨å¶æ•°ç»´åº¦
* `$2i+1$` ä»£è¡¨å¥‡æ•°ç»´åº¦
* `$d_{model}$` ä¸º 512

## æ®‹å·®é¡¹

åœ¨ç»§ç»­æ·±å…¥ä¸‹å»ä¹‹å‰ï¼Œç¼–ç å™¨ç»“æ„æœ‰ä¸ªç»†èŠ‚éœ€è¦æ³¨æ„ï¼šç¼–ç å™¨æ¯ä¸ªå­å±‚ (self-attention, ffnn) éƒ½æœ‰ä¸€ä¸ªæ®‹å·®è¿æ¥é¡¹ï¼Œ
å¹¶è·Ÿç€ layer normalization

![img](images/res1.png)

å¦‚æœæˆ‘ä»¬å°†å‘é‡å’Œ Self-attention å±‚çš„ layer-norm æ“ä½œå¯è§†åŒ–ï¼Œä¼šå¦‚ä¸‹æ‰€ç¤ºï¼š

![img](images/res2.png)

åœ¨è§£ç å™¨ä¹Ÿä¸€æ ·ï¼Œå¦‚æœ Transformer å †å äº† 2 å±‚çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå®ƒä¼šå¦‚ä¸‹æ‰€ç¤ºï¼š

![img](images/res3.png)

# è§£ç æ¨¡å—

ç°åœ¨ï¼Œæˆ‘ä»¬è®²äº†ç¼–ç å™¨å¤§éƒ¨åˆ†çš„å†…å®¹ï¼Œæˆ‘ä»¬ä¹ŸåŸºæœ¬äº†è§£è§£ç å™¨çš„æˆåˆ†ï¼Œä½†æˆ‘ä»¬æ¥çœ‹ä¸‹å®ƒä»¬æ˜¯æ€æ ·ä¸€èµ·å·¥ä½œçš„

## Encoder-Deocder Attention å±‚

ç¼–ç å™¨å…ˆå¤„ç†è¾“å…¥åºåˆ—ï¼Œæœ€ä¸Šå±‚ç¼–ç å™¨çš„è¾“å‡ºè¢«è½¬æ¢æˆä¸€ç»„å…³æ³¨å‘é‡ `$K$` å’Œ `$V$`ï¼Œ
å®ƒä»¬è¢«è¾“å…¥è¿› encoder-decoder attention å±‚å†…çš„æ¯ä¸ªè§£ç å™¨é‡Œï¼Œå¸®åŠ©è§£ç å™¨å…³æ³¨è¾“å…¥åºåˆ—ä¸­åˆé€‚çš„éƒ¨åˆ†

![img](images/decoder.gif)

æ¥ä¸‹æ¥ä¸æ–­é‡å¤è§£ç è¿‡ç¨‹ï¼Œç›´åˆ°ç”Ÿæˆå‡ºç‰¹æ®Šç¬¦å·ï¼Œæ ‡å¿— Transformer è§£ç å™¨å®Œæˆäº†è¾“å‡ºã€‚
æ¯æ­¥çš„è¾“å‡ºè¢«å–‚å…¥ä¸‹ä¸€æ—¶é—´è½®æ•°ä¸‹çš„åº•å±‚è§£ç å™¨ï¼Œç„¶åä¸æ–­å‘ä¸Šè¿è¾“è§£ç ç»“æœã€‚ä¸å¯¹å¾…ç¼–ç å™¨è¾“å…¥ä¸€æ ·ï¼Œ
æˆ‘ä»¬å‘é‡åŒ–å’ŒåŠ ä½ç½®ç¼–ç ç»™è§£ç å™¨çš„è¾“å…¥ï¼Œä»¥ä¾¿è§£ç å™¨äº†è§£åˆ°æ¯ä¸ªè¯çš„ä½ç½®ä¿¡æ¯

![img](images/decoder.png)

è§£ç å™¨ä¸­çš„ self-attention å±‚å’Œç¼–ç å™¨çš„æœ‰ç‚¹ä¸ä¸€æ ·ï¼šåœ¨è§£ç å™¨ä¸­ï¼Œself-attention å±‚åªå…è®¸å…³æ³¨è¾“å‡ºåºåˆ—çš„å†å²ä½ç½®ï¼Œ
è¿™é€šè¿‡ self-attention çš„ softmax å‰ï¼Œé®æ©æœªæ¥ä½ç½®å®ç° (å³è®¾ç½®å®ƒä»¬ä¸º -inf)ã€‚

encoder-decoder attention å±‚å·¥ä½œå°±åƒ multi-head self-attentionï¼Œé™¤äº†å®ƒæ˜¯ä»å‰ä¸€å±‚åˆ›å»ºå‡º Query çŸ©é˜µï¼Œ
ç„¶åæ¥æ”¶ç¼–ç å™¨è¾“å‡ºçš„ Key å’Œ Value çŸ©é˜µ

## Linear å’Œ Softmax å±‚

è§£ç å™¨ä»¬è¾“å‡ºæµ®ç‚¹å‘é‡ï¼Œæˆ‘ä»¬æ€ä¹ˆå°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªè¯å‘¢ï¼Ÿè¿™æ˜¯æœ€åçš„ Linear å’Œ Softmax å±‚æ‰€åšçš„å·¥ä½œã€‚

* çº¿æ€§å±‚å°±æ˜¯ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå°†è§£ç å™¨çš„è¾“å‡ºå‘é‡æ˜ å°„åˆ°ä¸€ä¸ªå¾ˆå¤§å¾ˆå¤§çš„å‘é‡å« logits å‘é‡
    - å‡è®¾æˆ‘ä»¬æ¨¡å‹é€è¿‡å­¦ä¹ è®­ç»ƒé›†ï¼ŒçŸ¥é“ 1 ä¸‡ä¸ªç‹¬ç‰¹çš„è‹±æ–‡å•è¯ (å³æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºè¯æ±‡)ã€‚
      è¿™å°±éœ€è¦ logits å‘é‡æœ‰ 1 ä¸‡ä¸ªæ ¼å­ï¼Œæ¯ä¸ªæ ¼å­å¯¹åº”ç€ä¸€ä¸ªç‹¬ç‰¹è¯çš„åˆ†æ•°
* Softmax å±‚å°†è¿™äº›åˆ†æ•°è½¬åŒ–ä¸ºæ¦‚ç‡ (0-1 çš„æ­£æ•°)ï¼Œé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„æ ¼å­ï¼Œç„¶åå¯¹åº”çš„å•è¯ä½œä¸ºå½“å‰æ—¶é—´æ­¥æ•°ä¸‹çš„è¾“å‡º

![img](images/linear_softmax.png)

# è®­ç»ƒæ¦‚è¦

æˆ‘ä»¬å·²ç»è®²äº† Transformer çš„å‰å‘è¿‡ç¨‹ï¼Œç°åœ¨æ¥çœ‹ä¸‹è®­ç»ƒçš„å†…å®¹

è®­ç»ƒæ—¶ï¼Œæœªè®­ç»ƒçš„æ¨¡å‹ä¼šé€šè¿‡ä¸€æ ·çš„å‰å‘è¿‡ç¨‹ï¼Œä½†ç”±äºæˆ‘ä»¬æ˜¯åœ¨æœ‰æ ‡ç­¾çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯”è¾“å‡ºå’ŒçœŸå®å€¼ã€‚
ä¸ºäº†å¯è§†åŒ–ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„è¾“å‡ºå•è¯åªåŒ…å« 6 ä¸ªè¯ 

> `a`ï¼Œ`am`ï¼Œ`i`ï¼Œ`thanks`ï¼Œ`student` å’Œ `â€œ<eos>â€`(`end of sentence`çš„ç®€å†™)

![img](images/word.png)

ä¸€æ—¦æˆ‘ä»¬å®šä¹‰äº†è¾“å‡ºè¯è¡¨ (output vocabulary)ï¼Œæˆ‘ä»¬èƒ½ç”¨ç›¸åŒé•¿åº¦çš„å‘é‡å»æ„å»ºè¯è¡¨é‡Œçš„æ¯ä¸ªè¯ï¼Œ
è¿™è¾¹æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–ç  `am` æˆå¦‚ä¸‹å‘é‡ï¼š

![img](images/word2.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¨è®ºä¸‹æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œåœ¨è®­ç»ƒé˜¶æ®µç”¨äºä¼˜åŒ–çš„åº¦é‡æŒ‡æ ‡ï¼Œä½¿å¾—æ¨¡å‹è®­ç»ƒå¾—æ›´åŠ å‡†ç¡®

# æŸå¤±å‡½æ•°

å‡è®¾æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹å®ç°å°† â€œmerciâ€ ç¿»è¯‘æˆ â€œthanksâ€ã€‚
è¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬æƒ³è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæŒ‡å‘è¯ â€œthanksâ€ã€‚ä½†å› ä¸ºæ¨¡å‹è¿˜æœªè®­ç»ƒï¼Œå®ƒè¿˜ä¸å¯ä»¥å‘ç”Ÿ

![img](images/loss1.png)

æˆ‘ä»¬æ€ä¹ˆå¯¹æ¯”ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Ÿç®€å•ç›¸å‡å°±å¥½(å¯¹äºç»†èŠ‚ï¼Œå¯ä»¥çœ‹äº¤å‰ç†µå’Œ KLæ•£åº¦ç›¸å…³èµ„æ–™)ã€‚
æ³¨æ„è¿™åªæ˜¯ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼Œæ›´çœŸå®æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šç”¨ä¸€ä¸ªå¥å­ï¼Œä¾‹å¦‚ `je suis Ã©tudiant`ï¼Œ
æœŸå¾…è¾“å‡º `i am a student`ã€‚åœ¨è¿™é‡Œæ˜¯å¸Œæœ›æˆ‘ä»¬æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼š

* æ¯ä¸ªæ¦‚ç‡åˆ†å¸ƒè¢«è¡¨å¾æˆä¸€ä¸ªé•¿åº¦ä¸ºè¯è¡¨å°ºå¯¸(`vocab_size`)çš„å‘é‡ (æˆ‘ä»¬ç®€å•ä¾‹å­ä¸º 6ï¼Œä½†ç°å®æ˜¯ 3 ä¸‡æˆ– 5 ä¸‡)
* ç¬¬ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒæœ‰æœ€é«˜çš„æ¦‚ç‡æ ¼å­æ˜¯åœ¨è¯ â€œiâ€ ä¸Š
* ç¬¬äºŒä¸ªæ¦‚ç‡åˆ†å¸ƒæœ‰æœ€é«˜çš„æ¦‚ç‡æ ¼å­æ˜¯åœ¨è¯ "am" ä¸Š
* ...
* ç›´åˆ°ç¬¬äº”ä¸ªæ¦‚ç‡åˆ†å¸ƒæŒ‡å‘ `<end of sentence>` ç¬¦å·ï¼Œè¿™é‡Œä¹Ÿæœ‰æ ¼å­å…³è”åˆ°å®ƒ

![img](images/loss2.png)

åœ¨è¶³å¤Ÿå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹è¶³å¤Ÿå¤šçš„æ—¶é—´ï¼Œæˆ‘ä»¬æœŸæœ›ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒä¼šå¦‚ä¸‹æ‰€ç¤ºï¼š

![img](images/loss3.png)

ç°åœ¨ï¼Œå› ä¸ºæ¨¡å‹æ¯æ­¥ç”Ÿæˆä¸€ä¸ªè¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾æ¨¡å‹é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯ï¼Œè€Œä¸¢å¼ƒå‰©ä½™çš„è¯ï¼Œ
è¿™ç§æ–¹å¼ç§°ä¸ºè´ªå©ªè§£ç ã€‚å¦ä¸€ç§æ–¹å¼æ˜¯ä¾¿æ˜¯ beam search äº†ï¼Œæ¯”æ–¹è¯´ç¬¬ä¸€æ­¥é¢„æµ‹æ—¶ï¼Œ'I' å’Œ 'a' æ˜¯ä¸¤ä¸ª Top æ¦‚ç‡è¯ï¼Œ
ç„¶åï¼Œå¦‚æœä»¥ 'I' å’Œ 'a' åˆ†åˆ«ä½œä¸ºç¬¬ä¸€ä¸ªé¢„æµ‹å€¼ï¼Œå»è¿›è¡Œä¸‹ä¸€æ­¥é¢„æµ‹ï¼Œ
å¦‚æœ 'I' ä½œä¸ºç¬¬ä¸€è¯é¢„æµ‹ç¬¬äºŒä¸ªè¯ä¸‹çš„è¯¯å·®æ¯” 'a' ä½œä¸ºç¬¬ä¸€è¯é¢„æµ‹ç¬¬äºŒä¸ªè¯ä¸‹çš„è¯¯å·®å°ï¼Œ
é‚£ä¹ˆä¾¿ä¿ç•™ 'I' ä½œä¸ºç¬¬ä¸€è¯ï¼Œä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚åœ¨æˆ‘ä»¬ä¾‹å­ä¸­ï¼Œ
beam_size æ˜¯ 2(æ„å‘³ç€ä»»ä½•æ—¶å€™ï¼Œä¸¤ä¸ªè¯ (æœªå®Œæˆçš„ç¿»è¯‘) çš„å‡è®¾éƒ½è¢«ä¿ç•™åœ¨ memory ä¸­)ï¼Œ
ç„¶å top_beams ä¹Ÿæ˜¯ 2 ä¸ª(æ„å‘³ç€æˆ‘ä»¬ä¼šè¿”å› 2 ä¸ªç¿»è¯‘)ï¼Œè¿™äº›éƒ½æ˜¯è¶…å‚å¯ä»¥è°ƒæ•´çš„


# Transformer ç»“æ„è§£æ

![img](images/transformer.jpg)

# Transformer æ•°å­¦è¡¨ç¤º

`$$\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$`

`$$\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &=\operatorname{Concat}\left(\operatorname{head}_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\
\text { where }\, head_{i} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}$$`

# Transformer è¦ç‚¹é—®é¢˜

1. Transformer æ˜¯å¦‚ä½•è§£å†³é•¿è·ç¦»ä¾èµ–çš„é—®é¢˜çš„ï¼Ÿ
    - Transformer æ˜¯é€šè¿‡å¼•å…¥ Scale-Dot-Product æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆåºåˆ—ä¸Šä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œè§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚
      ä»¥æ–‡æœ¬æ•°æ®ä¸ºä¾‹ï¼Œåœ¨å¾ªç¯ç¥ç»ç½‘ç»œ LSTM ç»“æ„ä¸­ï¼Œè¾“å…¥åºåˆ—ä¸Šç›¸è·å¾ˆè¿œçš„ä¸¤ä¸ªå•è¯æ— æ³•ç›´æ¥å‘ç”Ÿäº¤äº’ï¼Œ
      åªèƒ½é€šè¿‡éšè—å±‚è¾“å‡ºæˆ–è€…ç»†èƒçŠ¶æ€æŒ‰ç…§æ—¶é—´æ­¥éª¤ä¸€ä¸ªä¸€ä¸ªå‘åè¿›è¡Œä¼ é€’ã€‚
      å¯¹äºä¸¤ä¸ªåœ¨åºåˆ—ä¸Šç›¸è·éå¸¸è¿œçš„å•è¯ï¼Œä¸­é—´ç»è¿‡çš„å…¶å®ƒå•è¯è®©éšè—å±‚è¾“å‡ºå’Œç»†èƒçŠ¶æ€æ··å…¥äº†å¤ªå¤šçš„ä¿¡æ¯ï¼Œ
      å¾ˆéš¾æœ‰æ•ˆåœ°æ•æ‰è¿™ç§é•¿è·ç¦»ä¾èµ–ç‰¹å¾ã€‚ä½†æ˜¯åœ¨ Scale-Dot-Product æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œ
      åºåˆ—ä¸Šçš„æ¯ä¸ªå•è¯éƒ½ä¼šå’Œå…¶å®ƒæ‰€æœ‰å•è¯åšä¸€æ¬¡ç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œ
      è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ä¸­å•è¯ä¹‹é—´çš„äº¤äº’æ˜¯å¼ºåˆ¶çš„ä¸å—è·ç¦»å½±å“çš„ï¼Œæ‰€ä»¥å¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜
2. Transformer åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µå¯ä»¥åœ¨æ—¶é—´(åºåˆ—)ç»´åº¦ä¸Šè¿›è¡Œå¹¶è¡Œå—ï¼Ÿ
    - åœ¨è®­ç»ƒé˜¶æ®µï¼ŒEncoder å’Œ Decoder åœ¨æ—¶é—´(åºåˆ—)ç»´åº¦éƒ½æ˜¯å¹¶è¡Œçš„ï¼›
      åœ¨æµ‹è¯•é˜¶æ®µï¼ŒEncoder åœ¨åºåˆ—ç»´åº¦æ˜¯å¹¶è¡Œçš„ï¼ŒDecoder æ˜¯ä¸²è¡Œçš„
    - é¦–å…ˆï¼ŒEncoder éƒ¨åˆ†åœ¨è®­ç»ƒé˜¶æ®µå’Œé¢„æµ‹é˜¶æ®µéƒ½å¯ä»¥å¹¶è¡Œæ¯”è¾ƒå¥½ç†è§£ï¼Œ
      æ— è®ºåœ¨è®­ç»ƒè¿˜æ˜¯é¢„æµ‹é˜¶æ®µï¼Œå®ƒå¹²çš„äº‹æƒ…éƒ½æ˜¯æŠŠå·²çŸ¥çš„å®Œæ•´è¾“å…¥ç¼–ç æˆ memoryï¼Œ
      åœ¨åºåˆ—ç»´åº¦å¯ä»¥å¹¶è¡Œ
    - å¯¹äº Decoder éƒ¨åˆ†æœ‰äº›å¾®å¦™ã€‚åœ¨é¢„æµ‹é˜¶æ®µ Decoder è‚¯å®šæ˜¯ä¸èƒ½å¹¶è¡Œçš„ï¼Œå› ä¸º Decoder å®é™…ä¸Šæ˜¯ä¸€ä¸ªè‡ªå›å½’ï¼Œ
      å®ƒå‰é¢ `$k-1$` ä½ç½®çš„è¾“å‡ºä¼šå˜æˆç¬¬ `$k$` ä½çš„è¾“å…¥çš„ã€‚å‰é¢æ²¡æœ‰è®¡ç®—å®Œï¼Œåé¢æ˜¯æ‹¿ä¸åˆ°è¾“å…¥çš„ï¼Œè‚¯å®šä¸å¯ä»¥å¹¶è¡Œã€‚
      é‚£ä¹ˆè®­ç»ƒé˜¶æ®µèƒ½å¦å¹¶è¡Œå‘¢ï¼Ÿè™½ç„¶è®­ç»ƒé˜¶æ®µçŸ¥é“äº†å…¨éƒ¨çš„è§£ç ç»“æœï¼Œä½†æ˜¯è®­ç»ƒé˜¶æ®µè¦å’Œé¢„æµ‹é˜¶æ®µä¸€è‡´å•Šï¼Œ
      å‰é¢çš„è§£ç è¾“å‡ºä¸èƒ½å—åˆ°åé¢è§£ç ç»“æœçš„å½±å“å•Šã€‚
      ä½† Transformer é€šè¿‡åœ¨ Decoder ä¸­å·§å¦™åœ°å¼•å…¥ Mask æŠ€å·§ï¼Œä½¿å¾—åœ¨ç”¨ Attention æœºåˆ¶åšåºåˆ—ç‰¹å¾èåˆçš„æ—¶å€™ï¼Œ
      æ¯ä¸ªå•è¯å¯¹ä½äºå®ƒä¹‹åçš„å•è¯çš„æ³¨æ„åŠ›å¾—åˆ†éƒ½ä¸º 0ï¼Œè¿™æ ·å°±ä¿è¯äº†å‰é¢çš„è§£ç è¾“å‡ºä¸ä¼šå—åˆ°åé¢è§£ç ç»“æœçš„å½±å“ï¼Œ
      å› æ­¤ Decoder åœ¨è®­ç»ƒé˜¶æ®µå¯ä»¥åœ¨åºåˆ—ç»´åº¦åšå¹¶è¡Œ
3. Scaled-Dot Product Attention ä¸ºä»€ä¹ˆè¦é™¤ä»¥ `$\sqrt{d_k}$`?
    - ä¸ºäº†é¿å… `$d_k$` å˜å¾—å¾ˆå¤§æ—¶ softmax å‡½æ•°çš„æ¢¯åº¦è¶‹äº 0ã€‚
      å‡è®¾ Q å’Œ K ä¸­çš„å–å‡ºçš„ä¸¤ä¸ªå‘é‡ `$q$` å’Œ `$k$` çš„æ¯ä¸ªå…ƒç´ å€¼éƒ½æ˜¯æ­£æ€éšæœºåˆ†å¸ƒï¼Œ
      æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ä¸¤ä¸ªç‹¬ç«‹çš„æ­£æ€éšæœºå˜é‡çš„ç§¯ä¾ç„¶æ˜¯ä¸€ä¸ªæ­£æ€éšæœºå˜é‡ï¼Œ
      é‚£ä¹ˆä¸¤ä¸ªå‘é‡åšç‚¹ç§¯ï¼Œä¼šå¾—åˆ° `$d_k$` ä¸ªæ­£æ€éšæœºå˜é‡çš„å’Œï¼Œ
      æ•°å­¦ä¸Š `$d_k$` ä¸ªæ­£æ€éšæœºå˜é‡çš„å’Œä¾ç„¶æ˜¯ä¸€ä¸ªæ­£æ€éšæœºå˜é‡ï¼Œ
      å…¶æ–¹å·®æ˜¯åŸæ¥çš„ `$d_k$` å€ï¼Œæ ‡å‡†å·®æ˜¯åŸæ¥çš„ `$\sqrt{d_k}$` å€ã€‚
      å¦‚æœä¸åš scale, å½“ `$d_k$` å¾ˆå¤§æ—¶ï¼Œæ±‚å¾—çš„ `$QK^T$` å…ƒç´ çš„ç»å¯¹å€¼å®¹æ˜“å¾ˆå¤§ï¼Œ
      å¯¼è‡´è½åœ¨ softmax çš„æç«¯åŒºåŸŸ(è¶‹äº 0 æˆ–è€… 1)ï¼Œæç«¯åŒºåŸŸ softmax å‡½æ•°çš„æ¢¯åº¦å€¼è¶‹äº 0ï¼Œ
      ä¸åˆ©äºæ¨¡å‹å­¦ä¹ ã€‚é™¤ä»¥ `$\sqrt{d_k}$`ï¼Œæ°å¥½åšäº†å½’ä¸€ï¼Œä¸å— `$d_k$` å˜åŒ–å½±å“
4. MultiHeadAttention çš„å‚æ•°æ•°é‡å’Œ head æ•°é‡æœ‰ä½•å…³ç³»?
    - MultiHeadAttention çš„å‚æ•°æ•°é‡å’Œ head æ•°é‡æ— å…³ã€‚
      å¤šå¤´æ³¨æ„åŠ›çš„å‚æ•°æ¥è‡ªå¯¹ QKV çš„ä¸‰ä¸ªå˜æ¢çŸ©é˜µä»¥åŠå¤šå¤´ç»“æœ concat åçš„è¾“å‡ºå˜æ¢çŸ©é˜µã€‚
      å‡è®¾åµŒå…¥å‘é‡çš„é•¿åº¦æ˜¯ d_model, ä¸€å…±æœ‰ h ä¸ª head. å¯¹æ¯ä¸ª headï¼Œ
      `$W_{i}^{Q},W_{i}^{K},W_{i}^{V}$` è¿™ä¸‰ä¸ªå˜æ¢çŸ©é˜µçš„å°ºå¯¸éƒ½æ˜¯ `$d_model \times (d_model/h)$`ï¼Œ
      æ‰€ä»¥ h ä¸ª head æ€»çš„å‚æ•°æ•°é‡å°±æ˜¯ `$3 \times d_model \times (d_model/h) \times h = 3 \times d_model \times d_model$`ã€‚
      å®ƒä»¬çš„è¾“å‡ºå‘é‡é•¿åº¦éƒ½å˜æˆ `$d_model/h$`ï¼Œç»è¿‡ attention ä½œç”¨åå‘é‡é•¿åº¦ä¿æŒï¼Œ
      h ä¸ª head çš„è¾“å‡ºæ‹¼æ¥åˆ°ä¸€èµ·åå‘é‡é•¿åº¦è¿˜æ˜¯ d_modelï¼Œ
      æ‰€ä»¥æœ€åè¾“å‡ºå˜æ¢çŸ©é˜µçš„å°ºå¯¸æ˜¯ `$d_modelÃ—d_model$`ã€‚
      å› æ­¤ï¼ŒMultiHeadAttention çš„å‚æ•°æ•°é‡ä¸º `$4 \times d_model \times d_model$`ï¼Œå’Œ head æ•°é‡æ— å…³
5. Transformer æœ‰ä»€ä¹ˆç¼ºç‚¹ï¼Ÿ
    - Transformer ä¸»è¦çš„ç¼ºç‚¹æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯æ³¨æ„åŠ›æœºåˆ¶ç›¸å¯¹åºåˆ—é•¿åº¦çš„å¤æ‚åº¦æ˜¯ `$O(n^2)$`ï¼Œç¬¬äºŒä¸ªæ˜¯å¯¹ä½ç½®ä¿¡æ¯çš„
        - ç¬¬ä¸€ï¼ŒTransformer åœ¨ç”¨ Attention æœºåˆ¶åšåºåˆ—ç‰¹å¾èåˆçš„æ—¶å€™ï¼Œ
          æ¯ä¸¤ä¸ªå•è¯ä¹‹é—´éƒ½è¦è®¡ç®—ç‚¹ç§¯è·å¾—æ³¨æ„åŠ›å¾—åˆ†ï¼Œè¿™ä¸ªè®¡ç®—å¤æ‚åº¦å’Œåºåˆ—çš„é•¿åº¦å¹³æ–¹æˆæ­£æ¯”ï¼Œ
          å¯¹äºä¸€äº›ç‰¹åˆ«é•¿çš„åºåˆ—ï¼Œå¯èƒ½å­˜åœ¨ç€æ€§èƒ½ç“¶é¢ˆï¼Œæœ‰ä¸€äº›é’ˆå¯¹è¿™ä¸ªé—®é¢˜çš„æ”¹è¿›æ–¹æ¡ˆå¦‚ Linformer
        - ç¬¬äºŒä¸ªæ˜¯ Transformer é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ä¸¤ä¸¤ä½ç½®åšç‚¹ä¹˜æ¥èåˆåºåˆ—ç‰¹å¾ï¼Œ
          è€Œä¸æ˜¯åƒå¾ªç¯ç¥ç»ç½‘ç»œé‚£æ ·ç”±å…ˆåˆ°ååœ°å¤„ç†åºåˆ—ä¸­çš„æ•°æ®ï¼Œå¯¼è‡´ä¸¢å¤±äº†å•è¯ä¹‹é—´çš„ä½ç½®ä¿¡æ¯å…³ç³»ï¼Œ
          é€šè¿‡åœ¨è¾“å…¥ä¸­å¼•å…¥æ­£ä½™å¼¦å‡½æ•°æ„é€ çš„ä½ç½®ç¼–ç  PositionEncoding ä¸€å®šç¨‹åº¦ä¸Šè¡¥å……äº†ä½ç½®ä¿¡æ¯ï¼Œ
          ä½†è¿˜æ˜¯ä¸å¦‚å¾ªç¯ç¥ç»ç½‘ç»œé‚£æ ·è‡ªç„¶å’Œé«˜æ•ˆ

# PyTorch ç¤ºä¾‹

```python
import torch 
from torch import nn 

#éªŒè¯MultiheadAttentionå’Œheadæ•°é‡æ— å…³
inputs = torch.randn(8,200,64) #batch_size, seq_length, features

attention_h8 = nn.MultiheadAttention(
    embed_dim = 64,
    num_heads = 8,
    bias=True,
    batch_first=True
)

attention_h16 = nn.MultiheadAttention(
    embed_dim = 64,
    num_heads = 16,
    bias=True,
    batch_first=True
)


out_h8 = attention_h8(inputs,inputs,inputs)
out_h16 = attention_h16(inputs,inputs,inputs)

from torchkeras import summary 
summary(attention_h8,input_data_args=(inputs,inputs,inputs));

summary(attention_h16,input_data_args=(inputs,inputs,inputs));
```


```python
import torch 
from torch import nn 
from copy import deepcopy

#å¤šå¤´æ³¨æ„åŠ›çš„ä¸€ç§ç®€æ´å®ç°

class ScaledDotProductAttention(nn.Module):
    "Compute 'Scaled Dot Product Attention'"
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(self,query, key, value, mask=None, dropout=None):
        d_k = query.size(-1)
        scores = query@key.transpose(-2,-1) / d_k**0.5     
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e20)
        p_attn = F.softmax(scores, dim = -1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return p_attn@value, p_attn
    
class MultiHeadAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = nn.ModuleList([deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])
        
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        self.attention = ScaledDotProductAttention()
        
    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model => h x d_k 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = self.attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
```

# å‚è€ƒ

* [Transformer](https://mp.weixin.qq.com/s?__biz=MzUyNzA1OTcxNg==&mid=2247486160&idx=1&sn=2dfdedb2edbca76a0c7b110ca9952e98&chksm=fa0414bbcd739dad0ccd604f6dd5ed99e8ab7f713ecafc17dd056fc91ad85968844e70bbf398&scene=178&cur_album_id=1577157748566310916#rd)
* [Hugging Face](https://huggingface.co/docs/transformers/quicktour)
* [ğŸ¤— Transformers æ•™ç¨‹ï¼špipelineä¸€é”®é¢„æµ‹](https://mp.weixin.qq.com/s/1dtk5gCa7C-wyVQ9vIuRYw)
* [Transformer çš„ä¸€å®¶](https://mp.weixin.qq.com/s/ArzUQHQ-imSpWRPt6XG9FQ)
* [Transformer çŸ¥ä¹åŸç†è®²è§£](https://zhuanlan.zhihu.com/p/48508221)
* [Transformer å“ˆä½›åšå®¢ä»£ç è®²è§£](http://nlp.seas.harvard.edu/annotated-transformer/)
