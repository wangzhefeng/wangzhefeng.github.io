---
title: 正则化
author: 王哲峰
date: '2022-07-13'
slug: dl-yolo
categories:
  - deeplearning
tags:
  - model
---


# 神经网络过拟合

## 监督学习的目的

监督机器学习的目的是为了让建立的模型能够发现数据中普遍的一般的规律, 这个普遍的一般的规律无论对于训练集合适未知的测试集, 都具有较好的拟合能力。
假设空间中模型千千万, 当我们站在上帝视角, 心里相信总会有个最好的模型可以拟合我们的训练数据, 而且这个模型不会对训练集过度学习, 
它能够从训练集中尽可能的学到适用于所有潜在样本的“普遍规律”, 不会将数据中噪声也学习了。这样的模型也就是我们想要的、能够有较低的泛化误差的模型.

## 模型过拟合

- **过拟合含义:**
   - 过拟合是指在机器学习模型训练过程中, 模型对训练数据学习过度, 将数据中包含的噪声和误差也学习了, 使得模型在训练集上表现很好, 而在测试集上表现很差的一种现象。
- **发生过拟合的原因:**
   - 训练数据少
   - 模型比较初级, 无法解释复杂的数据
   - 模型拥有大量的参数(模型复杂度高)
- **解决或者缓解过拟合的方法:**
   - 获取更多的训练数据
   - 选用更好更加集成的模型
   - 为损失函数添加正则化项

## 监督学习正则化

- 监督机器学习的核心原理:
     `$$argmin \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i; \delta)) + \lambda J(f)$$`
  - 上述公式是机器学习中最核心、最关键、最能概述监督学习的核心思想的公式
- 监督机器学习的核心问题      
  - 确定正则化参数的同时最小化经验风险。最小化经验风险是为了让模型更加充分的拟合给定的训练数据, 
     而正则化参数则是控制这模型的复杂度, 防止我们过分的拟合训练数据, 从上面的公式可以看出, 
     监督机器学习的模型效果的控制有两项
- 经验风险项
  - 经验风险项主要是由训练数据集控制, 一般要求模型将最小化经验误差, 为的是模型极大程度的拟合训练数据, 如果该项过大则可能导致欠拟合, 欠拟合好办, 继续训练就是了.
  - 至少80%的单一机器学习模型都是上面这个公式可以解释的。无非就是对这两项变着法儿换样子而已。对于第一项的损失函数:
     - 如果形式是平方损失 (square loss) ,就是线性回归
     - 如果是对数损失 (log loss) , 就是对数几率回归
     - 如果是合页损失 (hingeloss) , 就是支持向量机
     - 如果是指数损失 (exp loss), 就是 Adaboost
- 正则化项
  - 从统计学习的角度来看, 对监督机器学习加入正则化项是结构风险最小化策略的实现, 正则化项一般是模型复杂度的单调递增函数, 模型越复杂, 正则化值就越大, 所以正则化项的存在能够使得我们的模型避免走向过拟合, 即防止模型过分拟合训练数据
  - 对于正则化项, `$\lambda` 是正则化系数, 通常是大于 0 的较小的正数(0.01, 0.001, ...), 是一种调整经验误差和正则化项之间关系的系数。所以, 在实际的训练过程中,  `$\lambda`
     作为一种超参数很大程度上决定了模型的好坏.
     - `$\lambda = 0` 时相当于该公式没有正则化项, 模型全力讨好第一项, 将经验误差进行最小化, 往往这也是最容易发生过拟合的时候.
     - 随着 `$\lambda` 逐渐增大, 正则化项在模型选择中的话语权越来越高, 对模型的复杂性的惩罚也越来越厉害, 模型中参数的值逐渐接近 `$0` 或等于 `$0`.
  - 对于正则化项, 正则化项的形式有很多, 但常见的也就是 `$L1` 和 `$L2` 正则化。也有 `$L0` 正则化.
     - `$L0` 正则化也就是 `$L0` 范数, 即矩阵中所有非 `$0` 元素的个数。`$L0` 范数就是希望要正则化的参数矩阵 `$W` 大多数元素都为 `$0`, 
        简单粗暴, 让参数矩阵 `$W` 大多数元素为 `$0` 就是实现稀疏而已.
     - `$L1` 范数就是矩阵中各元素绝对值之和, `$L1` 范数通常用于实现参数矩阵的稀疏性。稀疏通常是为了特征选择和易于解释方面的考虑.
        在机器学习领域, `$L0` 和 `$L1` 都可以实现矩阵的稀疏性, 但在实践中, `$L1` 要比 `$L0` 具备更好的泛化求解特性而广受青睐.
     - 相较于 `$L0` 和 `$L1$` , 其实 `$L2` 才是正则化中的天选之子。在各种防止过拟合和正则化处理过程中, `$L2`
        正则化可谓风头无二。`$L2` 范数是指矩阵中各元素的平方和后的求根结果。采用 `$L2`
        范数进行正则化的原理在于最小化参数矩阵的每个元素, 使其无限接近于 `$0` 但又不像 `$L1` 那样等于 `$0`, 
        为什么参数矩阵中每个元素变得很小就能防止过拟合？用深度神经网络来举例, 在 `$L2` 正则化中, 如果正则化系数变得比较大, 
        参数矩阵 `$W` 中的每个元素都在变小, 线性计算的和 `$Z` 也会变小, 激活函数在此时相对呈线性状态, 
        这样就大大简化了深度神经网络的复杂性, 因而可以防止过拟合.

## 常用正则化方法

 - `$L1` 正则化, Lasso         
       `$$\min \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i)) + \lambda ||w||$$`
 - `$L2` 正则化, 岭回归(Ridge Regression)         
       `$$\min \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i)) + \frac{\lambda}{2} ||w||^{2}$$`
- 两者都是对回归损失函数加一个约束项, lasso 加的是回归系数的 `$L1` 范数, 岭回归加的是回归系数 `$L2` 范数.

# 神经网络正则化

神经网络的正则化方法:

- 权重衰减
    - `$L_2$` 正则化
    - `$L_1$` 正则化
    - `$L_\infty$` 正则化
- Dropout

## 权值衰减(L1,L2 正则化)

- 未正则化的交叉熵损失函数
     `$$J = -\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)$$`
- L1 正则化的交叉熵损失函数         
     `$$J = \underbrace{-\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)}\_{\text{cross-entropy cost}} + \underbrace{\frac{1}{m}\lambda\sum_{l}\sum_{k}\sum_{j} ||W_{k,j}^{[L]}||}\_{\text{L1 regularization cost}}$$`
- L2 正则化的交叉熵损失函数
     `$$J = \underbrace{-\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)}\_{\text{cross-entropy cost}} + \underbrace{\frac{1}{m}\frac{\lambda}{2}\sum_{l}\sum_{k}\sum_{j} W_{k,j}^{[L]2}}\_{\text{L2 regularization cost}}$$`

## Dropout

- 当网络的模型变得很复杂时, 权值衰减方法不能有效地对过拟合进行抑制; 
- Dropout在神经网络学习的过程中随机删除神经元:
- 训练时, 随机选出隐藏层的神经元, 然后将其删除, 被删除的神经元不再进行信号的传递
- 训练时, 每传递一次数据, 就会随机选择要删除的神经元
- 测试时, 虽然会传递所有的神经元信号, 但是对于各个神经元的输出, 要乘上训练时的删除比例后再输出

Dropout 是指在神经网络训练的过程中, 对所有神经元按照一定的概率进行消除的处理方式。在训练深度神经网络时, Dropout
能够在很大程度上简化神经网络结构, 防止神经网络过拟合。所以, 从本质上而言, Dropout 也是一种神经网络的正则化方法。

假设我们要训练了一个多隐藏层的神经网络, 该神经网络存在着过拟合。于是我们决定使用 Dropout 方法来处理, 
Dropout 为该网络每一层的神经元设定一个失活 (drop) 概率, 在神经网络训练过程中, 我们会丢弃一些神经元节点, 
在网络图上则表示为该神经元节点的进出连线被删除。最后我们会得到一个神经元更少、模型相对简单的神经网络, 
这样一来原先的过拟合情况就会大大的得到缓解。这样说似乎并没有将 Dropout 正则化原理解释清楚.

为什么 Dropout 可以可以通过正则化发挥防止过拟合的功能？

- 因为 Dropout 可以随时随机的丢弃任何一个神经元, 神经网络的训练结果不会依赖于任何一个输入特征, 每一个神经元都以这种方式进行传播, 
   并为神经元的所有输入增加一点权重, Dropout 通过传播所有权重产生类似于 L2 正则化收缩权重的平方范数的效果, 这样的权重压缩类似于 L2
   正则化的权值衰减, 这种外层的正则化起到了防止过拟合的作用。

所以说, 总体而言, Dropout 的功能类似于 L2 正则化, 但又有所区别。另外需要注意的一点是, 对于一个多层的神经网络, 
我们的 Dropout 某层神经元的概率并不是一刀切的。对于不同神经元个数的神经网络层, 我们可以设置不同的失活或者保留概率, 
对于含有较多权值的层, 我们可以选择设置较大的失活概率 (即较小的保留概率) 。所以, 总结来说就是如果你担心某些层所含神经
元较多或者比其他层更容易发生过拟合, 我们可以将该层的失活概率设置的更高一些。

## 数据增强


## 提前终止
