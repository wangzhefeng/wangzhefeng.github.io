---
title: 参数初始化
author: 王哲峰
date: '2022-07-13'
slug: dl-yolo
categories:
  - deeplearning
tags:
  - model
---

参数初始化
==========

1.参数初始化

   - 在神经网络的学习中, 权重 `$W`
      的的初始值特别重要。设定什么样的权重初始值, 经常关系到神经网络的学习能否成功; 

   - 不能将权重初始值全部设为0:

   - 因为在误差反向传播中, 所有权重都会进行相同的更新; 

      - 比如:在2层神经网络中, 假设第1层和第2层的权重为0, 这样一来, 正向传播时, 因为输入层的权重是0, 所有第2层的权重神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值, 这意味着反向传播时第2层的权重全部都会进行姓童的更新, 因此, 权重被更新为相同的值, 并拥有了对称的值, 这使得神经网络拥有许多不同的权重的意义就丧失了。

   - 为了防止“权重均一化”, 必须随机生成初始值; 

   - 梯度消失(gradient vanishing)

   - 当使用sigmoid函数作为激活函数时, 激活层的激活值呈偏向0和1的分布, 随着输出不断靠近0或1, 它导数的值逐渐接近0, 因此, 偏向0和1的数据分布会造成反向传播中梯度的值不断减小, 最后消失。层次加深的深度学习中, 梯度消失的问题更加严重; 

   - 表现力受限

   - 如果有多个神经元都输出几乎相同的值, 那他们就没有存在的意义了。比如, 如果100个神经元都输出几乎相同的值, 那么也可以由1个神经元来表示基本相同的事情。因此, 激活值在分布上有所偏向; 

   - 各层的激活函值的分布都要求有适当地广度。

   - 因为通过在各层间传递多样性的数据, 神经网络可以进行高效的学习, 反之, 如果传递的是有所偏向的数据, 就会出现梯度消失或者“表现力受限”的问题, 导致学习无法顺利进行; 

Xavier初始值:
~~~~~~~~~~~~~~

   在Xavier
   Glorot等人的论文中, 推荐了权重初始值, 俗称“Xavier初始值”。在一般的深度学习框架中, Xavier初始值已经被作为标准使用; 

Xavier的论文中, 为了使各层的激活值呈现出具有相同广度的分布, 推导了合适的权重尺度。推导出的结论是:

**`$如果前一层的节点数为 n, 则初始值使用标准差为 \frac{1}{\sqrt{n}} 的分布; `**

He初始值
~~~~~~~~

   Xavier初始值是以激活函数是线性函数为前提推导出来的, 因为sigmoid函数和tanh函数左右对称, 且中央附近可以视作线性函数, 所以适合使用Xavier初始值; 
   当激活函数使用ReLU函数时, 一般推荐ReLU专用的初始值, Kaiming
   He等人推荐了一种初始值, 俗称“He初始值”; 

`$当如果前一层的节点数为 n, 则初始值使用标准差为\sqrt{\frac{2}{n}}的高斯分布; `

**结论:**

- 当激活函数使用ReLU时, 权重初始值使用\ **He初始值**\ ; 

- 当激活函数为sigmoid或tanh等S型函数时, 初始值使用\ **Xavier初始值**\ ; 


2.Batch Normalization
---------------------

   - 设定合适的权重初始值, 各层的激活值分布就会有适当地广度, 从而可以顺利地进行学习; 

   - 为了使各层拥有适当的广度, Batch Normalization
      方法“强制性”地调整激活值的分布; 


2.1 Batch Normalization原理
~~~~~~~~~~~~~~~~~~~~~~~~~~~

   - Batch Normalization
      的思路是调整各层的激活值分布使其拥有适当的广度。为此, 要向神经网络中插入对数据分布进行的正规化层, 即Batch
      Normalization层。

Batch
Normalization, 顾名思义, 以进行学习时的mini-batch为单位, 按mini-batch进行正规化。具体来说, 就是对mini-batch数据进行数据分布的均值为0, 方差为1的正规化, 数学表示如下:

`$\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i$`

`$\sigma_{B}^{2} \leftarrow \frac{1}{m}\sum_{i=1}^{m}(x_i - \mu_B)^2`

`$\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_{B}^{2} + \epsilon}}$`

这里对 mini-batch 的 m 个输入数据的集合
`$B=\{x_1, x_2, \ldots, x_m\}$` 求均值 `$\mu_B$` 和方差
`$\sigma_B^{2}$` 。然后对输入数据进行均值为 0, 方差为 1
的正规化。其中 `$\epsilon` 取一个较小的值
`$10e-7$` 。即将mini-batch的输入数据
`$\{x_1, x_2, \ldots, x_m\}$` 变换为均值为0, 方差为1的数据
`$\{\hat{x_1}, \hat{x_2}, \ldots, \hat{x_m}\}$` 。通过将这个处理插入到激活函数的前面或后面, 可以减小数据分布的偏向。

接着Batch
Normalization层会对正规化后的数据进行缩放和平移的变换, 数学表示如下:

`$y_i \leftarrow \gamma \hat{x_i} + \beta`

其中:

- `$\gamma` 和 `$\beta$` : 是参数, 初始值一般设为
   `$\gamma=1`, `$\beta=0$` , 然后通过学习整合到合适的值; 


2.2 Batch Normalization优点
~~~~~~~~~~~~~~~~~~~~~~~~~~~

- 可以使学习快速进行(可以增大学习率); 

- 不那么依赖初始值(对初始值不敏感); 

- 抑制过拟合(降低Dropout等的必要性); 


3.超参数的调优
--------------

   - 神经网络中的超参数是指, 各层的神经元数量, batch大小, 参数更新时的学习率, 权值衰减参数(正则化参数)等; 

   - 不能使用测试数据评估超参数的性能

   - 调整超参数时, 必须使用超参数专用的确认数据, 用于调整超参数的数据一般称为验证数据(validation
      data); 

   - 模型训练数据的使用:

   - 训练数据用于参数(权重和偏置)的学习; 

   - 验证数据用于超参数的性能评估; 

   - 测试数据确认泛化能力, 要在最后使用(比较理想的是只用一次); 
